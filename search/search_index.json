{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Documentation: https://googlecloudplatform.github.io/applied-ai-engineering-samples/</p> <p>Source Code: https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples</p> <p></p> <p>Welcome to the Google Cloud Applied AI Engineering repository. This repository contains reference guides, blueprints, code samples, and hands-on labs developed by the Google Cloud Applied AI Engineering team.</p> <p></p>"},{"location":"#applied-ai-engineering-catalog","title":"Applied AI Engineering: Catalog","text":""},{"location":"#generative-ai-on-vertex-ai","title":"Generative AI on Vertex AI","text":"<p>This section contains code samples and hands-on labs demonstrating the use of Generative AI models and tools in Vertex AI.</p> Foundation Models Evaluation RAG &amp; Grounding Agents Others <ul> <li>Gemini Prompting Recipes</li> </ul> <ul> <li>Vertex GenAI Evaluation</li> <li>Gemini Evals Playbook</li> </ul> <ul> <li>Vertex AI Search</li> <li>Retrieval Augmented Generation</li> </ul>"},{"location":"#google-cloud-aiml-infrastructure","title":"Google Cloud AI/ML infrastructure","text":"<p>This section has reference guides and blueprints that compile best practices, and prescriptive guidance for running large-scale AI/ML workloads on Google Cloud AI/ML infrastructure.</p>"},{"location":"#research-operationalization","title":"Research Operationalization","text":"<p>This section has code samples demonstrating operationalization of latest research models or frameworks from Google DeepMind and Research teams on Google Cloud including Vertex AI.</p>"},{"location":"#solutions-catalog","title":"Solutions Catalog","text":"<p>In addition to code samples in this repo, you may want to check out the following solutions published by Google Cloud Applied AI Engineering.</p> Solution Description Open Data Q&amp;A        The Open Data QnA python solution enables you to chat with your databases by leveraging LLM Agents on Google Cloud. The solution enables a conversational approach to interact with your data by implementing state-of-the-art NL2SQL / Text2SQL methods.      GenAI for Marketing        Showcasing Google Cloud's generative AI for marketing scenarios via application frontend, backend, and detailed, step-by-step guidance for setting up and utilizing generative AI tools, including examples of their use in crafting marketing materials like blog posts and social media content, nl2sql analysis, and campaign personalization.      GenAI for Customer Experience Modernization        This solution shows how customers can have modern, engaging interactions with brands, and companies can improve the end user, agent, and customer experiences with a modern customer service platform on Google Cloud.       Creative Studio | Vertex AI        Creative Studio is a Vertex AI generative media example user experience to highlight the use of Imagen and other generative media APIs on Google Cloud.      RAG Playground        RAG Playground is a platform to experiment with RAG (Retrieval Augmented Generation) techniques. It integrates with LangChain and Vertex AI, allowing you to compare different retrieval methods and/or LLMs on your own datasets. This helps you build, refine, and evaluate RAG-based applications."},{"location":"#getting-help","title":"Getting help","text":"<p>If you have any questions or if you found any problems with this repository, please report through GitHub issues.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This is not an officially supported Google product. The code in this repository is for demonstrative purposes only.</p>"},{"location":"ai-infrastructure/","title":"Google Cloud AI/ML infrastructure","text":"<p>This folder contains reference guides and blueprints that compile best practices, and prescriptive guidelines for running large-scale AI/ML workloads, including Large Language and Generative AI models, on Google Cloud AI/ML infrastructure.</p> <ul> <li>TPU Training on GKE. This is a reference guide for executing large-scale training workloads on Cloud TPUs in Google Kubernetes Engine (GKE).</li> </ul>"},{"location":"ai-infrastructure/terraform-modules/bootstrap/","title":"Automation bootstrap","text":"<p>This Terraform module establishes the initial configuration of a Google Cloud project that requires elevated administrative permissions. Its primary objective is to set up Terraform and Cloud Build automation for subsequent provisioning tasks. The module enables the specified set of services and sets up an automation service account along with an automation GCS bucket. Optionally, the module can create a Google Cloud project.</p>"},{"location":"ai-infrastructure/terraform-modules/bootstrap/#examples","title":"Examples","text":"<pre><code>module \"automation_bootstrap\" {\n  source              = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/bootstrap\"\n  project_id          = \"project-id\" \n  automation_bucket   = {\n    name =     \"automation-bucket-name\"\n    location = \"us-central1\"\n  automation_sa_name  = \"service-account-name\"\n  services            = [\n    \"aiplatform.googleapis.com\"\n  ]\n  roles               = [\n    \"roles/aiplatform.user\"\n  ]\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/bootstrap/#impersonating-automation-service-account","title":"Impersonating automation service account","text":"<p>To be able to use the automation service account, the account that will be used to run Terraform commands in the other deployment stages needs to  have the <code>iam.serviceAccountTokenCreator</code> rights on the automation service account. You can grant this permission using the following command. Make sure to set the AUTOMATION_SERVICE_ACCOUNT and TERRAFORM_USER_ACCOUNT variables to the email addresses of the accounts in your environment.</p> <pre><code>AUTOMATION_SERVICE_ACCOUNT=you-automation-service-account-name@jk-mlops-dev.iam.gserviceaccount.com\nTERRAFORM_USER_ACCOUNT=your-terraform-user@foo.com\n\ngcloud iam service-accounts add-iam-policy-binding $AUTOMATION_SERVICE_ACCOUNT --member=\"user:$TERRAFORM_USER_ACCOUNT\" --role='roles/iam.serviceAccountTokenCreator'\n</code></pre> <p>If the impersonating account itself is a service account, such as the Cloud Build service account:</p> <pre><code>AUTOMATION_SERVICE_ACCOUNT=you-automation-service-account-name@jk-mlops-dev.iam.gserviceaccount.com\nTERRAFORM_USER_ACCOUNT=your-terraform-user@foo.com\n\ngcloud iam service-accounts add-iam-policy-binding $AUTOMATION_SERVICE_ACCOUNT --member=\"serviceAccount:$TERRAFORM_USER_ACCOUNT\" --role='roles/iam.serviceAccountTokenCreator'\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/bootstrap/#input-variables","title":"Input variables","text":"Name Description Type Required Default project_id The project ID, where to enable services and create an automation service account and an automation bucket <code>string</code> \u2713 deletion_protection Prevent Terraform from destroying the automation bucket. When this field is set, a terraform destroy or terraform apply that would delete the bucket will fail. <code>string</code> <code>true</code> create_automation_bucket Whether to create an automation bucket <code>bool</code> <code>true</code> automation_bucket Settings for the automation bucket <code>map(strings)</code> \u2713 create_automation_sa Whether to create an automation service account <code>bool</code> <code>true</code> automation_sa_name The name of the automation service account <code>string</code> \u2713 enable_apis Whether to enable services in the <code>services</code> variable <code>bool</code> <code>true</code> services The list of services to enable <code>list(strings)</code> \u2713 roles The list of roles to assign to the automation service account. These roles will only be assigned to a newly created account. If you are using an existing account, this list will be ignored <code>list(strings)</code> \u2713"},{"location":"ai-infrastructure/terraform-modules/bootstrap/#outputs","title":"Outputs","text":"Name Description automation_sa The email of the automation service account automation_gcs The name of the automation bucket <p>The module also creates two files in the <code>gs://&lt;AUTOMATION_BUCKET_NAME&gt;/providers</code></p> <ul> <li>the <code>providers.tf</code> file</li> </ul> <pre><code>provider \"google\" {\n  impersonate_service_account = \"automation-sa-name@project-id.iam.gserviceaccount.com\"\n}\nprovider \"google-beta\" {\n  impersonate_service_account = \"automation-sa-name@project-id.iam.gserviceaccount.com\"\n}\n</code></pre> <ul> <li>the <code>backend.tf</code> file</li> </ul> <pre><code>terraform {\n  backend \"gcs\" {\n    bucket                      = \"automation-bucket-name\"\n    impersonate_service_account = \"automation-sa-name@project-id.iam.gserviceaccount.com\"\n    # remove the newline between quotes and set the prefix to the folder for Terraform state\n    prefix = \"\n    \"\n  }\n}\n</code></pre> <p>You can utilize these files in the downstream Terraform stages to configure the management of Terraform state in Cloud Storage and enable Terraform impersonation.</p>"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/","title":"GKE for Large Model Training and Serving","text":"<p>This Terraform module configures a GKE-based infrastructure environment specifically designed for training and serving large and extremely large deep learning models, including the most recent Generative AI models.</p> <p>The central element of this environment is a VPC-native GKE Standard cluster.  Users of the module can decide whether to deploy the cluster within an existing VPC or create a new VPC specifically for the cluster. The cluster can be configured with multiple CPU, GPU or TPU node pools. The node pools use a custom service account. This service account can be an existing one or a newly created account.</p> <p>Beyond the cluster, users have the option to create additional services such as Artifact Registry or Cloud Storage buckets.</p> <p>The module carries out the following tasks: - If a reference to an existing VPC is not provided, it will create a network, a subnet, and IP ranges for GKE pods and services. - Optionally, it can provision Cloud NAT - If a reference to an existing service account is not provided, the module will create a new service account and assign it to a user-defined set of security roles. - Deploys a standard, VPC-native GKE cluster that is configured to utilize Workload Identity. - Creates a user defined number of CPU node pools - Creates a user defined number of TPU node pools - Creates a user defined number of GPU node pools - The node pools are configured to use a custom service account - Optionally, it can create an Artifact Registry. - Creates the specified number of user-defined Cloud Storage buckets.</p>"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#examples","title":"Examples","text":""},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#gke-tpu-training-environment","title":"GKE TPU training environment","text":"<p>This example demonstrates how to configure an environment optimized for executing large-scale training workloads on TPUs. In this sample, a new VPC, a new service account, and a new Artifact Registry are created. All resources are generated using default values for the majority of the settings.</p> <pre><code>module \"tpu-training-cluster\" {\n    source     = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/gke-aiml\n    project_id = \"project_id\"\n    region     = \"us-central2\"\n    vpc_config = {\n        network_name = \"gke-cluster-network\"\n        subnet_name  = \"gke-cluster-subnetwork\"\n    }\n    node_pool_sa = {\n        name = \"gke-node-pool-sa\"\n    }\n    cluster_config = {\n        name = \"gke-tpu-training-cluster\"\n    }\n    cpu_node_pools = {\n        default-cpu-node-pool = {\n            zones  = [\"us-central2-a\"]\n            labels = {\n                default-node-pool=true\n            }\n        }\n    }\n    tpu_node_pools = {\n        tpu-v4-16-podslice-1 = {\n            zones    = [\"us-central2-b\"]\n            tpu_type = \"v4-16\"\n        }\n        tpu-v4-16-podslice-2 = {\n           zones    = [\"us-central2-b\"]\n            tpu_type = \"v4-16\"\n        }\n    }\n    gcs_configs = {\n      training-artifacts-bucket = {} \n    }\n    registry_config = {\n        name     = \"training-images\"\n        location = \"us\"\n    }\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#gke-gpu-training-environment","title":"GKE GPU training environment","text":"<p>This example demonstrates how to configure an environment optimized for executing large-scale training workloads on GPUs. In this sample, a new VPC, a new service account, and a new Artifact Registry are created. All resources are generated using default values for the majority of the settings. You can use all the GPU machine types and accelerator types available to you. Those are the ones supported: GPU doc</p> <pre><code>module \"gpu-training-cluster\" {\n    source     = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/gke-aiml\n    project_id = \"project_id\"\n    region     = \"us-central1\"\n    vpc_config = {\n        network_name = \"gke-cluster-network\"\n        subnet_name  = \"gke-cluster-subnetwork\"\n    }\n    node_pool_sa = {\n        name = \"gke-node-pool-sa\"\n    }\n    cluster_config = {\n        name = \"gke-gpu-training-cluster\"\n    }\n    gpu_node_pools = {\n    l4-gpu-node-pool = {\n      zones          = [\"us-central1-a\"]\n      min_node_count = 1\n      max_node_count = 2\n      machine_type   = \"g2-standard-4\"\n      accelerator_type = \"nvidia-l4\"\n      accelerator_count=1\n      disk_size_gb   = 200\n      taints         = {}\n      labels         = {}\n    }\n  }\n    gcs_configs = {\n      training-artifacts-bucket = {} \n    }\n    registry_config = {\n        name     = \"training-images\"\n        location = \"us\"\n    }\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#variables","title":"Variables","text":"Name Description Type Required Default project_id Environment project ID <code>string</code> \u2713 region Environment region <code>string</code> \u2713 deletion_protection Prevent Terraform from destroying data storage resources (storage buckets, GKE clusters). When this field is set, a terraform destroy or terraform apply that would delete data storage resources will fail. <code>string</code> <code>true</code> cluster_config Cluster level configurations <code>object({...})</code> <code>{...}</code> vpc_config Network configurations of a VPC to create. Must be specified if vpc_reg is null <code>object({...})</code> <code>{...}</code> vpc_ref Settings for the  existing VPC to use for the environment. If null, a new VPC based on the <code>vpc_config</code> will be created <code>object({...})</code> <code>{...}</code> node_pool_sa Settings for a node pool service account <code>object({...})</code> <code>{...}</code> cpu_node_pools Settings for CPU node pools <code>map(object({...}))</code> <code>{...}</code> tpu_node_pools Settings for TPU node pools. See below for more information about TPU slice types <code>map(object({...}))</code> <code>{...}</code> gpu_node_pools Settings for GPU node pools <code>map(object({...}))</code> <code>{...}</code> gcs_configs Settings for Cloud Storage buckets <code>map(object({...}))</code> <code>{...}</code> registry_config Settings for Artifact Registry <code>object({...})</code> <code>{...}</code>"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#specifying-tpu-type","title":"Specifying TPU type","text":"<p>When configuring TPU node pools, ensure that you set the TPU type to one of the following values:</p> TPU type name Slice type Slice topology TPU VM type Number of VMs in a slice Number of chips in a VM v5litepod-1 tpu-v5-lite-podslice 1x1 ct5lp-hightpu-1 1 1 v5litepod-4 tpu-v5-lite-podslice 2x2 ct5lp-hightpu-4t 1 4 v5litepod-8 tpu-v5-lite-podslice 2x4 ct5lp-hightpu-4t 1 8 v5litepod-16 tpu-v5-lite-podslice 4x4 ct5lp-hightpu-4t 4 4 v5litepod-32 tpu-v5-lite-podslice 4x8 ct5lp-hightpu-4t 8 4 v5litepod-64 tpu-v5-lite-podslice 8x8 ct5lp-hightpu-4t 16 4 v5litepod-128 tpu-v5-lite-podslice 8x16 ct5lp-hightpu-4t 32 4 v5litepod-256 tpu-v5-lite-podslice 16x16 ct5lp-hightpu-4t 64 4 v4-8 tpu-v4-podslice 2x2x1 ct4p-hightpu-4t 1 4 v4-16 tpu-v4-podslice 2x2x2 ct4p-hightpu-4t 2 4 v4-32 tpu-v4-podslice 2x2x4 ct4p-hightpu-4t 4 4 v4-64 tpu-v4-podslice 2x4x4 ct4p-hightpu-4t 8 4 v4-128 tpu-v4-podslice 4x4x4 ct4p-hightpu-4t 16 4 v4-256 tpu-v4-podslice 4x4x8 ct4p-hightpu-4t 32 4 v4-512 tpu-v4-podslice 4x8x8 ct4p-hightpu-4t 64 4 v4-1024 tpu-v4-podslice 8x8x8 ct4p-hightpu-4t 128 4 v4-1536 tpu-v4-podslice 8x8x12 ct4p-hightpu-4t 192 4 v4-2048 tpu-v4-podslice 8x8x16 ct4p-hightpu-4t 256 4 v4-4096 tpu-v4-podslice 8x16x16 ct4p-hightpu-4t 512 4 v5p-8 tpu-v5p-slice 2x2x1 ct5p-hightpu-4t 1 4 v5p-16 tpu-v5p-slice 2x2x2 ct5p-hightpu-4t 2 4 v5p-32 tpu-v5p-slice 2x2x4 ct5p-hightpu-4t 4 4 v5p-64 tpu-v5p-slice 2x4x4 ct5p-hightpu-4t 8 4 v5p-128 tpu-v5p-slice 4x4x4 ct5p-hightpu-4t 16 4 v5p-256 tpu-v5p-slice 4x4x8 ct5p-hightpu-4t 32 4 v5p-384 tpu-v5p-slice 4x4x12 ct5p-hightpu-4t 48 4 v5p-512 tpu-v5p-slice 4x8x8 ct5p-hightpu-4t 64 4 v5p-640 tpu-v5p-slice 4x4x20 ct5p-hightpu-4t 80 4 v5p-768 tpu-v5p-slice 4x8x12 ct5p-hightpu-4t 96 4 v5p-896 tpu-v5p-slice 4x4x28 ct5p-hightpu-4t 112 4 v5p-1024 tpu-v5p-slice 8x8x8 ct5p-hightpu-4t 128 4 v5p-1152 tpu-v5p-slice 4x12x12 ct5p-hightpu-4t 144 4 v5p-1280 tpu-v5p-slice 4x8x20 ct5p-hightpu-4t 160 4 v5p-1408 tpu-v5p-slice 4x4x44 ct5p-hightpu-4t 176 4 v5p-1536 tpu-v5p-slice 8x8x12 ct5p-hightpu-4t 192 4 v5p-1664 tpu-v5p-slice 4x4x52 ct5p-hightpu-4t 208 4 v5p-1792 tpu-v5p-slice 4x8x28 ct5p-hightpu-4t 224 4 v5p-1920 tpu-v5p-slice 4x12x20 ct5p-hightpu-4t 240 4 v5p-2048 tpu-v5p-slice 8x8x16 ct5p-hightpu-4t 256 4 v5p-2176 tpu-v5p-slice 4x4x68 ct5p-hightpu-4t 272 4 v5p-2304 tpu-v5p-slice 8x12x12 ct5p-hightpu-4t 288 4 v5p-2432 tpu-v5p-slice 4x4x76 ct5p-hightpu-4t 304 4 v5p-2560 tpu-v5p-slice 8x8x20 ct5p-hightpu-4t 320 4 v5p-2688 tpu-v5p-slice 4x12x28 ct5p-hightpu-4t 336 4 v5p-2816 tpu-v5p-slice 4x8x44 ct5p-hightpu-4t 352 4 v5p-2944 tpu-v5p-slice 4x4x92 ct5p-hightpu-4t 368 4 v5p-3072 tpu-v5p-slice 4x12x16 ct5p-hightpu-4t 384 4 v5p-3200 tpu-v5p-slice 4x20x20 ct5p-hightpu-4t 400 4 v5p-3328 tpu-v5p-slice 4x8x52 ct5p-hightpu-4t 416 4 v5p-3456 tpu-v5p-slice 12x12x12 ct5p-hightpu-4t 432 4 v5p-3584 tpu-v5p-slice 8x8x28 ct5p-hightpu-4t 448 4 v5p-3712 tpu-v5p-slice 4x4x116 ct5p-hightpu-4t 464 4 v5p-3840 tpu-v5p-slice 8x12x20 ct5p-hightpu-4t 480 4 v5p-3968 tpu-v5p-slice 4x4x124 ct5p-hightpu-4t 496 4 v5p-4096 tpu-v5p-slice 8x16x16 ct5p-hightpu-4t 512 4 v5p-4224 tpu-v5p-slice 4x12x44 ct5p-hightpu-4t 528 4 v5p-4352 tpu-v5p-slice 4x8x68 ct5p-hightpu-4t 544 4 v5p-4480 tpu-v5p-slice 4x20x28 ct5p-hightpu-4t 560 4 v5p-4608 tpu-v5p-slice 12x12x16 ct5p-hightpu-4t 576 4 v5p-4736 tpu-v5p-slice 4x4x148 ct5p-hightpu-4t 592 4 v5p-4864 tpu-v5p-slice 4x8x76 ct5p-hightpu-4t 608 4 v5p-4992 tpu-v5p-slice 4x12x52 ct5p-hightpu-4t 624 4 v5p-5120 tpu-v5p-slice 8x16x20 ct5p-hightpu-4t 640 4 v5p-5248 tpu-v5p-slice 4x4x164 ct5p-hightpu-4t 656 4 v5p-5376 tpu-v5p-slice 8x12x28 ct5p-hightpu-4t 672 4 v5p-5504 tpu-v5p-slice 4x4x172 ct5p-hightpu-4t 688 4 v5p-5632 tpu-v5p-slice 8x8x44 ct5p-hightpu-4t 704 4 v5p-5760 tpu-v5p-slice 12x12x20 ct5p-hightpu-4t 720 4 v5p-5888 tpu-v5p-slice 4x8x92 ct5p-hightpu-4t 736 4 v5p-6016 tpu-v5p-slice 4x4x188 ct5p-hightpu-4t 752 4 v5p-6144 tpu-v5p-slice 12x16x16 ct5p-hightpu-4t 768 4 v5p-6272 tpu-v5p-slice 4x28x28 ct5p-hightpu-4t 784 4 v5p-6400 tpu-v5p-slice 8x20x20 ct5p-hightpu-4t 800 4 v5p-6528 tpu-v5p-slice 4x12x68 ct5p-hightpu-4t 816 4 v5p-6656 tpu-v5p-slice 8x8x52 ct5p-hightpu-4t 832 4 v5p-6784 tpu-v5p-slice 4x4x212 ct5p-hightpu-4t 848 4 v5p-6912 tpu-v5p-slice 12x12x24 ct5p-hightpu-4t 864 4 v5p-7040 tpu-v5p-slice 4x20x44 ct5p-hightpu-4t 880 4 v5p-7168 tpu-v5p-slice 8x16x28 ct5p-hightpu-4t 896 4 v5p-7296 tpu-v5p-slice 4x12x76 ct5p-hightpu-4t 912 4 v5p-7424 tpu-v5p-slice 4x8x116 ct5p-hightpu-4t 928 4 v5p-7552 tpu-v5p-slice 4x4x236 ct5p-hightpu-4t 944 4 v5p-7680 tpu-v5p-slice 12x16x20 ct5p-hightpu-4t 960 4 v5p-7808 tpu-v5p-slice 4x4x244 ct5p-hightpu-4t 976 4 v5p-7936 tpu-v5p-slice 4x8x124 ct5p-hightpu-4t 992 4 v5p-8064 tpu-v5p-slice 12x12x28 ct5p-hightpu-4t 1008 4 v5p-8192 tpu-v5p-slice 16x16x16 ct5p-hightpu-4t 1024 4 v5p-8320 tpu-v5p-slice 4x20x52 ct5p-hightpu-4t 1040 4 v5p-8448 tpu-v5p-slice 8x12x44 ct5p-hightpu-4t 1056 4 v5p-8704 tpu-v5p-slice 8x8x68 ct5p-hightpu-4t 1088 4 v5p-8832 tpu-v5p-slice 4x12x92 ct5p-hightpu-4t 1104 4 v5p-8960 tpu-v5p-slice 8x20x28 ct5p-hightpu-4t 1120 4 v5p-9216 tpu-v5p-slice 12x16x24 ct5p-hightpu-4t 1152 4 v5p-9472 tpu-v5p-slice 4x8x148 ct5p-hightpu-4t 1184 4 v5p-9600 tpu-v5p-slice 12x20x20 ct5p-hightpu-4t 1200 4 v5p-9728 tpu-v5p-slice 8x8x76 ct5p-hightpu-4t 1216 4 v5p-9856 tpu-v5p-slice 4x28x44 ct5p-hightpu-4t 1232 4 v5p-9984 tpu-v5p-slice 8x12x52 ct5p-hightpu-4t 1248 4 v5p-10240 tpu-v5p-slice 16x16x20 ct5p-hightpu-4t 1280 4 v5p-10368 tpu-v5p-slice 12x12x36 ct5p-hightpu-4t 1296 4 v5p-10496 tpu-v5p-slice 4x8x164 ct5p-hightpu-4t 1312 4 v5p-10752 tpu-v5p-slice 12x16x28 ct5p-hightpu-4t 1344 4 v5p-10880 tpu-v5p-slice 4x20x68 ct5p-hightpu-4t 1360 4 v5p-11008 tpu-v5p-slice 4x8x172 ct5p-hightpu-4t 1376 4 v5p-11136 tpu-v5p-slice 4x12x116 ct5p-hightpu-4t 1392 4 v5p-11264 tpu-v5p-slice 8x16x44 ct5p-hightpu-4t 1408 4 v5p-11520 tpu-v5p-slice 12x20x24 ct5p-hightpu-4t 1440 4 v5p-11648 tpu-v5p-slice 4x28x52 ct5p-hightpu-4t 1456 4 v5p-11776 tpu-v5p-slice 8x8x92 ct5p-hightpu-4t 1472 4 v5p-11904 tpu-v5p-slice 4x12x124 ct5p-hightpu-4t 1488 4 v5p-12032 tpu-v5p-slice 4x8x188 ct5p-hightpu-4t 1504 4 v5p-12160 tpu-v5p-slice 4x20x76 ct5p-hightpu-4t 1520 4 v5p-12288 tpu-v5p-slice 16x16x24 ct5p-hightpu-4t 1536 4 v5p-13824 tpu-v5p-slice 12x24x24 ct5p-hightpu-4t 1728 4 v5p-17920 tpu-v5p-slice 16x20x28 ct5p-hightpu-4t 2240 4"},{"location":"ai-infrastructure/terraform-modules/gke-aiml/#outputs","title":"Outputs","text":"Name Description node_pool_sa_email The email of the node pool sa cluster_name The name of the GKE cluster cluster_region The region of the GKE cluster gcs_buckets The names and locations of the created GCS buckets artifact_registry_id The full ID of the created Arifact Registry artifact_registry_image_path Artifact Registry path"},{"location":"ai-infrastructure/terraform-modules/kueue-config/","title":"Kueue Configuration","text":"<p>This Terraform module configures the Kueue resources in your GKE cluster.  Kueue is a set of APIs and controller for job queueing. It is a job-level manager that decides when a job should be admitted to start (as in pods can be created) and when it should stop (as in active pods should be deleted). </p> <p>The module configures Kueue with a single Cluster Queue and a single Local Queue. Additionally, it sets up a set of PriorityClass and Resource Flavor resources. Currently, the module configures Resource Flavors for common Cloud TPU v4, v5e, and v5p configurations.</p> <p>The module assumes that Kueue API has been already installed on the GKE cluster.</p>"},{"location":"ai-infrastructure/terraform-modules/kueue-config/#examples","title":"Examples","text":"<pre><code>module \"kueue\" {\n  source             = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/jobset-kueue\"\n  cluster_name       = \"gke-cluster\" \n  location           = \"us-central1\"\n  namespace          = \"tpu-training\"\n  cluster_queue_name = \"cluster-queue\"\n  local_queue_name   = \"local-queue\"\n  tpu_resources      = [\n    {\n        name      = \"v5litepod-16\",\n        num_chips = 32\n    },\n    {\n        name      = \"v5litepod-256\"\n        num_chips = 256\n    }\n  ] \n\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/kueue-config/#input-variables","title":"Input variables","text":"Name Description Type Required Default cluster_name The name of a GKE cluster <code>string</code> \u2713 location The location of a GKE cluster <code>string</code> \u2713 namespace The name of a Kubernetes namespace for the Local Queue <code>string</code> \u2713 cluster_queue_name The name of the Cluster Queue <code>string</code> \u2713 local_queue_name The name of the Local Queue <code>string</code> \u2713 tpu_resources The list of TPU resources available in the cluster. This list will be used to configure the <code>resourceGroups</code> section of the <code>ClusterQueue</code> resource <code>list(map)</code> \u2713 <p>The <code>name</code> field in the <code>tpu_resources</code> variable specifies a TPU slice type as defined in the table below. The <code>num_chips</code> field should be set to the total number of chips available for a given TPU slice type.</p> TPU type name Slice type Slice topology TPU VM type Number of VMs in a slice Number of chips in a VM v5litepod-1 tpu-v5-lite-podslice 1x1 ct5lp-hightpu-1 1 1 v5litepod-4 tpu-v5-lite-podslice 2x2 ct5lp-hightpu-4t 1 4 v5litepod-8 tpu-v5-lite-podslice 2x4 ct5lp-hightpu-4t 1 8 v5litepod-16 tpu-v5-lite-podslice 4x4 ct5lp-hightpu-4t 4 4 v5litepod-32 tpu-v5-lite-podslice 4x8 ct5lp-hightpu-4t 8 4 v5litepod-64 tpu-v5-lite-podslice 8x8 ct5lp-hightpu-4t 16 4 v5litepod-128 tpu-v5-lite-podslice 8x16 ct5lp-hightpu-4t 32 4 v5litepod-256 tpu-v5-lite-podslice 16x16 ct5lp-hightpu-4t 64 4 v4-8 tpu-v4-podslice 2x2x1 ct4p-hightpu-4t 1 4 v4-16 tpu-v4-podslice 2x2x2 ct4p-hightpu-4t 2 4 v4-32 tpu-v4-podslice 2x2x4 ct4p-hightpu-4t 4 4 v4-64 tpu-v4-podslice 2x4x4 ct4p-hightpu-4t 8 4 v4-128 tpu-v4-podslice 4x4x4 ct4p-hightpu-4t 16 4 v4-256 tpu-v4-podslice 4x4x8 ct4p-hightpu-4t 32 4 v4-512 tpu-v4-podslice 4x8x8 ct4p-hightpu-4t 64 4 v4-1024 tpu-v4-podslice 8x8x8 ct4p-hightpu-4t 128 4 v4-1536 tpu-v4-podslice 8x8x12 ct4p-hightpu-4t 192 4 v4-2048 tpu-v4-podslice 8x8x16 ct4p-hightpu-4t 256 4 v4-4096 tpu-v4-podslice 8x16x16 ct4p-hightpu-4t 512 4 v5p-8 tpu-v5p-slice 2x2x1 ct5p-hightpu-4t 1 4 v5p-16 tpu-v5p-slice 2x2x2 ct5p-hightpu-4t 2 4 v5p-32 tpu-v5p-slice 2x2x4 ct5p-hightpu-4t 4 4 v5p-64 tpu-v5p-slice 2x4x4 ct5p-hightpu-4t 8 4 v5p-128 tpu-v5p-slice 4x4x4 ct5p-hightpu-4t 16 4 v5p-256 tpu-v5p-slice 4x4x8 ct5p-hightpu-4t 32 4 v5p-384 tpu-v5p-slice 4x4x12 ct5p-hightpu-4t 48 4 v5p-512 tpu-v5p-slice 4x8x8 ct5p-hightpu-4t 64 4 v5p-640 tpu-v5p-slice 4x4x20 ct5p-hightpu-4t 80 4 v5p-768 tpu-v5p-slice 4x8x12 ct5p-hightpu-4t 96 4 v5p-896 tpu-v5p-slice 4x4x28 ct5p-hightpu-4t 112 4 v5p-1024 tpu-v5p-slice 8x8x8 ct5p-hightpu-4t 128 4 v5p-1152 tpu-v5p-slice 4x12x12 ct5p-hightpu-4t 144 4 v5p-1280 tpu-v5p-slice 4x8x20 ct5p-hightpu-4t 160 4 v5p-1408 tpu-v5p-slice 4x4x44 ct5p-hightpu-4t 176 4 v5p-1536 tpu-v5p-slice 8x8x12 ct5p-hightpu-4t 192 4 v5p-1664 tpu-v5p-slice 4x4x52 ct5p-hightpu-4t 208 4 v5p-1792 tpu-v5p-slice 4x8x28 ct5p-hightpu-4t 224 4 v5p-1920 tpu-v5p-slice 4x12x20 ct5p-hightpu-4t 240 4 v5p-2048 tpu-v5p-slice 8x8x16 ct5p-hightpu-4t 256 4 v5p-2176 tpu-v5p-slice 4x4x68 ct5p-hightpu-4t 272 4 v5p-2304 tpu-v5p-slice 8x12x12 ct5p-hightpu-4t 288 4 v5p-2432 tpu-v5p-slice 4x4x76 ct5p-hightpu-4t 304 4 v5p-2560 tpu-v5p-slice 8x8x20 ct5p-hightpu-4t 320 4 v5p-2688 tpu-v5p-slice 4x12x28 ct5p-hightpu-4t 336 4 v5p-2816 tpu-v5p-slice 4x8x44 ct5p-hightpu-4t 352 4 v5p-2944 tpu-v5p-slice 4x4x92 ct5p-hightpu-4t 368 4 v5p-3072 tpu-v5p-slice 4x12x16 ct5p-hightpu-4t 384 4 v5p-3200 tpu-v5p-slice 4x20x20 ct5p-hightpu-4t 400 4 v5p-3328 tpu-v5p-slice 4x8x52 ct5p-hightpu-4t 416 4 v5p-3456 tpu-v5p-slice 12x12x12 ct5p-hightpu-4t 432 4 v5p-3584 tpu-v5p-slice 8x8x28 ct5p-hightpu-4t 448 4 v5p-3712 tpu-v5p-slice 4x4x116 ct5p-hightpu-4t 464 4 v5p-3840 tpu-v5p-slice 8x12x20 ct5p-hightpu-4t 480 4 v5p-3968 tpu-v5p-slice 4x4x124 ct5p-hightpu-4t 496 4 v5p-4096 tpu-v5p-slice 8x16x16 ct5p-hightpu-4t 512 4 v5p-4224 tpu-v5p-slice 4x12x44 ct5p-hightpu-4t 528 4 v5p-4352 tpu-v5p-slice 4x8x68 ct5p-hightpu-4t 544 4 v5p-4480 tpu-v5p-slice 4x20x28 ct5p-hightpu-4t 560 4 v5p-4608 tpu-v5p-slice 12x12x16 ct5p-hightpu-4t 576 4 v5p-4736 tpu-v5p-slice 4x4x148 ct5p-hightpu-4t 592 4 v5p-4864 tpu-v5p-slice 4x8x76 ct5p-hightpu-4t 608 4 v5p-4992 tpu-v5p-slice 4x12x52 ct5p-hightpu-4t 624 4 v5p-5120 tpu-v5p-slice 8x16x20 ct5p-hightpu-4t 640 4 v5p-5248 tpu-v5p-slice 4x4x164 ct5p-hightpu-4t 656 4 v5p-5376 tpu-v5p-slice 8x12x28 ct5p-hightpu-4t 672 4 v5p-5504 tpu-v5p-slice 4x4x172 ct5p-hightpu-4t 688 4 v5p-5632 tpu-v5p-slice 8x8x44 ct5p-hightpu-4t 704 4 v5p-5760 tpu-v5p-slice 12x12x20 ct5p-hightpu-4t 720 4 v5p-5888 tpu-v5p-slice 4x8x92 ct5p-hightpu-4t 736 4 v5p-6016 tpu-v5p-slice 4x4x188 ct5p-hightpu-4t 752 4 v5p-6144 tpu-v5p-slice 12x16x16 ct5p-hightpu-4t 768 4 v5p-6272 tpu-v5p-slice 4x28x28 ct5p-hightpu-4t 784 4 v5p-6400 tpu-v5p-slice 8x20x20 ct5p-hightpu-4t 800 4 v5p-6528 tpu-v5p-slice 4x12x68 ct5p-hightpu-4t 816 4 v5p-6656 tpu-v5p-slice 8x8x52 ct5p-hightpu-4t 832 4 v5p-6784 tpu-v5p-slice 4x4x212 ct5p-hightpu-4t 848 4 v5p-6912 tpu-v5p-slice 12x12x24 ct5p-hightpu-4t 864 4 v5p-7040 tpu-v5p-slice 4x20x44 ct5p-hightpu-4t 880 4 v5p-7168 tpu-v5p-slice 8x16x28 ct5p-hightpu-4t 896 4 v5p-7296 tpu-v5p-slice 4x12x76 ct5p-hightpu-4t 912 4 v5p-7424 tpu-v5p-slice 4x8x116 ct5p-hightpu-4t 928 4 v5p-7552 tpu-v5p-slice 4x4x236 ct5p-hightpu-4t 944 4 v5p-7680 tpu-v5p-slice 12x16x20 ct5p-hightpu-4t 960 4 v5p-7808 tpu-v5p-slice 4x4x244 ct5p-hightpu-4t 976 4 v5p-7936 tpu-v5p-slice 4x8x124 ct5p-hightpu-4t 992 4 v5p-8064 tpu-v5p-slice 12x12x28 ct5p-hightpu-4t 1008 4 v5p-8192 tpu-v5p-slice 16x16x16 ct5p-hightpu-4t 1024 4 v5p-8320 tpu-v5p-slice 4x20x52 ct5p-hightpu-4t 1040 4 v5p-8448 tpu-v5p-slice 8x12x44 ct5p-hightpu-4t 1056 4 v5p-8704 tpu-v5p-slice 8x8x68 ct5p-hightpu-4t 1088 4 v5p-8832 tpu-v5p-slice 4x12x92 ct5p-hightpu-4t 1104 4 v5p-8960 tpu-v5p-slice 8x20x28 ct5p-hightpu-4t 1120 4 v5p-9216 tpu-v5p-slice 12x16x24 ct5p-hightpu-4t 1152 4 v5p-9472 tpu-v5p-slice 4x8x148 ct5p-hightpu-4t 1184 4 v5p-9600 tpu-v5p-slice 12x20x20 ct5p-hightpu-4t 1200 4 v5p-9728 tpu-v5p-slice 8x8x76 ct5p-hightpu-4t 1216 4 v5p-9856 tpu-v5p-slice 4x28x44 ct5p-hightpu-4t 1232 4 v5p-9984 tpu-v5p-slice 8x12x52 ct5p-hightpu-4t 1248 4 v5p-10240 tpu-v5p-slice 16x16x20 ct5p-hightpu-4t 1280 4 v5p-10368 tpu-v5p-slice 12x12x36 ct5p-hightpu-4t 1296 4 v5p-10496 tpu-v5p-slice 4x8x164 ct5p-hightpu-4t 1312 4 v5p-10752 tpu-v5p-slice 12x16x28 ct5p-hightpu-4t 1344 4 v5p-10880 tpu-v5p-slice 4x20x68 ct5p-hightpu-4t 1360 4 v5p-11008 tpu-v5p-slice 4x8x172 ct5p-hightpu-4t 1376 4 v5p-11136 tpu-v5p-slice 4x12x116 ct5p-hightpu-4t 1392 4 v5p-11264 tpu-v5p-slice 8x16x44 ct5p-hightpu-4t 1408 4 v5p-11520 tpu-v5p-slice 12x20x24 ct5p-hightpu-4t 1440 4 v5p-11648 tpu-v5p-slice 4x28x52 ct5p-hightpu-4t 1456 4 v5p-11776 tpu-v5p-slice 8x8x92 ct5p-hightpu-4t 1472 4 v5p-11904 tpu-v5p-slice 4x12x124 ct5p-hightpu-4t 1488 4 v5p-12032 tpu-v5p-slice 4x8x188 ct5p-hightpu-4t 1504 4 v5p-12160 tpu-v5p-slice 4x20x76 ct5p-hightpu-4t 1520 4 v5p-12288 tpu-v5p-slice 16x16x24 ct5p-hightpu-4t 1536 4 v5p-13824 tpu-v5p-slice 12x24x24 ct5p-hightpu-4t 1728 4 v5p-17920 tpu-v5p-slice 16x20x28 ct5p-hightpu-4t 2240 4"},{"location":"ai-infrastructure/terraform-modules/kueue-config/#outputs","title":"Outputs","text":"<p>The module does not have any outputs</p>"},{"location":"ai-infrastructure/terraform-modules/metrics-tracking/","title":"Services for performance metrics tracking","text":"<p>This Terraform module creates and configures Pub/Sub and BigQuery services to facilitate the tracking of performance metrics during load testing. Load generation tools like Locust can be seamlessly integrated with the metrics tracking services by publishing Pub/Sub messages conforming to the message schema configured by the module. The content of these messages is stored and managed in BigQuery for subsequent reporting and analysis.</p>"},{"location":"ai-infrastructure/terraform-modules/metrics-tracking/#examples","title":"Examples","text":"<pre><code>module \"locust-tracking\" {\n    source     = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/metrics-tracking\n    project_id = \"project_id\"\n    pubsub_config = {\n       topic_name        = \"locust_pubsub_sink\"\n       subscription_name = \"locust_metrics_bq_subscription\"\n       schema_name       = \"locust_metrics_schema\"\n    }\n    bq_config = {\n        dataset_name = \"locust_metrics_dataset\"\n        location     = \"US\"\n        table_name   = \"locust_metrics_table\"\n    }\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/metrics-tracking/#variables","title":"Variables","text":"Name Description Type Required Default project_id Project ID <code>string</code> \u2713 deletion_protection Prevent Terraform from destroying data storage Pubsub and BigQuery resources). When this field is set, a terraform destroy or terraform apply that would delete data storage resources will fail. <code>string</code> <code>true</code> pubsub_config Pubsub configuration settings <code>object({...})</code> \u2713 bq_config Bigquery configuration settings <code>object({...})</code> \u2713"},{"location":"ai-infrastructure/terraform-modules/metrics-tracking/#outputs","title":"Outputs","text":"Name Description performance_metrics_dataset_id The ID of a BigQuery dataset performance_metrics_table_id The ID of a BigQuery table perforamance_metrics_topic_name The fully qualified name of the Pubsub topic performance_metrics_bq_subscription The fully qualified name of the Pubsub BigQuery subscription"},{"location":"ai-infrastructure/terraform-modules/workload-identity/","title":"Workload Identity Configuration","text":"<p>This Terraform module configures workload identity federation for Google Kubernetes Engine (GKE). The module offers the flexibility to utilize an existing IAM service account, Kubernetes service account, and Kubernetes namespace, or create new ones as needed.</p>"},{"location":"ai-infrastructure/terraform-modules/workload-identity/#examples","title":"Examples","text":"<pre><code>module \"wid\" {\n  source       = \"github.com/GoogleCloudPlatform/applied-ai-engineering-samples//ai-infrastructure/terraform-modules/workload-identity\"\n  cluster_name = \"gke-cluster\" \n  location     = \"us-central1\"\n  project_id   = \"project-id\"\n  wid_sa_name  = \"iam-wid-sa\"\n  wid_sa_roles = [\"storage.objectAdmin\", \"logging.logWriter\"]]\n  ksa_name     = \"wid-ksa\"\n  namespace    = \"wid-namespace\"\n\n}\n</code></pre>"},{"location":"ai-infrastructure/terraform-modules/workload-identity/#input-variables","title":"Input variables","text":"Name Description Type Required Default project_id The project ID <code>string</code> \u2713 cluster_name The name of a GKE cluster <code>string</code> \u2713 location The location of a GKE cluster <code>string</code> \u2713 namespace The name of a Kubernetes namespace <code>string</code> \u2713 namespace_create Whether to create a new namspace <code>bool</code> <code>true</code> ksa_name The name of a Kubernetes service account <code>string</code> \u2713 kubernetes_service_account_create Whether to create a new Kubernetes service account <code>bool</code> <code>true</code> wid_sa_name The name of an IAM service account <code>string</code> \u2713 wid_sa_roles The list of IAM roles to assign to the IAM service account <code>list(strings)</code> \u2713 google_service_account_create Whether to create a new IAM service account <code>bool</code> <code>true</code>"},{"location":"ai-infrastructure/terraform-modules/workload-identity/#outputs","title":"Outputs","text":"Name Description wid_sa_email The email of the IAM  service account wid_sa_name The name of the IAM service account namespace The name of the Kubernetes namespace ksa_name The name of the Kubernetes service account created_resources The IDs of newly created resources"},{"location":"ai-infrastructure/tpu-training-on-gke/","title":"Running TPU training workloads on GKE","text":"<p>This reference guide compiles best practices, prescriptive guidance, and code samples for running large-scale machine learning training workloads with TPU v4, TPU v5p, and TPU v5e on Google Kubernetes Engine (GKE).</p> <p>The guide covers two main topics: - Configuring a GKE based environment for large scale training on Cloud TPUs   - This section describes how to configure a GKE cluster to optimize it for running large-scale machine learning training workloads on Cloud TPUs. - Defining, Submitting, and Monitoring Training Jobs   - This section provides guidance on how to define, submit, and manage training jobs using the Kubernetes JobSet and Kueue APIs.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#architecture-of-the-training-environment","title":"Architecture of the training environment","text":"<p>The diagram below depicts a high-level architecture of the training environment.</p> <p></p> <p>The foundation of the environment is a regional, VPC-native GKE cluster. The cluster has two types of node pools:  - A single node pool with CPU-only nodes and  - Several TPU node pools</p> <p>This cluster topology supports running both single-slice and multislice TPU training jobs.</p> <p>Following are the components supporting the environment:</p> <ul> <li>Cloud Storage buckets for saving training datasets and artifacts produced by training jobs (such as logs and checkpoints)</li> <li>Cloud Artifact Registry for packaging and managing the training, data processing, and other components of a training workload as Docker container images.</li> <li>Vertex AI TensorBoard for tracking and visualizing training metrics.</li> <li>Cloud Monitoring for collecting and analyzing non-functional performance metrics</li> <li>Cloud Logging for managing logs produced by training workloads.</li> <li>Training workloads impersonate an Identity and Access Management (IAM) service accounts to access Google Cloud services, such as Cloud Storage and Vertex AI TensorBoard.</li> </ul>"},{"location":"ai-infrastructure/tpu-training-on-gke/#training-workload-processing","title":"Training workload processing","text":"<p>The following diagram illustrates the process of submitting and processing training workloads in the training environment.</p> <p></p> <p>In this guide we advocate using the Kubernetes JobSet API as the preferred method of coordinating large-scale distributed machine learning training workloads on Kubernetes. When combined with the Kubernetes Kueue job queuing API, it provides flexible and comprehensive training job orchestration.</p> <p>The training environment's Kueue configuration  consists of a single ClusterQueue and multiple LocalQueues. This topology provides basic multi-tenancy and supports managing and prioritizing jobs submitted by multiple teams.</p> <p>All training workloads are represented as JobSet resources. A JobSet resource may contain multiple job types, such as a core distributed training job and an auxiliary job that manages TensorBoard logs and other artifacts generated by the training job.</p> <p>JobSet workloads are submitted to a namespaced LocalQueue that points to a ClusterQueue. As illustrated in the diagram, in our reference implementation, there is a single cluster queue.</p> <p>Kueue monitors when resources (such as TPU slices) required by a workload (JobSet) are available, and then decides when to admit the workload and how to allocate the workload's components to the cluster's node pools. </p> <p>For example, a training workload can contain two types of jobs: - A multislice distributed training job - A job that uploads TensorBoard logs generated by the training job to Vertex AI TensorBoard</p> <p>When all the resources required by this workload become available, the training job's workers are started on the requested number of TPU slices. The TensorBoard uploader is started on one of the nodes in the CPU node pool.</p> <p>If the compute resources required by other submitted workloads are not available, these workloads are queued and scheduled for admission based on the priorities that have been defined in the Kueue configuration.</p> <p>To submit a JobSet-defined workload, you need to create a YAML JobSet resource definition. There are a few different ways to do this. In this guide, we demonstrate two approaches: - Using Kustomize, which helps you create YAML JobSet resource definitions directly. - Using  xpk, which provides an easy-to-use Python-based CLI.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#setup","title":"Setup","text":"<p>The deployment process is automated using Cloud Build, Terraform, and Kustomize. The Cloud Build configuration file  defines two deployment stages:</p> <p>In the first stage a Terraform configuration is applied, which:</p> <ul> <li> Creates a network, a subnet, and IP ranges for GKE pods and services.</li> <li> Creates a VPC-native cluster.</li> <li> Creates a node pool with nodes equipped with CPUs only.</li> <li> Creates a specified number of TPU node pools.</li> <li> Creates an IAM service account for Workload Identity and an IAM service account to be used as a custom node pool service account.</li> <li> Configures the cluster for Workload Identity.</li> <li> Creates a Google Cloud Storage bucket.</li> <li> Creates a Vertex TensorBoard instance</li> <li> Creates an Artifact Registry</li> </ul> <p>In the second stage, the JobSet and Kueue custom resources are installed and Kueue is configured as described in the previous section. </p> <p>[!WARNING]  Your project must have sufficient quota to provision TPU resources. Else, you can request for a higher quota limit.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#configure-pre-requisites","title":"Configure pre-requisites","text":"<p>Before submitting the Cloud Build build, you need to:</p> <ul> <li> Create a new Google Cloud project or select an existing one.</li> <li> Enable the necessary services.</li> <li> Configure an automation service account and an automation Google Cloud storage bucket.</li> </ul> <p>The following services are required by the base environment: - <code>cloudbuild.googleapis.com</code> - <code>artifactregistry.googleapis.com</code> - <code>cloudkms.googleapis.com</code> - <code>cloudresourcemanager.googleapis.com</code> - <code>container.googleapis.com</code> - <code>compute.googleapis.com</code> - <code>container.googleapis.com</code> - <code>iam.googleapis.com</code> - <code>iamcredentials.googleapis.com</code> - <code>serviceusage.googleapis.com</code> - <code>stackdriver.googleapis.com</code> - <code>storage-component.googleapis.com</code> - <code>storage.googleapis.com</code> - <code>sts.googleapis.com</code> - <code>aiplatform.googleapis.com</code></p> <p>You also need a GCS bucket that will be used for managing Terraform state and other Terraform artifacts and a service account that will be impersonated by Terraform when provisioning the environment. The service account should have the following project level roles: - <code>iam.securityAdmin</code> - <code>iam.serviceAccountAdmin</code> - <code>compute.networkAdmin</code> - <code>container.admin</code> - <code>iam.serviceAccountUser</code> - <code>storage.admin</code> - <code>artifactregistry.admin</code> - <code>aiplatform.user</code> - <code>serviceusage.serviceUsageConsumer</code></p> <p>If you lack administrative-level permissions to enable GCP services or to create and configure service accounts in your project, your project administrator must perform these tasks. However, if you are a project owner, you can enable the services and create and configure the automation service account as part of the Configure automation settings step.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#configure-automation-settings","title":"Configure automation settings","text":"<p>During this step, Terraform is configured to utilize the specified automation bucket and service account. Optionally, if configured, it can also enable the necessary services and create both the automation service account and the automation bucket.</p> <ol> <li>Clone this repo</li> <li>Change the current folder to environment/0-bootstrap</li> <li>Copy the terraform.tfvars.tmpl file to <code>terraform.tfvars</code></li> <li>Modify the <code>terraform.tfvars</code> file to reflect your environment</li> <li><code>project_id</code> - your project ID</li> <li><code>deletion_protection</code> - Set to <code>true</code> to protect you cluster and GCS buckets from accidental deletion by Terraform apply/destroy commands. Unless this field is set to false, a terraform destroy or terraform apply that would delete the cluster or non-empty GCS buckets will fail.</li> <li><code>create_automation_bucket</code> - set to <code>true</code> if you want to create a new automation bucket; set to <code>false</code> if you want to use an existing bucket</li> <li><code>automation_bucket</code> - the name and location of a bucket you want to use for automation. If you use an existing bucket the <code>location</code> field will be ignored</li> <li><code>create_automation_sa</code> - set to <code>true</code> if you want to create a new automation service account; set to <code>false</code> if you want to use an existing service account</li> <li><code>automation_sa_name</code> - the name of an automation service account to be used by Terraform for impersonation</li> <li><code>enable_apis</code> - set to <code>true</code> if you want to enable the services listed in the <code>services</code> variable</li> <li><code>services</code> - the list of services to enable in your project</li> <li><code>roles</code> - the list of roles to assign to an automation services account. These roles will only be assigned to a newly created account. If you are using an existing account, this list will be ignored.</li> <li>Execute the <code>terraform init</code> command</li> <li>Execute the <code>terraform apply</code> command</li> </ol> <p>The Terraform configuration generates prepopulated template files for configuring the Terraform backend and providers, which can be utilized in the following setup stages. These template files are stored in the <code>gs://&lt;YOUR-AUTOMATION-BUCKET/providers</code> and <code>gs://&lt;YOUR-AUTOMATION-BUCKET/tfvars</code> folders. </p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#grant-cloud-build-impersonating-rights","title":"Grant Cloud Build impersonating rights","text":"<p>To be able to impersonate the automation service account, the Cloud Build service account needs to have the <code>iam.serviceAccountTokenCreator</code> rights on the automation service account.</p> <pre><code>AUTOMATION_SERVICE_ACCOUNT=&lt;AUTOMATTION_SERVICE_ACOUNT_EMAIL&gt;\nCLOUD_BUILD_SERVICE_ACCOUNT=&lt;PROJECT_NUMBER&gt;@cloudbuild.gserviceaccount.com\n\ngcloud iam service-accounts add-iam-policy-binding $AUTOMATION_SERVICE_ACCOUNT --member=\"serviceAccount:$CLOUD_BUILD_SERVICE_ACCOUNT\" --role='roles/iam.serviceAccountTokenCreator'\n</code></pre> <p>Replace  with your project number. Replace  with the email of your automation service account. If you created the automation service account using the bootstrap Terraform you can retrieve its email by executing the <code>terraform output automation_sa</code> command from the <code>environment\\0-bootstrap</code> folder."},{"location":"ai-infrastructure/tpu-training-on-gke/#deploy","title":"Deploy","text":""},{"location":"ai-infrastructure/tpu-training-on-gke/#clone-the-github-repo","title":"Clone the GitHub repo.","text":"<p>If you haven't already run the bootstrap stage, please clone this repository now.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git\n</code></pre> <p>Change the current directory, to <code>ai-infrastructure/tpu-training-on-gke/environment</code>.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#configure-build-parameters","title":"Configure build parameters","text":"<p>To configure the Terraform steps in the build, copy the terraform.tfvars.tmpl template file in the 1-base-infrastructure folder to <code>terraform.tfvars</code>. Make modifications to the <code>terraform.tfvars</code> file to align it with your specific environment. At the very least, you should set the following variables:</p> <ul> <li><code>project_id</code> - your project ID</li> <li><code>region</code> - your region for a VPC and a GKE cluster</li> <li><code>prefix</code> - the prefix that will be added to the default names of resources provisioned by the configuration</li> <li><code>tensorboard_config.region</code> - the region of a TensorBoard instance</li> <li><code>create_artifact_registry</code> - set to <code>true</code> to create a new artifact registry</li> <li><code>cpu_node_pools</code> - The <code>terraform.tfvars.tmpl</code> template provides an example configuration for a single autoscaling node pool.  </li> <li><code>tpu_node_pools</code> - The  template shows an example configuration for two TPU node pools: one with a single v5e-4 pod slice and the other with a single v5e-16 pod slice. Modify the <code>tpu_node_pools</code> variable to provision different TPU node pool configurations, as described below.</li> </ul> <p>If you wish to modify other default settings, such as the default name suffixes for a cluster or GCS bucket names, you can override the defaults specified in the variables.tf file within your <code>terraform.tfvars</code> file.</p> <p>When configuring TPU node pools, ensure that you set the TPU type to one of the following values:</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#tpu-types","title":"TPU types","text":"TPU type name Slice type Slice topology TPU VM type Number of VMs in a slice Number of chips in a VM v5litepod-4 tpu-v5-lite-podslice 2x2 ct5lp-hightpu-4t 1 4 v5litepod-16 tpu-v5-lite-podslice 4x4 ct5lp-hightpu-4t 4 4 v5litepod-32 tpu-v5-lite-podslice 4x8 ct5lp-hightpu-4t 8 4 v5litepod-64 tpu-v5-lite-podslice 8x8 ct5lp-hightpu-4t 16 4 v5litepod-128 tpu-v5-lite-podslice 8x16 ct5lp-hightpu-4t 32 4 v5litepod-256 tpu-v5-lite-podslice 16x16 ct5lp-hightpu-4t 64 4 v4-8 tpu-v4-podslice 2x2x1 ct4p-hightpu-4t 1 4 v4-16 tpu-v4-podslice 2x2x2 ct4p-hightpu-4t 2 4 v4-32 tpu-v4-podslice 2x2x4 ct4p-hightpu-4t 4 4 v4-64 tpu-v4-podslice 2x4x4 ct4p-hightpu-4t 8 4 v4-128 tpu-v4-podslice 4x4x4 ct4p-hightpu-4t 16 4 v4-256 tpu-v4-podslice 4x4x8 ct4p-hightpu-4t 32 4 v4-512 tpu-v4-podslice 4x8x8 ct4p-hightpu-4t 64 4 v4-1024 tpu-v4-podslice 8x8x8 ct4p-hightpu-4t 128 4 v4-1536 tpu-v4-podslice 8x8x12 ct4p-hightpu-4t 192 4 v4-2048 tpu-v4-podslice 8x8x16 ct4p-hightpu-4t 256 4 v4-4096 tpu-v4-podslice 8x16x16 ct4p-hightpu-4t 512 4 v5p-8 tpu-v5p-slice 2x2x1 ct5p-hightpu-4t 1 4 v5p-16 tpu-v5p-slice 2x2x2 ct5p-hightpu-4t 2 4 v5p-32 tpu-v5p-slice 2x2x4 ct5p-hightpu-4t 4 4 v5p-64 tpu-v5p-slice 2x4x4 ct5p-hightpu-4t 8 4 v5p-128 tpu-v5p-slice 4x4x4 ct5p-hightpu-4t 16 4 v5p-256 tpu-v5p-slice 4x4x8 ct5p-hightpu-4t 32 4 v5p-384 tpu-v5p-slice 4x4x12 ct5p-hightpu-4t 48 4 v5p-512 tpu-v5p-slice 4x8x8 ct5p-hightpu-4t 64 4 v5p-640 tpu-v5p-slice 4x4x20 ct5p-hightpu-4t 80 4 v5p-768 tpu-v5p-slice 4x8x12 ct5p-hightpu-4t 96 4 v5p-896 tpu-v5p-slice 4x4x28 ct5p-hightpu-4t 112 4 v5p-1024 tpu-v5p-slice 8x8x8 ct5p-hightpu-4t 128 4 v5p-1152 tpu-v5p-slice 4x12x12 ct5p-hightpu-4t 144 4 v5p-1280 tpu-v5p-slice 4x8x20 ct5p-hightpu-4t 160 4 v5p-1408 tpu-v5p-slice 4x4x44 ct5p-hightpu-4t 176 4 v5p-1536 tpu-v5p-slice 8x8x12 ct5p-hightpu-4t 192 4 v5p-1664 tpu-v5p-slice 4x4x52 ct5p-hightpu-4t 208 4 v5p-1792 tpu-v5p-slice 4x8x28 ct5p-hightpu-4t 224 4 v5p-1920 tpu-v5p-slice 4x12x20 ct5p-hightpu-4t 240 4 v5p-2048 tpu-v5p-slice 8x8x16 ct5p-hightpu-4t 256 4 v5p-2176 tpu-v5p-slice 4x4x68 ct5p-hightpu-4t 272 4 v5p-2304 tpu-v5p-slice 8x12x12 ct5p-hightpu-4t 288 4 v5p-2432 tpu-v5p-slice 4x4x76 ct5p-hightpu-4t 304 4 v5p-2560 tpu-v5p-slice 8x8x20 ct5p-hightpu-4t 320 4 v5p-2688 tpu-v5p-slice 4x12x28 ct5p-hightpu-4t 336 4 v5p-2816 tpu-v5p-slice 4x8x44 ct5p-hightpu-4t 352 4 v5p-2944 tpu-v5p-slice 4x4x92 ct5p-hightpu-4t 368 4 v5p-3072 tpu-v5p-slice 4x12x16 ct5p-hightpu-4t 384 4 v5p-3200 tpu-v5p-slice 4x20x20 ct5p-hightpu-4t 400 4 v5p-3328 tpu-v5p-slice 4x8x52 ct5p-hightpu-4t 416 4 v5p-3456 tpu-v5p-slice 12x12x12 ct5p-hightpu-4t 432 4 v5p-3584 tpu-v5p-slice 8x8x28 ct5p-hightpu-4t 448 4 v5p-3712 tpu-v5p-slice 4x4x116 ct5p-hightpu-4t 464 4 v5p-3840 tpu-v5p-slice 8x12x20 ct5p-hightpu-4t 480 4 v5p-3968 tpu-v5p-slice 4x4x124 ct5p-hightpu-4t 496 4 v5p-4096 tpu-v5p-slice 8x16x16 ct5p-hightpu-4t 512 4 v5p-4224 tpu-v5p-slice 4x12x44 ct5p-hightpu-4t 528 4 v5p-4352 tpu-v5p-slice 4x8x68 ct5p-hightpu-4t 544 4 v5p-4480 tpu-v5p-slice 4x20x28 ct5p-hightpu-4t 560 4 v5p-4608 tpu-v5p-slice 12x12x16 ct5p-hightpu-4t 576 4 v5p-4736 tpu-v5p-slice 4x4x148 ct5p-hightpu-4t 592 4 v5p-4864 tpu-v5p-slice 4x8x76 ct5p-hightpu-4t 608 4 v5p-4992 tpu-v5p-slice 4x12x52 ct5p-hightpu-4t 624 4 v5p-5120 tpu-v5p-slice 8x16x20 ct5p-hightpu-4t 640 4 v5p-5248 tpu-v5p-slice 4x4x164 ct5p-hightpu-4t 656 4 v5p-5376 tpu-v5p-slice 8x12x28 ct5p-hightpu-4t 672 4 v5p-5504 tpu-v5p-slice 4x4x172 ct5p-hightpu-4t 688 4 v5p-5632 tpu-v5p-slice 8x8x44 ct5p-hightpu-4t 704 4 v5p-5760 tpu-v5p-slice 12x12x20 ct5p-hightpu-4t 720 4 v5p-5888 tpu-v5p-slice 4x8x92 ct5p-hightpu-4t 736 4 v5p-6016 tpu-v5p-slice 4x4x188 ct5p-hightpu-4t 752 4 v5p-6144 tpu-v5p-slice 12x16x16 ct5p-hightpu-4t 768 4 v5p-6272 tpu-v5p-slice 4x28x28 ct5p-hightpu-4t 784 4 v5p-6400 tpu-v5p-slice 8x20x20 ct5p-hightpu-4t 800 4 v5p-6528 tpu-v5p-slice 4x12x68 ct5p-hightpu-4t 816 4 v5p-6656 tpu-v5p-slice 8x8x52 ct5p-hightpu-4t 832 4 v5p-6784 tpu-v5p-slice 4x4x212 ct5p-hightpu-4t 848 4 v5p-6912 tpu-v5p-slice 12x12x24 ct5p-hightpu-4t 864 4 v5p-7040 tpu-v5p-slice 4x20x44 ct5p-hightpu-4t 880 4 v5p-7168 tpu-v5p-slice 8x16x28 ct5p-hightpu-4t 896 4 v5p-7296 tpu-v5p-slice 4x12x76 ct5p-hightpu-4t 912 4 v5p-7424 tpu-v5p-slice 4x8x116 ct5p-hightpu-4t 928 4 v5p-7552 tpu-v5p-slice 4x4x236 ct5p-hightpu-4t 944 4 v5p-7680 tpu-v5p-slice 12x16x20 ct5p-hightpu-4t 960 4 v5p-7808 tpu-v5p-slice 4x4x244 ct5p-hightpu-4t 976 4 v5p-7936 tpu-v5p-slice 4x8x124 ct5p-hightpu-4t 992 4 v5p-8064 tpu-v5p-slice 12x12x28 ct5p-hightpu-4t 1008 4 v5p-8192 tpu-v5p-slice 16x16x16 ct5p-hightpu-4t 1024 4 v5p-8320 tpu-v5p-slice 4x20x52 ct5p-hightpu-4t 1040 4 v5p-8448 tpu-v5p-slice 8x12x44 ct5p-hightpu-4t 1056 4 v5p-8704 tpu-v5p-slice 8x8x68 ct5p-hightpu-4t 1088 4 v5p-8832 tpu-v5p-slice 4x12x92 ct5p-hightpu-4t 1104 4 v5p-8960 tpu-v5p-slice 8x20x28 ct5p-hightpu-4t 1120 4 v5p-9216 tpu-v5p-slice 12x16x24 ct5p-hightpu-4t 1152 4 v5p-9472 tpu-v5p-slice 4x8x148 ct5p-hightpu-4t 1184 4 v5p-9600 tpu-v5p-slice 12x20x20 ct5p-hightpu-4t 1200 4 v5p-9728 tpu-v5p-slice 8x8x76 ct5p-hightpu-4t 1216 4 v5p-9856 tpu-v5p-slice 4x28x44 ct5p-hightpu-4t 1232 4 v5p-9984 tpu-v5p-slice 8x12x52 ct5p-hightpu-4t 1248 4 v5p-10240 tpu-v5p-slice 16x16x20 ct5p-hightpu-4t 1280 4 v5p-10368 tpu-v5p-slice 12x12x36 ct5p-hightpu-4t 1296 4 v5p-10496 tpu-v5p-slice 4x8x164 ct5p-hightpu-4t 1312 4 v5p-10752 tpu-v5p-slice 12x16x28 ct5p-hightpu-4t 1344 4 v5p-10880 tpu-v5p-slice 4x20x68 ct5p-hightpu-4t 1360 4 v5p-11008 tpu-v5p-slice 4x8x172 ct5p-hightpu-4t 1376 4 v5p-11136 tpu-v5p-slice 4x12x116 ct5p-hightpu-4t 1392 4 v5p-11264 tpu-v5p-slice 8x16x44 ct5p-hightpu-4t 1408 4 v5p-11520 tpu-v5p-slice 12x20x24 ct5p-hightpu-4t 1440 4 v5p-11648 tpu-v5p-slice 4x28x52 ct5p-hightpu-4t 1456 4 v5p-11776 tpu-v5p-slice 8x8x92 ct5p-hightpu-4t 1472 4 v5p-11904 tpu-v5p-slice 4x12x124 ct5p-hightpu-4t 1488 4 v5p-12032 tpu-v5p-slice 4x8x188 ct5p-hightpu-4t 1504 4 v5p-12160 tpu-v5p-slice 4x20x76 ct5p-hightpu-4t 1520 4 v5p-12288 tpu-v5p-slice 16x16x24 ct5p-hightpu-4t 1536 4 v5p-13824 tpu-v5p-slice 12x24x24 ct5p-hightpu-4t 1728 4 v5p-17920 tpu-v5p-slice 16x20x28 ct5p-hightpu-4t 2240 4"},{"location":"ai-infrastructure/tpu-training-on-gke/#modify-workload-identity-and-kueue-configurations","title":"Modify Workload Identity and Kueue configurations","text":"<p>By default the following names and identifiers are used when configuring Workload Identity Federation and Kueue - The IAM service account for WID - <code>&lt;prefix&gt;-wid-sa</code> - The Kubernetes service account - <code>wid-ksa</code> - The Cluster Queue name - <code>cluster-queue</code> - The Local Queue name - <code>tpu-training-jobs</code> - The Namespace for WID Kubernetes accoutn and Local Queue - <code>tpu-training</code></p> <p>If you want to change these defaults, create a <code>terraform.tfvars</code> file in the <code>2-gke-config</code> and override the default values from the environment/2-gke-config/variables.tf file.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#submit-the-build","title":"Submit the build","text":"<p>To initiate the build, execute the following command:</p> <pre><code>export PROJECT_ID=&lt;PROJECT_ID&gt;\nexport AUTOMATION_BUCKET=&lt;YOUR_AUTOMATION_BUCKET&gt;\nexport AUTOMATION_ACCOUNT=&lt;YOUR_AUTOMATION_ACCOUNT&gt;\nexport ENV_NAME=&lt;ENV_STATE_FOLDER&gt; \nexport JOBSET_API_VERSION=v0.3.0\nexport KUEUE_API_VERSION=v0.5.3 \n\ngcloud builds submit \\\n  --project $PROJECT_ID \\\n  --config cloudbuild.provision.yaml \\\n  --substitutions _JOBSET_API_VERSION=$JOBSET_API_VERSION,_KUEUE_API_VERSION=$KUEUE_API_VERSION,_AUTOMATION_BUCKET=$AUTOMATION_BUCKET,_ENV_NAME=$ENV_NAME,_AUTOMATION_ACCOUNT=$AUTOMATION_ACCOUNT \\\n  --timeout \"2h\" \\\n  --machine-type=e2-highcpu-32 \n</code></pre> <p>Replace the following values: - <code>&lt;PROJECT_ID&gt;</code> with your project ID - <code>&lt;YOUR_AUTOMATION_BUCKET&gt;</code> with your automation bucket - <code>&lt;YOUR_AUTOMATION_ACCOUNT&gt;</code> with you automation service account - <code>&lt;ENV_STATE_FOLDER&gt;</code> with the name of the folder within your automation bucket where Terraform state and other artifacts will be managed</p> <p>The examples in this repo have been tested with <code>v0.4.0</code> version of the JobSet API and <code>v0.5.3</code> version of the Kueue API.</p> <p>To track the progress of the build, you can either follow the link displayed in Cloud Shell or visit the Cloud Build page on the Google Cloud Console.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#training-workloads-examples","title":"Training workloads examples","text":"<p>The <code>examples</code> folder contains code samples that demonstrate how to configure, submit and manage a number of different training workloads.</p> <p>Refer to the README in the <code>examples</code> folder for detailed instructions.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/#cleanup-environment","title":"Cleanup Environment","text":"<p>To destroy the environment and clean up all the provisioned resources:</p> <pre><code>export PROJECT_ID=&lt;PROJECT_ID&gt;\nexport AUTOMATION_BUCKET=&lt;YOUR_AUTOMATION_BUCKET&gt;\nexport ENV_NAME=&lt;TF_STATE_FOLDER&gt;\n\ngcloud builds submit \\\n  --project $PROJECT_ID \\\n  --config cloudbuild.destroy.yaml \\\n  --substitutions _AUTOMATION_BUCKET=$AUTOMATION_BUCKET,_ENV_NAME=$ENV_NAME \\\n  --timeout \"2h\" \\\n  --machine-type=e2-highcpu-32 \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/","title":"TPU training workloads examples","text":"<p>Before continuing with this guide, ensure you have provisioned the training environment as outlined in the environment setup. In this reference guide  we recommend using the JobSet and Kueue APIs as the preferred way to orchestrate large-scale distributed training workloads on GKE. You can create JobSet yaml configurations in a variety of ways. Our examples demonstrate two approaches: - Using Kustomize. Kustomize is a tool that streamlines and simplifies the creation and adaptation of complex configurations like JobSets. It provides robust configuration management and template-free customization. The examples of creating JobSet configurations using Kustomize are in the jobset folder. - Using xpk. xpk (Accelerated Processing Kit) is a Python-based tool that helps to orchestrate large-scale training jobs on GKE. xpk provides a simple command-line interface for managing GKE clusters and submitting training workloads that are encapsulated as JobSet configurations. In this reference guide, we do not use cluster management capabilities. We use xpk to configure and submit training workloads to the GKE-based training environment provisioned during the setup. The xpk examples are in the xpk folder.</p> <p>The examples are all based on the MaxText code base. MaxText is a high-performance, highly scalable, open-source LLM code base written in pure Python/Jax. It is optimized for Google Cloud TPUs and can achieve 55% to 60% MFU (model flops utilization). MaxText is designed to be a launching point for ambitious LLM projects in both research and production. It is also an excellent code base for demonstrating large-scale training design and operational patterns as attempted in this guide.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/#prerequisites-for-running-examples","title":"Prerequisites for running examples","text":""},{"location":"ai-infrastructure/tpu-training-on-gke/examples/#build-the-maxtext-container-image-and-download-training-datasets","title":"Build the MaxText container image and download training datasets","text":"<p>Before you can run the examples, you need to package MaxText in a training container image. You also need to copy the datasets required by the samples to your Cloud Storage  artifact repository. We have automated this process with Cloud Build. </p> <p>NOTE: Ensure you are working from  the <code>examples</code> directory</p> <pre><code>export PROJECT_ID=&lt;PROJECT_ID&gt;\nexport ARTIFACT_BUCKET=gs://&lt;ARTIFACT_BUCKET&gt;\nexport ARTIFACT_REGISTRY_PATH=&lt;ARTIFACT_REGISTRY_PATH&gt;\nexport AUTOMATION_ACCOUNT=&lt;AUTOMATION_SERVICE_ACCOUNT&gt;\nexport JAX_VERSION=NONE\nexport MODE=stable\n\ngcloud builds submit \\\n  --project $PROJECT_ID \\\n  --config build-images-datasets.yaml \\\n  --substitutions _ARTIFACT_BUCKET=$ARTIFACT_BUCKET,_ARTIFACT_REGISTRY_PATH=$ARTIFACT_REGISTRY_PATH,_AUTOMATION_ACCOUNT=$AUTOMATION_ACCOUNT,_JAX_VERSION=$JAX_VERSION,_MODE=$MODE \\\n  --machine-type=e2-highcpu-32 \\\n  --quiet\n</code></pre> <p>Replace the following values: - <code>&lt;PROJECT_ID&gt;</code> - your project ID. - <code>&lt;ARTIFACT_BUCKET&gt;</code> - the name of the Google Cloud Storage (GCS) bucket where you want to manage training artifacts like datasets and checkpoints. Recall that if you haven't made any changes to the defaults during the environment setup, the name should be <code>&lt;YOUR_PREFIX&gt;-artifact-repository</code>. - <code>&lt;ARTIFACT_REGISTRY_PATH&gt;</code> - the path to the Artifact Registry that you intend to use for pushing the Maxtext  container image. Keep in mind that the default path, as established during the setup process, is <code>us-docker.pkg.dev/&lt;YOUR_PROJECT_ID&gt;/&lt;YOUR_PREFIX&gt;-training-images</code>. If you made any modifications to these defaults, please make the necessary updates  accordingly. - <code>&lt;AUTOMATION_SERVICE_ACCOUNT&gt;</code> - your automation service account. Refer to the environment setup section. - By default, the MaxText image will be built with the default version of Jax. If you want to use a specific version, modifyt the <code>JAX_VERSION</code> setting.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/#set-up-your-development-environment","title":"Set up your development environment","text":"<p>Before you can run the examples, it's necessary to install the latest versions of Kustomize and xpk on your development workstation.</p> <ul> <li> <p>To install Kustomize, please follow the instructions in the Kustomize documentation. </p> </li> <li> <p>To install xpk</p> </li> </ul> <pre><code>pip install xpk\n</code></pre> <p>You also need to set credentials to your GKE cluster. <pre><code>gcloud container clusters get-credentials &lt;CLUSTER_NAME&gt; --region &lt;CLUSTER_REGION&gt;\n</code></pre></p> <p>Replace <code>&lt;CLUSTER_NAME&gt;</code> and <code>&lt;CLUSTER_REGION&gt;</code> to match your environment.</p> <p>[!NOTE] You may be prompted to to install the <code>gke-gcloud-auth-plugin</code> binary to use kubectl with the cluser. Run <code>gcloud components install gke-gcloud-auth-plugin</code> to install the plugin.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/#running-examples","title":"Running examples","text":"<p>For detailed instructions on running specific examples refer to README documents in the <code>jobset</code> and <code>xpk</code> folders.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/","title":"JobSet API Examples","text":"<p>This folder contains two sets of examples that demonstrate how to configure and execute training workloads using  the JobSet and Kueue APIs.</p> <ul> <li>THe <code>TPU Hello World</code> folder  offers examples for exploring different data and model parallelism strategies in both single-slice and multi-slice TPU configurations.</li> <li>The MaxText section provides examples of both single-slice and multi-slice pre-training for a MaxText model with 6.5 billion parameters.</li> </ul> <p>We utilize Kustomize to streamline the customization of JobSet resource YAML definitions. </p> <p>The base_jobset folder houses a Kustomize base for JobSet configurations. The tpu_hello_world and maxtext folders contain Kustomize overlays that adapt the base configuration for use with the TPU Hello World and MaxText examples, respectively.</p> <p>[!IMPORTANT]  When configuring the examples, you will need to substitute the placeholders with values that match your specific environment. This includes the name of the Kubernetes namespace, the Kubernetes service account for use with the Workload Identity, the Artifact Registry path, the name of a Google Cloud Storage (GCS) bucket, and the full path of a TensorBoard instance. Bear in mind that, unless you made changes to the defaults during the environment setup or if you didn't utilize the automated setup, these resources were created with the following names.</p> <ul> <li>Kubernetes namespace - <code>tpu-training</code></li> <li>Artifact Registry - <code>us-docker.pkg.dev/&lt;YOUR_PROJECT_ID&gt;/&lt;YOUR_PREFIX&gt;-training-images</code></li> <li>Cloud Storage bucket - <code>&lt;YOUR_PREFIX&gt;-artifact-repository</code> </li> <li>Kubernetes service account  - <code>wid-sa</code></li> <li>TensorBoard instance full name - The format should be - <code>projects/&lt;PROJECT_ID&gt;/locations/&lt;TENSORBOARD_REGION&gt;/tensorboard/&lt;TENSORBOARD_ID&gt;</code>. If you provisioned your environment using the automated setup, you can retrieve the TensorBoard name from the Terraform state, using the <code>terraform output tensorboard_id</code> command. You can also get the <code>&lt;TENSORBOARD_ID&gt;</code> from Vertex Experiments on the TensorBoard instances tab. The default display name for the TensorBoard instance created during the setup is <code>TPU Training</code>. </li> </ul>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#tpu-hello-world","title":"TPU Hello World","text":"<p>In the <code>tpu_hello_world</code> folder you will find examples of experimenting with different data and model parallelism strategies. The examples use the <code>shardings.py</code> script from MaxText that is designed to make experimentation with different parallelism options easy for both single slice and multislice settings. For more information about parallelism strategies and TPU Multislice refer to the Cloud TPU Multislice Overview article. </p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#configure-the-job","title":"Configure the job","text":"<p>Set the current folder to <code>tpu_hello_world</code></p> <pre><code>cd &lt;REPO_ROOT_DIR&gt;/ai-infrastructure/tpu-training-on-gke/examples/jobset/tpu_hello_world\n</code></pre> <p>Replace <code>&lt;REPO_ROOT_DIR&gt;</code> with the full path to the root of the cloned repo.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#update-namespace-images-and-job-suffix","title":"Update namespace, images, and job suffix","text":"<p>Remember to update the values in the <code>kustomization.yaml</code> file to align with your specific environment.</p> <p>Set the namespace:</p> <pre><code>kustomize edit set namespace &lt;NAMESPACE&gt;\n</code></pre> <p>Replace <code>&lt;NAMESPACE&gt;</code> with the name of the Kubernetes namespace that was created during the setup, where the Kueue local queue has been provisioned. </p> <p>Set the Maxtext container image:</p> <pre><code>kustomize edit set image python=&lt;ARTIFACT_REGISTRY_PATH&gt;/maxtext-runner:latest\n</code></pre> <p>Replace <code>&lt;ARTIFACT_REGISTRY_PATH&gt;</code> with the path to your Artifact Registry. </p> <p>Set the job ID suffix: </p> <pre><code>kustomize edit set namesuffix -- &lt;NAME_SUFFIX&gt;\n</code></pre> <p>Replace <code>&lt;NAME_SUFFIX&gt;</code> with the suffix that will be appended to the default job name, which is <code>tpu-helloworld</code>. You can utilize the name suffix to prevent naming conflicts between concurrent jobs or to maintain completed jobs for tracking purposes.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#configure-job-topology-and-shardingspy-parameters","title":"Configure job topology and <code>shardings.py</code> parameters","text":"<p>Create the <code>parameters.env</code> file with the following key-value settings:</p> <pre><code>TPU_SLICE_TYPE=&lt;TPU_SLICE_TYPE&gt; \nTPU_TOPOLOGY=&lt;TPU_TOPOLOGY&gt; \nLOCAL_QUEUE=&lt;LOCAL_QUEUE_NAME&gt;\nICI_PARALLELISM=&lt;ICI_PARALLELISM&gt;\nJOB_PARALLELISM=&lt;JOB_PARALLELISM&gt; \nNUM_SLICE=&lt;NUM_SLICES&gt;\n</code></pre> <p>Replace the following values: - <code>&lt;TPU_SLICE_TYPE&gt;</code> and <code>&lt;TPU_TOPOLOGY&gt;</code> with the type and topology of a TPU slice you want to run your job on. For TPU v4, use <code>tpu-v4-podslice</code> for <code>&lt;TPU_SLICE_TYPE&gt;</code>. For TPU v5e, use <code>tpu-v5-lite-podslice</code>. For TPU v5p, use <code>tpu-v5p-slice</code>. For TPU v4, define the topology in 3-tuples, for example <code>2x2x2</code>. For TPU v5e, define the topology in 2-tuples. For TPU v5p, define the topology in 3-tuples. Refer to TPU on GKE documentation for detailed information on TPU configurations. - <code>&lt;LOCAL_QUEUE_NAME&gt;</code> with the name of the  local Kueue queue in your namespace. Recall that the default name as created during the setup is <code>tpu-job-queue</code> - <code>&lt;ICI_PARALLELISM&gt;</code> with the value that is equal to the number of chips in the TPU slice  - <code>&lt;JOB_PARALLELISM&gt;</code> with the value that matches the number of TPU VMs in the TPU slice - <code>&lt;NUM_SLICES&gt;</code> with the number of TPU slices on which you want to run the training job. Make sure to have at least this number of TPU node pools in your environment.</p> <p>For your convenience, we have supplied two template files:</p> <ul> <li><code>parameters.env.single_slice</code> with example settings tailored for a single slice job on a TPU v5e-16 slice.</li> <li><code>parameters.env.multi_slice</code> with example settings configured for a multi-slice job spanning two TPU v5e-16 slices.</li> </ul>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#run-the-job","title":"Run the job","text":"<pre><code>kubectl apply -k . \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#monitor-jobs","title":"Monitor jobs","text":"<p>You can review execution logs using GKE Console or from the command line using <code>kubectl</code>.</p> <ul> <li> <p>To get the Kueue workloads: <pre><code>kubectl get workloads -n &lt;NAMESPACE&gt;\n</code></pre></p> </li> <li> <p>To get the JobSets: <pre><code>kubectl get jobsets -n &lt;NAMESPACE&gt;\n</code></pre></p> </li> <li> <p>To get pods in your namespace, including pods started by your workload: <pre><code>kubectl get pods -n &lt;NAMESPACE&gt;\n</code></pre></p> </li> </ul> <p>[!NOTE] If your workload failed than the above command will not return the workload's pods as the JobSet operator cleans up all failed jobs. If you want to review logs from the failed workload use GKE Console.</p> <ul> <li>To display logs for a pod: <pre><code>kubectl logs -f -n &lt;NAMESPACE&gt; &lt;YOUR POD&gt;\n</code></pre></li> </ul> <p>Once the job is completed successfully, you will see a message similar to the following: <pre><code>average time: 0.4840158, timings (seconds) [0.484098, 0.483838, 0.484114, 0.484056, 0.483973]\ntime is 0.4840158 seconds, TFLOP is 105.553116266496, TFLOP/s is 218.07783189411586\n</code></pre></p> <ul> <li>To remove your workload and all resources that it created execute: <pre><code>kubectl delete -k .\n</code></pre></li> </ul>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#maxtext-pre-training","title":"MaxText pre-training","text":"<p>The <code>maxtext</code> folder contains examples of pre-training a MaxText 8 billion parameters model on the English C4 dataset.</p> <p>The <code>maxtext/jobset-spec-patch.yaml</code> file includes overrides for the base JobSet configuration. This file configures a JobSet resource with two job templates: one named <code>slice</code> for starting the MaxText trainer and another named <code>tensorboard</code> for launching the TensorBoard uploader. </p> <p>The tensorboard job is responsible for uploading TensorBoard logs generated during the MaxText training job to a Vertex AI TensorBoard instance.</p> <p>Runtime parameters for both the MaxText trainer and the TensorBoard uploader are specified through environment variables set within the <code>maxtext-parameters</code> ConfigMap.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#configure-the-job_1","title":"Configure the job","text":"<p>Set the current folder to <code>maxtext</code></p> <pre><code>cd &lt;REPO_ROOT_DIR&gt;/ai-infrastructure/tpu-training-on-gke/examples/jobset/maxtext\n</code></pre> <p>Replace <code>&lt;REPO_ROOT_DIR&gt;</code> with the full path to the root of the cloned repo.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#update-namespace-images-and-job-suffix_1","title":"Update namespace, images, and job suffix","text":"<p>Remember to update the values in the <code>kustomization.yaml</code> file to align with your specific environment.</p> <p>Set the namespace:</p> <pre><code>kustomize edit set namespace &lt;NAMESPACE&gt;\n</code></pre> <p>Replace <code>&lt;NAMESPACE&gt;</code> with the name of the Kubernetes namespace that was created during the setup, where the Kueue local queue has been provisioned. </p> <p>Set the Maxtext container image:</p> <pre><code>kustomize edit set image maxtext-runner-image=&lt;ARTIFACT_REGISTRY_PATH&gt;/maxtext-runner:latest\n</code></pre> <p>Replace <code>&lt;ARTIFACT_REGISTRY_PATH&gt;</code> with the path to your Artifact Registry. </p> <p>Set the job ID suffix: </p> <pre><code>kustomize edit set namesuffix -- &lt;NAME_SUFFIX&gt;\n</code></pre> <p>Replace <code>&lt;NAME_SUFFIX&gt;</code> with the suffix that will be appended to the default job name, which is <code>maxtext-run</code>. You can utilize the name suffix to prevent naming conflicts between concurrent jobs or to maintain completed jobs for tracking purposes.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#configure-job-topology-and-maxtext-trainer-parameters","title":"Configure job topology and MaxText trainer parameters","text":"<p>Create the <code>parameters.env</code> file with the following key-value settings:</p> <pre><code>TPU_SLICE_TYPE=&lt;TPU_SLICE_TYPE&gt; \nTPU_TOPOLOGY=&lt;TPU_TOPOLOGY&gt; \nLOCAL_QUEUE=&lt;LOCAL_QUEUE_NAME&gt; \nICI_PARALLELISM=&lt;ICI_PARALLELISM&gt; \nJOB_PARALLELISM=&lt;JOB_PARALLELISM&gt; \nNUM_SLICES=&lt;NUM_SLICES&gt; \nBASE_OUTPUT_DIRECTORY=&lt;BASE_OUTPUT_DIRECTORY&gt; \nRUN_NAME=&lt;RUN_NAME&gt; \nTENSORBOARD_NAME=&lt;TENSORBOARD_NAME&gt; \nDATASET_PATH=&lt;DATASET_PATH&gt; \nARGS=&lt;ARGS&gt; \nLIBTPU_INIT_ARGS=&lt;LIBTPU_INIT_ARGS&gt;\n</code></pre> <p>Replace the following values: - <code>&lt;TPU_SLICE_TYPE&gt;</code> and <code>&lt;TPU_TOPOLOGY&gt;</code> with the type and topology of a TPU slice you want to run your job on. For TPU v4, use <code>tpu-v4-podslice</code> for <code>&lt;TPU_SLICE_TYPE&gt;</code>. For TPU v5e, use <code>tpu-v5-lite-podslice</code>. For TPU v5p, use <code>tpu-v5p-slice</code>. For TPU v4, define the topology in 3-tuples, for example <code>2x2x2</code>. For TPU v5e, define the topology in 2-tuples. For TPU v5p, define the topology in 3-tuples. Refer to TPU on GKE documentation for detailed information on TPU configurations. - <code>&lt;LOCAL_QUEUE_NAME&gt;</code> with the name of the Kueue local queue in your namespace. Recall that the default name as created during the setup is <code>tpu-job-queue</code> - <code>&lt;ICI_PARALLELISM&gt;</code> with the value that is equal to the number of chips in the TPU slice  - <code>&lt;JOB_PARALLELISM&gt;</code> with the value that matches the number of TPU VMs in the TPU slice - <code>&lt;NUM_SLICES&gt;</code> with the number of TPU slices on which you want to run the training job. Make sure to have at least this number of TPU node pools in your environment. - <code>&lt;BASE_OUTPUT_DIRECTORY&gt;</code> with the Cloud Storage location for checkpoints and logs. You can use the bucket created during the setup.  - <code>&lt;DATASET_PATH&gt;</code> with the Cloud Storage location of the C4 dataset. Specify the Cloud Storage location of the C4 dataset, excluding the <code>c4</code> folder name in the path. As part of the setup for the examples' prerequisites, the C4 dataset is copied to the <code>gs://&lt;ARTIFACT_BUCKET&gt;/datasets/c4</code> location. - <code>&lt;RUN_NAME&gt;</code> with the MaxText run name. MaxText will use this value to name the folders for checkpoints and TensorBoard logs in the <code>&lt;BASE_OUTPUT_DIRECTORY&gt;</code>. If you want to restart from a previously set checkpoint set this to the run name used for the previous run. Although not required it may be convenient to use the same name as the <code>&lt;NAME_SUFFIX&gt;</code>. - <code>&lt;TENSORBOARD_NAME&gt;</code> with the fully qualified name of the TensorBoardr instance to use for a training run tracking.  - <code>&lt;WID_KSA&gt;</code> with the name of Kubernetes service account to use for the Workload Identity.  - <code>&lt;ARGS&gt;</code> with any additional parameters you want to pass to the MaxText trainer. Refer to the below notes and the MaxText documentation for more info. - <code>&lt;LIBTPU_INIT_ARGS&gt;</code> with <code>libtpu</code> and XLA compiler settings. Refer to the below notes and the MaxText documentation for more info</p> <p>The MaxText trainer <code>MaxText/train.py</code> accepts a number of command line parameters that define a training regimen and model architecture. The required parameters are <code>run_name</code>, <code>base_output_directory</code>, and <code>dataset_path</code>. Other parameters are optional with the default values set in the MaxText config file. </p> <p>The necessary parameters are configured through the <code>RUN_NAME</code>, <code>BASE_OUTPUT_DIRECTORY</code>, and <code>DATASET_PATH</code> fields, while optional ones are set in the <code>ARGS</code> field within the <code>parameters.env</code> file.</p> <p>For both single slice and multi-slice job types, you can use the <code>ARGS</code> field to adjust training regimen parameters, including training steps, batch size, ICI  settings, DCN  parallelization settings, and parameters governing the model architecture.</p> <p>We've included example settings for a pretraining task for a ~8 billion parameter model on TPU v5e-16 pods. We also encourage you to experiment with your own settings. </p> <p>The example settings for a single slice training job are found in the <code>parameters.env.single_slice_8B</code> file, while the example settings for a multi-slice training job are provided in the <code>parameters.env.multi_slice_8B</code> file.</p> <p>[!WARNING] If you use the templates, do not forget to update them with the settings matching your environment.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#run-the-job_1","title":"Run the job","text":"<pre><code>kubectl apply -k . \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/jobset/#monitor-jobs_1","title":"Monitor jobs","text":"<p>You can monitor the runs using the techniques described in the <code>tpu_hello_world</code> section. Since both single slice and multislice workloads  upload TensorBoard metrics generated by the MaxText trainer to Vertex AI TensorBoard, you can also monitor the run - in real time - through Vertex Experiments. The experiment name that will receive the metrics is the same as the value configured in <code>RUN_NAME</code></p> <p></p> <ul> <li>To remove your workload and all resources that it created execute: <pre><code>kubectl delete -k . \n</code></pre></li> </ul>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/","title":"Running TPU workloads with xpk","text":"<p>xpk (Accelerated Processing Kit, pronounced x-p-k) is a Python based tool designed to help Cloud developers to orchestrate training jobs on accelerators such as TPUs and GPUs on GKE. </p> <p>There are two set of examples in this folder showing how to configure and run training workloads using xpk:</p> <ul> <li>Experimenting with different data and model parallelism strategies with in single slice and multislice TPU configurations.</li> <li>Pre-training a MaxText 6.5B parameter model in both single slice and multislice TPU configurations.</li> </ul> <p>xpk provides a simple command-line interface for managing GKE clusters and submitting training workloads that are encapsulated as JobSet resources. In this reference guide, we do not use its cluster management capabilities. We use xpk to configure and submit training workloads to the GKE-based training environment provisioned during the setup.</p> <p>Refer to the xpk documentation for detailed information on how to create, delete, and list workloads.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#setup","title":"Setup","text":""},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#install-xpk","title":"Install xpk","text":"<pre><code>pip install xpk\n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#update-kueue-configuration","title":"Update Kueue configuration","text":"<p>xpk uses JobSet and Kueue for running training workloads. It assumes that there is a LocalQueue named <code>multislice-queue</code> in the <code>default</code> namespace and submits workloads to this queue. </p> <p>If you employed the automated setup with the default settings, a local queue named <code>tpu-job-queue</code> was created within the <code>tpu-training</code> namespace. To use xpk with the default environment, you should create a new local queue in the <code>default</code> namespace.</p> <pre><code>cat &lt;&lt;EOF &gt;./local-queue.yaml\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: default \n  name: multislice-queue\nspec:\n  clusterQueue: cluster-queue \nEOF\n\nkubectl apply -f local-queue.yaml\n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#xpk-and-container-images","title":"xpk and container images","text":"<p>By default, when xpk prepares a workload it layers the local directory (<code>--script-dir</code>) into the base docker image, uploads the updated image to your project's Container Registry, and references the uploaded image in the JobSet template. You can specify the base docker image through the <code>--base-docker-image</code> parameter. If you do not specify the base image, xpk attempts to create one using the default settings embedded in <code>xpk.py</code>. xpk relies on the local installation of docker.</p> <p>If you don't want this layering behavior, you can specify the image to use through the <code>--docker-image</code> parameter.</p> <p>In our examples, we will set the <code>--base-docker-image</code> to the MaxText training image that was built as part of prerequisites for running examples. Make sure that you have a working installation of docker before running the below examples.</p> <p>Recall that if you utilized the automated setup with the default settings, the path to your Artifact Registry is:</p> <pre><code>us-docker.pkg.dev/&lt;YOUR_PROJECT_ID&gt;/&lt;YOUR_PREFIX&gt;-training-images\n</code></pre> <p>And the MaxText training image URI is:</p> <pre><code>us-docker.pkg.dev/&lt;YOUR_PROJECT_ID&gt;/&lt;YOUR_PREFIX&gt;-training-images/maxtext-runner:latest\n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#set-the-project-id-and-the-default-zone","title":"Set the project id and the default zone","text":"<p>The majority of xpk commands require the use of the <code>zone</code> parameter. xpk relies on the <code>zone</code> parameter to locate your clusters, even for regional clusters, as it derives the region information from the specified zone.</p> <p>If you have already configured the default zone and project ID for the Cloud SDK, there's no need to explicitly provide them when executing xpk commands.</p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\ngcloud config set compute/zone &lt;ZONE&gt;\n</code></pre> <p>Replace: - <code>&lt;PROJECT_ID&gt;</code> - With your project ID - <code>&lt;ZONE&gt;</code> - If your cluster is zonal, set it to your cluster's zone. However, if your cluster is regional, like the one provisioned by the automated setup, set it to one of the zones within the cluster's region where the node pools are provisioned.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#running-xpk-smoke-test","title":"Running xpk smoke test","text":"<p>To ensure that your setup is correct and that you can successfully run xpk workloads, we will submit a simple smoke test workload to your cluster.</p> <p>Set the current directory to:</p> <pre><code>cd &lt;REPO_ROOT_DIR&gt;/ai-infrastructure/tpu-training-on-gke/examples/xpk\n</code></pre> <p>Replace <code>&lt;REPO_ROOT_DIR&gt;</code> with the full path to the root of the cloned repo.</p> <p>Run the following command:</p> <pre><code>xpk workload create \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--base-docker-image &lt;MAX_TEXT_IMAGE_URI&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \\\n--tpu-type &lt;TPU_TYPE&gt; \\\n--command \"echo Hello World\" \n</code></pre> <p>Replace the following values: - <code>&lt;WORKLOAD_ID&gt;</code> - Choose a unique name for the workload. xpk will utilize this name when generating the name of a JobSet resource. - <code>&lt;MAX_TEXT_IMAGE_URI&gt;</code> - Set to the URI of the MaxText  container image. E.g. <code>us-docker.pkg.dev/&lt;YOUR_PROJECT_ID&gt;/&lt;YOUR_PREFIX&gt;-training-images/maxtext-runner:latest</code>  - <code>&lt;CLUSTER_NAME&gt;</code> - Replace with your cluster name - <code>&lt;TPU_TYPE&gt;</code> - Specify the TPU type of one of your TPU node pools. Note that xpk follows the same TPU type naming convention as used during the setup, and defined in the TPU Type table.</p> <p>In the command's output, you'll notice that xpk is constructing a container image by utilizing the MaxText image as its base and including the contents of the current directory within the image. After successfully building and pushing the image to the Artifact Registry, xpk proceeds to create and submit a JobSet workload. Additionally, it supplies a link to the GCP Console page, allowing you to monitor the workload. Note,  that you can also monitor the workload using standard <code>kubectl</code> commands.</p> <p>The last few lines printed by the command should look like that:</p> <pre><code>[XPK] Task: `Upload Docker Image` terminated with code `0`\n[XPK] Task: `Creating Workload` is implemented by `kubectl apply -f /tmp/tmpvxwfhxbm`, streaming output live.\n[XPK] Waiting for `Creating Workload`, for 0 seconds\njobset.jobset.x-k8s.io/test-workload-1 created\n[XPK] Task: `Creating Workload` terminated with code `0`\n[XPK] Follow your workload here: https://console.cloud.google.com/kubernetes/service/us-central2/gke-ml-cluster/default/test-workload-1/details?project=xxxx\n[XPK] Exiting XPK cleanly\n</code></pre> <p>To delete the smoke test workload execute:</p> <pre><code>xpk workload delete \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#running-sharding-experiments","title":"Running sharding experiments","text":"<p>In this section we provide instructions for running parallelism experiments similar to the <code>tpu_hello_world</code> examples in the <code>jobset</code> section.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#single-slice-ici-fsdp","title":"Single slice ICI FSDP","text":"<p>To run a configuration for a single slice workload with Interchip Interconnect (ICI) sharding using Fully Sharded Data Parallelism (FSDP), follow the steps below:</p> <ul> <li>Create a workload script. Make sure to modify the <code>--ici_fsdp_parallelism</code> parameter to match your TPU type. In the below example, the  <code>--ici_fsdp_parallelism=16</code>setting is configured for a TPU slice with 16 chips. E.g. v4-32, v5e-16 or v5p-32</li> </ul> <pre><code>cat &lt;&lt;EOF &gt;./ici-fsdp.sh\n#!/bin/bash\nset -e\n\npython3 pedagogical_examples/shardings.py --ici_fsdp_parallelism=16 --batch_size=131072 --embedding_dimension=2048\n\nEOF\n</code></pre> <ul> <li>Submit a workload</li> </ul> <pre><code>xpk workload create \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--base-docker-image &lt;MAX_TEXT_IMAGE_URI&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \\\n--tpu-type &lt;TPU_TYPE&gt;  \\\n--num-slices 1 \\\n--command \"bash ici-fsdp.sh\" \n</code></pre> <ul> <li>To delete the workload execute:</li> </ul> <pre><code>xpk workload delete \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#multislice-dcn-dp-and-ici-fsdp","title":"Multislice DCN DP and ICI FSDP","text":"<p>The below examples shows configuration for a multislice workload with data parallelism (DP) over data-center network (DCN) connections and FSDP over ICI.</p> <ul> <li>Create a workload script. Make sure to modify the <code>--ici_fsdp_parallelism</code> parameter to match your TPU type. </li> </ul> <pre><code>cat &lt;&lt;EOF &gt;./dcn-dp-ici-fsdp.sh\n#!/bin/bash\nset -e\n\npython3 pedagogical_examples/shardings.py --dcn_data_parallelism=2 --ici_fsdp_parallelism=16 --batch_size=131072 --embedding_dimension=2048\n\nEOF\n</code></pre> <ul> <li>Submit a workload</li> </ul> <pre><code>xpk workload create \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--base-docker-image &lt;MAX_TEXT_IMAGE_URI&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \\\n--tpu-type &lt;TPU_TYPE&gt;  \\\n--num-slices 2 \\\n--command \"bash dcn-dp-ici-fsdp.sh\" \n</code></pre> <ul> <li>To delete the workload execute:</li> </ul> <pre><code>xpk workload delete \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#running-maxtext-pretraining-workloads","title":"Running MaxText pretraining workloads","text":"<p>In this section we provide instructions for running MaxText pretraining for a 8B parameters model using the same configuration settings as in the <code>examples\\jobset\\maxtext</code>.</p>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#single-slice-pretraining","title":"Single slice pretraining","text":"<ul> <li>Create a workload script. </li> </ul> <p>[!IMPORTANT] Before executing the below command, replace the ,<code>&lt;RUN_NAME&gt;</code>, <code>&lt;DATASET_PATH&gt;</code>, <code>&lt;BASE_OUTPUT_DIRECTORY&gt;</code> placeholders with values reflecting your environment. Refer to the instructions for JobSet Maxtext examples for more information on how to set these parameters. Also, update the <code>ici_fsdp_parallelism</code> parameter to the number of chips in your TPU type.</p> <pre><code>cat &lt;&lt;EOF &gt;./single-slice-8b.sh\n#!/bin/bash\nset -e\n\nexport LIBTPU_INIT_ARGS=\"--xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_overlap_compute_collective_tc=true --xla_enable_async_all_gather=true\"\n\npython3 MaxText/train.py MaxText/configs/base.yml \\\nrun_name=&lt;RUN_NAME&gt; \\\ndataset_path=&lt;DATASET_PATH&gt; \\\nbase_output_directory=&lt;BASE_OUTPUT_DIRECTORY&gt; \\\nsteps=150 log_period=50 \\\nper_device_batch_size=6 global_parameter_scale=8 \\\nenable_checkpointing=false enable_profiler=false remat_policy=full \\\ndcn_data_parallelism=1 ici_fsdp_parallelism=16 \n\nEOF\n</code></pre> <ul> <li>Submit a workload</li> </ul> <pre><code>xpk workload create \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--base-docker-image &lt;MAX_TEXT_IMAGE_URI&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \\\n--tpu-type &lt;TPU_TYPE&gt;  \\\n--num-slices 1 \\\n--command \"bash single-slice-8b.sh\" \n</code></pre> <ul> <li>To delete the workload execute:</li> </ul> <pre><code>xpk workload delete \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \n</code></pre>"},{"location":"ai-infrastructure/tpu-training-on-gke/examples/xpk/#multislice-pretraining","title":"Multislice pretraining","text":"<ul> <li>Create a workload script. </li> </ul> <p>[!IMPORTANT] Before executing the below command, replace the ,<code>&lt;RUN_NAME&gt;</code>, <code>&lt;DATASET_PATH&gt;</code>, <code>&lt;BASE_OUTPUT_DIRECTORY&gt;</code> placeholders with values reflecting your environment. Refer to the instructions for JobSet Maxtext examples for more information on how to set these parameters. Also, update the <code>ici_fsdp_parallelism</code> parameter to the number of chips in your TPU type.</p> <pre><code>cat &lt;&lt;EOF &gt;./multi-slice-8b.sh\n#!/bin/bash\nset -e\n\nexport LIBTPU_INIT_ARGS=\"--xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_overlap_compute_collective_tc=true --xla_enable_async_all_gather=true\"\n\npython3 MaxText/train.py MaxText/configs/base.yml \\\nrun_name=&lt;RUN_NAME&gt; \\\ndataset_path=&lt;DATASET_PATH&gt; \\\nbase_output_directory=&lt;BASE_OUTPUT_DIRECTORY&gt; \\\nsteps=150 log_period=50 \\\nper_device_batch_size=6 global_parameter_scale=8 \\\nenable_checkpointing=false enable_profiler=false remat_policy=full \\\ndcn_data_parallelism=2 ici_fsdp_parallelism=16 \n\nEOF\n</code></pre> <ul> <li>Submit a workload</li> </ul> <pre><code>xpk workload create \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--base-docker-image &lt;MAX_TEXT_IMAGE_URI&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \\\n--tpu-type &lt;TPU_TYPE&gt;  \\\n--num-slices 2 \\\n--command \"bash multi-slice-8b.sh\" \n</code></pre> <ul> <li>To delete the workload execute:</li> </ul> <pre><code>xpk workload delete \\\n--workload &lt;WORKLOAD_ID&gt; \\\n--cluster &lt;CLUSTER_NAME&gt; \n</code></pre>"},{"location":"genai-on-vertex-ai/","title":"Generative AI on Vertex AI","text":"<p>This folder contains code samples and hands-on labs demonstrating the use of Generative AI models and tools in Vertex AI.</p> <ul> <li>Gemini: Consists of resources on how to use Gemini on Vertex AI in conjunction with other Vertex AI services for enterprise Generative AI use cases and solutions.</li> <li>Vertex AI LLM Evaluation Services: Demonstrate how to use Vertex AI LLM Evaluation Services in conjunction with other Vertex AI services. Additionally, we have provided notebooks that delve into the theory behind evaluation metrics.</li> <li>Vertex AI Search: A collection of notebooks, with varying levels of complexity, using Vertex AI Search.  The notebooks are aimed to serve as building blocks which can be combined together to achieve higher levels goals. </li> </ul>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/","title":"CrewAI on VertexAI Reasoning Engine","text":"Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <p>This demo uses the default crewai project skeleton template to allow the use of Gemini model.</p> <p>CrewAI is an open-source framework designed to make it easier to develop and manage applications that use multiple AI agents working together. Think of it like a team of specialized AI \"workers\" collaborating to achieve a common goal.</p> <p>At the time of the demo creation we used crewai version 0.63.6 and therefore some of the changes we mentioned might be outdate in future versions.</p> <p>We explicetly define library versions in order to avoid breaking this demo in the future.</p> <p>If you want to know more about starting a new CrewAI project from template look here: Starting Your CrewAI Project .</p> In\u00a0[\u00a0]: Copied! <pre>!pip install vertexai\n</pre> !pip install vertexai In\u00a0[\u00a0]: Copied! <pre>!pip install -q 'crewai[tools]==0.63.6' 'poetry'\n</pre> !pip install -q 'crewai[tools]==0.63.6' 'poetry' <p>Here you can define your CrewAI Project Name.</p> In\u00a0[\u00a0]: Copied! <pre>CREWAI_PROJECT_NAME = \"gcp_crewai\"   # @param {type:\"string\"}\n</pre> CREWAI_PROJECT_NAME = \"gcp_crewai\"   # @param {type:\"string\"} <p>Now lets create a crewai project. The code below makes sure it resets the directory where this notebook runs. Even though the first time running this notebook we will be in the notebooks current path, however in a cell below after we create the crewai project we get into our project directory once that is created.(i.e <code>CD CREWAI_PROJECT_NAME</code>). As a result future executions of this notebook need to reset to the default path.</p> In\u00a0[\u00a0]: Copied! <pre>HOME = get_ipython().getoutput('pwd')\nif (HOME[0].endswith(CREWAI_PROJECT_NAME)):\n  %cd ..\n  HOME = get_ipython().getoutput('pwd')\n\n!crewai create crew {CREWAI_PROJECT_NAME}\n</pre> HOME = get_ipython().getoutput('pwd') if (HOME[0].endswith(CREWAI_PROJECT_NAME)):   %cd ..   HOME = get_ipython().getoutput('pwd')  !crewai create crew {CREWAI_PROJECT_NAME} <p>Okey now that we created our crewai project lets switch directories and get into our project dir.</p> <p>p.s: You can see the created project folder in the file explorer on the left.</p> In\u00a0[\u00a0]: Copied! <pre>%cd {HOME[0]}/{CREWAI_PROJECT_NAME}\n!ls -la\n</pre> %cd {HOME[0]}/{CREWAI_PROJECT_NAME} !ls -la <p>The following command will install them in case they did not install during addition</p> In\u00a0[\u00a0]: Copied! <pre>!poetry install\n</pre> !poetry install In\u00a0[6]: Copied! <pre>PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\" # @param {type:\"string\"}\nSTAGING_BUCKET = \"gs://YOUR_STAGING_BUCKET_HERE\" # @param {type:\"string\"}\n</pre> PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"  # @param {type:\"string\"} LOCATION = \"us-central1\" # @param {type:\"string\"} STAGING_BUCKET = \"gs://YOUR_STAGING_BUCKET_HERE\" # @param {type:\"string\"} <p>Create the Bucket if it does not exist:</p> In\u00a0[\u00a0]: Copied! <pre>!set -x &amp;&amp; gsutil mb -p $PROJECT_ID -l $LOCATION $STAGING_BUCKET\n</pre> !set -x &amp;&amp; gsutil mb -p $PROJECT_ID -l $LOCATION $STAGING_BUCKET In\u00a0[\u00a0]: Copied! <pre># !gcloud auth login\n</pre> # !gcloud auth login In\u00a0[\u00a0]: Copied! <pre># Colab authentication - This is to authenticate colab to your account and project.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user(project_id=PROJECT_ID)\n    print(\"Authenticated\")\n</pre> # Colab authentication - This is to authenticate colab to your account and project. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user(project_id=PROJECT_ID)     print(\"Authenticated\") <p>Set model name according to litellm syntax.</p> In\u00a0[\u00a0]: Copied! <pre>MODEL_NAME = \"vertex_ai/gemini-2.0-flash-001\"  # @param {type:\"string\"}\n</pre> MODEL_NAME = \"vertex_ai/gemini-2.0-flash-001\"  # @param {type:\"string\"} <p>Now lets see how we can enable Gemini in a CrewAI project. CrewAI uses litellm and we can use the a vertex_ai model name for each of our agents. We need to edit Agent config to change default LLM to Vertex Gemini.</p> <p>here is an example:</p> <pre>reporting_analyst:\n  backstory: You're a meticulous analyst with a keen eye for detail. You're known\n    for your ability to turn complex data into clear and concise reports, making it\n    easy for others to understand and act on the information you provide.\n  goal: Create detailed reports based on {topic} data analysis and research findings\n  llm: vertex_ai/gemini-2.0-flash-001\n</pre> <p>We can define the LLM by editing the yaml in the editor, however we provide a script that does the same programatically.</p> <p>Feel free to inspect the file under CREWAI_PROJECT_NAME/src/CREWAI_PROJECT_NAME/config/agents.yaml before and after the execution of the cell below</p> In\u00a0[\u00a0]: Copied! <pre>import yaml\n\nagent_yaml = f\"./src/{CREWAI_PROJECT_NAME}/config/agents.yaml\"\n\nwith open(agent_yaml) as f:\n     agent_config = yaml.safe_load(f)\n\n# This loop removes additional new line characters in the end of a text value\nfor k,v  in agent_config.items():\n    for attribute,value in v.items():\n      if value.endswith(\"\\n\"):\n        v[attribute] = value[:-1]\n    # for each agent we add a key called llm and the model name of choice.\n    v['llm']=MODEL_NAME\n\nwith open(agent_yaml, \"w\") as f:\n    yaml.dump(agent_config, f)\n    print(f\"file {agent_yaml} successfully updated!\")\n</pre> import yaml  agent_yaml = f\"./src/{CREWAI_PROJECT_NAME}/config/agents.yaml\"  with open(agent_yaml) as f:      agent_config = yaml.safe_load(f)  # This loop removes additional new line characters in the end of a text value for k,v  in agent_config.items():     for attribute,value in v.items():       if value.endswith(\"\\n\"):         v[attribute] = value[:-1]     # for each agent we add a key called llm and the model name of choice.     v['llm']=MODEL_NAME  with open(agent_yaml, \"w\") as f:     yaml.dump(agent_config, f)     print(f\"file {agent_yaml} successfully updated!\") In\u00a0[\u00a0]: Copied! <pre>!poetry run {CREWAI_PROJECT_NAME}\n</pre> !poetry run {CREWAI_PROJECT_NAME} In\u00a0[\u00a0]: Copied! <pre>wrapper_file_content = (\"\"\"\nfrom src.{PROJECT_NAME}.crew import {CLASS_NAME}Crew as CrewProject\nfrom typing import Dict, List, Union\nimport vertexai\nimport os\n\nclass CrewAIApp:\n\n    def __init__(self, project: str, location: str) -&gt; None:\n        self.project_id = project\n        self.location = location\n\n    def set_up(self) -&gt; None:\n        os.environ['GOOGLE_CLOUD_PROJECT'] = self.project_id\n        return\n\n    def query(self, question: str) -&gt; Union[str, List[Union[str, Dict]]]:\n        res = CrewProject().crew().kickoff(inputs={{\"topic\": question}})\n        return res.__str__()\n\"\"\").format(PROJECT_NAME=CREWAI_PROJECT_NAME,\n            CLASS_NAME=''.join(word.title() for word in (CREWAI_PROJECT_NAME.split('_'))))\n\nwith open(f\"crew_ai_app.py\", \"w\") as f:\n    f.write(wrapper_file_content)\n</pre> wrapper_file_content = (\"\"\" from src.{PROJECT_NAME}.crew import {CLASS_NAME}Crew as CrewProject from typing import Dict, List, Union import vertexai import os  class CrewAIApp:      def __init__(self, project: str, location: str) -&gt; None:         self.project_id = project         self.location = location      def set_up(self) -&gt; None:         os.environ['GOOGLE_CLOUD_PROJECT'] = self.project_id         return      def query(self, question: str) -&gt; Union[str, List[Union[str, Dict]]]:         res = CrewProject().crew().kickoff(inputs={{\"topic\": question}})         return res.__str__() \"\"\").format(PROJECT_NAME=CREWAI_PROJECT_NAME,             CLASS_NAME=''.join(word.title() for word in (CREWAI_PROJECT_NAME.split('_'))))  with open(f\"crew_ai_app.py\", \"w\") as f:     f.write(wrapper_file_content)  In\u00a0[\u00a0]: Copied! <pre>from crew_ai_app import CrewAIApp\n\napp = CrewAIApp(project=PROJECT_ID, location=LOCATION)\napp.set_up()\nresponse_c = app.query(\"AI\")\n</pre> from crew_ai_app import CrewAIApp  app = CrewAIApp(project=PROJECT_ID, location=LOCATION) app.set_up() response_c = app.query(\"AI\") In\u00a0[\u00a0]: Copied! <pre>import vertexai\nfrom vertexai.preview import reasoning_engines\n\nvertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)\n</pre> import vertexai from vertexai.preview import reasoning_engines  vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET) <p>Lets see existing engines on our project</p> In\u00a0[\u00a0]: Copied! <pre>reasoning_engine_list = reasoning_engines.ReasoningEngine.list()\nprint(reasoning_engine_list)\n</pre> reasoning_engine_list = reasoning_engines.ReasoningEngine.list() print(reasoning_engine_list) <p>Reasoning engine instance needs to have the required libraries needed for crewai to execute successfully. As CrewAI uses poetry we will export the dependencies in a requirements.txt and process that to create the necessary reasoning engine requirements list</p> In\u00a0[\u00a0]: Copied! <pre>!poetry export --without-hashes --format=requirements.txt &gt; requirements.txt \\\n# &amp;&amp; pip install -r requirements.txt\n</pre> !poetry export --without-hashes --format=requirements.txt &gt; requirements.txt \\ # &amp;&amp; pip install -r requirements.txt In\u00a0[\u00a0]: Copied! <pre>with open('./requirements.txt') as f:\n    requirements = f.read().splitlines()\n</pre> with open('./requirements.txt') as f:     requirements = f.read().splitlines() In\u00a0[\u00a0]: Copied! <pre># Create a remote app with reasoning engine.\n# This may take few minutes to finish.\nfrom crew_ai_app import CrewAIApp\n\nreasoning_engine = reasoning_engines.ReasoningEngine.create(\n    CrewAIApp(project=PROJECT_ID, location=LOCATION),\n    display_name=\"Demo Addition App\",\n    description=\"A simple demo addition app\",\n    requirements=requirements,\n    extra_packages=['./src','./crew_ai_app.py'],\n)\n</pre> # Create a remote app with reasoning engine. # This may take few minutes to finish. from crew_ai_app import CrewAIApp  reasoning_engine = reasoning_engines.ReasoningEngine.create(     CrewAIApp(project=PROJECT_ID, location=LOCATION),     display_name=\"Demo Addition App\",     description=\"A simple demo addition app\",     requirements=requirements,     extra_packages=['./src','./crew_ai_app.py'], ) <p>Now the reasoning engine is deployed. You can access your reasoning engine in the future using the following reasource name:</p> In\u00a0[\u00a0]: Copied! <pre>print(reasoning_engine.resource_name)\n</pre> print(reasoning_engine.resource_name) <p>Test if our Crew on reasoning engine instance can respond. Let's get a report on Henry VIII. You can rerun the CrewAI with different topics to see how the Agents respond.</p> In\u00a0[\u00a0]: Copied! <pre>response = reasoning_engine.query(question=\"Henry VIII\")\nprint(response)\n</pre> response = reasoning_engine.query(question=\"Henry VIII\") print(response) In\u00a0[\u00a0]: Copied! <pre>#reasoning_engine.delete()\n</pre> #reasoning_engine.delete()"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#crewai-on-vertexai-reasoning-engine","title":"CrewAI on VertexAI Reasoning Engine\u00b6","text":"Author(s) Christos Aniftos Reviewer(s) Sokratis Kartakis Last updated 2024 11 14"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#installing-dependencies","title":"Installing dependencies\u00b6","text":"<p>First we need to install crewai which comes with a CLI command to start a new project. Additionally CrewAI is using poetry to manage dependencies.</p> <p>Lets install those 2 packages</p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#install-project-dependencies","title":"Install project dependencies\u00b6","text":""},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#initialize-the-google-cloud-vertex-ai-python-sdk","title":"Initialize the Google Cloud Vertex AI Python SDK\u00b6","text":""},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#set-your-project-id-location-and-staging-bucket","title":"Set Your Project ID, Location and Staging Bucket\u00b6","text":""},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#authenticate-user","title":"Authenticate user\u00b6","text":"<p>The method for authenticating your Google Cloud account is dependent on the environment in which this notebook is being executed. Depending on your Jupyter environment, you may have to manually authenticate.</p> <p>Refer to the subsequent sections for the appropriate procedure.</p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#1-for-vertex-ai-workbench","title":"1. For Vertex AI Workbench\u00b6","text":"<ul> <li>Do nothing as you are already authenticated.</li> </ul>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#2-local-jupyterlab-instance","title":"2. Local JupyterLab instance\u00b6","text":"<ul> <li>Uncomment and run code below:</li> </ul>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#3-for-colab-recommended","title":"3. For Colab (Recommended)\u00b6","text":"<ul> <li>If you are running this notebook on Google Colab, run the following cell to authenticate your environment.</li> </ul>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#running-our-crew-demo","title":"Running our crew demo\u00b6","text":"<p>By default this demo allows you to rin a researhc on a topic of choice using 2 agents, a Senior Data Researcher that runs a research on a given topic and a Reporting Analyst that prepares a report using the findings from the Researcher.</p> <p>Let's test our crew now that we have applied the changes. We will run it locally using the CLI.</p> <p>Because Agents do multiple calls to the VertexAI Gemini API it is possible that some of the executions will run out of quotas. If you get <code>RESOURCE_EXHAUSTED</code> error pause and try again after a minute.</p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#preapare-crewai-interface-for-reasoning-engine","title":"Preapare CrewAI interface for Reasoning Engine\u00b6","text":"<p>Now that we know CrewAI works locally we will go ahead and prepare for reasoning engine deployment.</p> <p>To be able to run CrewAI on Reasoning Engine we need to create a class that defines an <code>__init__</code>, <code>setup</code> and <code>query</code> functions and crew_ai_app.py.</p> <p>Below you can see what we are creating a crew_ai_app.py that can be used as our wrapper for reasoning engine deployment </p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#some-highlights","title":"Some highlights:\u00b6","text":"<ul> <li><p><code>def set_up(self)</code>: We define what happens when our application starts. Depending on your implementation here you might want to initialise other libraries, set logging etc. In our simple example we only set the project id as an environment variable to optain the right permissions to resourses.</p> </li> <li><p><code>CrewaiGcpCrew().crew().kickoff(inputs={\"topic\": question})</code>: runs the CrewAI for a given topic. The response should be returned as str</p> </li> </ul>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#test-wrapper-locally","title":"Test Wrapper locally\u00b6","text":"<p>Now that we created our wrapper we need to ensure that it can run and trigger crewai.</p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#time-to-initialise-vertexai-and-deploy-our-crew-to-reasoning-engine","title":"Time to initialise VertexAI and deploy our crew to reasoning engine\u00b6","text":""},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#its-deployment-time","title":"It's deployment time!\u00b6","text":"<p>Deployment takes few minutes. Good time to grap a coffee! \u2615</p>"},{"location":"genai-on-vertex-ai/agents/reasoning_engine/crewai/CrewAI_on_VertexAI_Reasoning_Engine/#cleanup","title":"Cleanup\u00b6","text":"<p>If you wish to delete the deployment from reasoning engine simply uncomment and run the following cell</p>"},{"location":"genai-on-vertex-ai/gemini/","title":"Overview","text":""},{"location":"genai-on-vertex-ai/gemini/#gemini-on-vertex-ai","title":"Gemini on Vertex AI","text":"<p>This folder contains comprehensive set of resources on how to use Gemini on Vertex AI in conjunction with other Vertex AI services for enterprise Generative AI use cases and solutions.</p>"},{"location":"genai-on-vertex-ai/gemini/#requirements","title":"Requirements","text":"<p>To run the notebooks you'll need access to a Google Cloud project with the Vertex AI API enabled.</p>"},{"location":"genai-on-vertex-ai/gemini/#using-this-repository","title":"Using this repository","text":"Description Contents        Gemini prompting recipes:       <code>prompting_recipes/</code>        Code samples demonstrating prompting techniques for Gemini specific use cases      multimodal prompting, pdf processing and spatial reasoning        Gemini Evals Playbook:       <code>evals_playbook/</code>        Tools built with Vertex AI services to streamline experimentation and evaluation of Generative AI applications used for pre-production performance testing, regression testing, or migration from other LLM APIs.      evaluating prompt iterations, sampling parameters, long context <p>[!TIP] Refer here for introduction to prompt design, and here for Vertex Gemini prompting strategies and best practices.</p>"},{"location":"genai-on-vertex-ai/gemini/#getting-help","title":"Getting Help","text":"<p>If you have any questions or find any problems, please report through GitHub issues.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/","title":"Overview","text":"Vertex AI: Gemini Evaluations Playbook Experiment, Evaluate &amp; Analyze model performance for your use cases"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#overview","title":"\u2728 Overview","text":"<p>The Gemini Evaluations Playbook provides recipes to streamline the experimentation and evaluation of Generative AI models for your use cases using Vertex Generative AI Evaluation service. This enables you to track and align model performance with your objectives, while providing insights to optimize the model under different conditions and configurations.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#experimentation-and-evaluation-workflow","title":"\ud83d\udccf Experimentation and evaluation workflow","text":"<p>Prompting strategies and best practices are essential for getting started with Gemini, but they're only the first step. To ensure your Generative AI solution with Gemini delivers repeatable and scalable performance, you need a systematic experimentation and evaluation process. This involves meticulous tracking of each experimental configuration, including prompt templates (system instructions, context, and few-shot learning examples), and model parameters like temperature and max output tokens.</p> <p>Your evaluation should go beyond overall results and report granular metrics for each experiment and not just final results for the evaluation exercise.</p> <p>By following this process, you'll not only maximize your GenAI solution's performance but also identify anti-patterns and system-level design improvements early on. This proactive approach is far more efficient than discovering issues after deployment.</p> <p></p> <p>[!NOTE] Refer here for adding automation to your experimentation workflow with the Vertex AI Prompt Optimizer.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#architecture","title":"\ud83d\udccf Architecture","text":"<p>The following diagram depicts the architecture of the Gemini Evaluations Playbook. The architecture leverages   - Vertex Generative AI Evaluation service for running evaluations  - Google BigQuery for logging prompts, experiments and eval runs.</p> <p></p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#key-features","title":"\ud83e\udde9 Key Features","text":"<p>The Gemini Evaluations Playbook (referred as Evals Playbook) provides following key features:</p> \u2705 Define, track and compare experiments Define and track a hierarchical structure of tasks, experiments, and evaluation runs to systematically organize and track your evaluation efforts.  \u2705 Log evaluation results with prompts and responses Manage and log experiment configurations and results to BigQuery, enabling comprehensive analysis.  \u2705 Customize evaluation runs Customize evaluations by configuring prompt templates, generation parameters, safety settings, and evaluation metrics to match your specific use case.  \u2705 Comprehensive Metrics Track a range of built-in and custom metrics to gain a holistic understanding of model performance.  \u2705 Iterative refinement Analyze insights from evaluation to iteratively refine prompts, model configurations, and fine-tuning to achieve optimal outcomes."},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#getting-started","title":"\ud83c\udfc1 Getting Started","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#step-1-clone-the-repository","title":"STEP 1. Clone the repository","text":"<pre><code>git clone https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; cd applied-ai-engineering-samples/genai-on-vertex-ai/gemini/evals_playbook\n</code></pre>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#step-2-prepare-your-environment","title":"STEP 2. Prepare your environment","text":"<p>Start with 0_gemini_evals_playbook_setup notebook  to install required libraries (using Poetry) and configure the necessary resources on Google Cloud. This includes setting up a BigQuery dataset and saving configuration parameters.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#step-3-experiment-evaluate-and-analyze","title":"STEP 3. Experiment, evaluate, and analyze","text":"<p>Run the 1_gemini_evals_playbook_evaluate notebook to design experiments, assess model performance on your generative AI tasks, and analyze evaluation results including side-by-side comparison of results across different experiments and runs.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#step-4-optimize-with-grid-search","title":"STEP 4. Optimize with grid search","text":"<p>Run the 2_gemini_evals_playbook_grid_search notebook to systematically explore different experiment configurations  by testing various prompt templates or model settings (like temperature), or combinations of these using a grid-search style approach.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#repository-structure","title":"\ud83e\uddec Repository Structure","text":"<pre><code>.\n\u251c\u2500\u2500 bigquery_sqls\n  \u2514\u2500\u2500 evals_bigquery.sql\n\u2514\u2500\u2500 docs\n\u2514\u2500\u2500 notebooks\n  \u2514\u2500\u2500 0_gemini_evals_playbook_setup.ipynb\n  \u2514\u2500\u2500 1_gemini_evals_playbook_evaluate.ipynb\n  \u2514\u2500\u2500 2_gemini_evals_playbook_gridsearch.ipynb\n\u2514\u2500\u2500 utils\n  \u2514\u2500\u2500 config.py\n  \u2514\u2500\u2500 evals_playbook.py\n\u2514\u2500\u2500 config.ini\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> Navigating repository structure  - [`/evals_bigquery.sql`](/utils/evals_bigquery.sql): SQL queries to create BigQuery datasets and tables - [`/notebooks`](/notebooks): Notebooks demonstrating the usage of Evals Playbook - [`/utils`](/utils): Utility or helper functions for running notebooks - [`/congig.ini`](/config.ini): Save and reuse configuration parameters created in[0_gemini_evals_playbook_setup](/notebooks/0_gemini_evals_playbook_setup.ipynb) - [`/docs`](/docs): Documentation explaining key concepts"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#documentation","title":"\ud83d\udcc4 Documentation","text":"<ul> <li>Evals Playbook usage</li> <li><code>Architecture</code></li> <li><code>Data Schema</code></li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#quotas-and-limits","title":"\ud83d\udea7 Quotas and limits","text":"<p>Verify you have sufficient quota to run experiments and evaluations: - BigQuery quotas - Vertex AI Gemini quotas</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#license","title":"\ud83e\udeaa License","text":"<p>Distributed with the Apache-2.0 license. </p> <p>Also contains code derived from the following third-party packages: * Python * pandas * LLM Comparator</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/#getting-help","title":"\ud83d\ude4b Getting Help","text":"<p>If you have any questions or if you found any problems with this repository, please report through GitHub issues.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/","title":"Evals Playbook: Prepare your environment","text":"In\u00a0[1]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Vertex AI: Gemini Evaluations Playbook  Prepare your environment  Run in Colab       Run in Colab Enterprise       View on GitHub       Open in Vertex AI Workbench      In\u00a0[\u00a0]: Copied! <pre># Install poetry\n! pip uninstall poetry -y\n! pip install poetry --quiet\n\n# Run the poetry commands below to set up the environment\n! poetry lock # resolve dependencies (also auto create poetry venv if not exists)\n! poetry install --quiet # installs dependencies\n! poetry env info # displays the evn just created and the path to it\n</pre> # Install poetry ! pip uninstall poetry -y ! pip install poetry --quiet  # Run the poetry commands below to set up the environment ! poetry lock # resolve dependencies (also auto create poetry venv if not exists) ! poetry install --quiet # installs dependencies ! poetry env info # displays the evn just created and the path to it In\u00a0[\u00a0]: Copied! <pre>import IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") In\u00a0[1]: Copied! <pre># Define variables\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\n</pre> # Define variables PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} LOCATION = \"us-central1\"  # @param {type:\"string\"} In\u00a0[2]: Copied! <pre>STAGING_BUCKET = \"[your-bucket-name]\"  # @param {type:\"string\"}\nSTAGING_BUCKET_URI = f\"gs://{STAGING_BUCKET}\"\n</pre> STAGING_BUCKET = \"[your-bucket-name]\"  # @param {type:\"string\"} STAGING_BUCKET_URI = f\"gs://{STAGING_BUCKET}\" In\u00a0[\u00a0]: Copied! <pre>from google.cloud import storage\n\nstorage_client = storage.Client(project=PROJECT_ID)\n\n# Check if bucket exists, create if not\nif not storage_client.bucket(STAGING_BUCKET).exists():\n    storage_client.create_bucket(STAGING_BUCKET)\n    print(f\"Bucket {STAGING_BUCKET} created!\")\nelse:\n    print(f\"Bucket {STAGING_BUCKET} already exists\")\n\nbucket = storage_client.get_bucket(STAGING_BUCKET)\n# Verify the storage bucket project\nprint(f\"Bucket is in the project {bucket.client.project}\")\n</pre> from google.cloud import storage  storage_client = storage.Client(project=PROJECT_ID)  # Check if bucket exists, create if not if not storage_client.bucket(STAGING_BUCKET).exists():     storage_client.create_bucket(STAGING_BUCKET)     print(f\"Bucket {STAGING_BUCKET} created!\") else:     print(f\"Bucket {STAGING_BUCKET} already exists\")  bucket = storage_client.get_bucket(STAGING_BUCKET) # Verify the storage bucket project print(f\"Bucket is in the project {bucket.client.project}\") In\u00a0[\u00a0]: Copied! <pre># Enable required APIs\n! gcloud services enable \\\n    iam.googleapis.com \\\n    storage-component.googleapis.com \\\n    compute.googleapis.com \\\n    aiplatform.googleapis.com \\\n    bigquery.googleapis.com \\\n    cloudresourcemanager.googleapis.com \\\n    --project $PROJECT_ID\n</pre> # Enable required APIs ! gcloud services enable \\     iam.googleapis.com \\     storage-component.googleapis.com \\     compute.googleapis.com \\     aiplatform.googleapis.com \\     bigquery.googleapis.com \\     cloudresourcemanager.googleapis.com \\     --project $PROJECT_ID In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\n\nmodule_path = os.path.abspath(os.path.join(\"..\"))\nsys.path.append(module_path)\n\nimport vertexai\n\nvertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)\n\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n</pre> import os import sys  module_path = os.path.abspath(os.path.join(\"..\")) sys.path.append(module_path)  import vertexai  vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)  print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") <p>Configure dataset name and table names to define data schema in BigQuery.</p> \u26a0\ufe0f You can leave the variables defined below as is, unless needed to change. \u26a0\ufe0f In\u00a0[6]: Copied! <pre># BigQuery datasets\nBQ_DATASET_ID = \"gemini_evals_playbook\"  # @param {type:\"string\"}\nBQ_LOCATION = \"US\"\n\n# DO NOT CHANGE\nBQ_TABLES_SQL_PATH = os.path.join(module_path, \"bigquery_sqls\", \"evals_bigquery.sql\")\nBQ_PREFIX = \"eval\"\nBQ_T_EVAL_TASKS = f\"{BQ_PREFIX}_tasks\"\nBQ_T_EXPERIMENTS = f\"{BQ_PREFIX}_experiments\"\nBQ_T_PROMPTS = f\"{BQ_PREFIX}_prompts\"\nBQ_T_DATASETS = f\"{BQ_PREFIX}_datasets\"\nBQ_T_EVAL_RUN_DETAILS = f\"{BQ_PREFIX}_run_details\"\nBQ_T_EVAL_RUNS = f\"{BQ_PREFIX}_runs\"\n</pre> # BigQuery datasets BQ_DATASET_ID = \"gemini_evals_playbook\"  # @param {type:\"string\"} BQ_LOCATION = \"US\"  # DO NOT CHANGE BQ_TABLES_SQL_PATH = os.path.join(module_path, \"bigquery_sqls\", \"evals_bigquery.sql\") BQ_PREFIX = \"eval\" BQ_T_EVAL_TASKS = f\"{BQ_PREFIX}_tasks\" BQ_T_EXPERIMENTS = f\"{BQ_PREFIX}_experiments\" BQ_T_PROMPTS = f\"{BQ_PREFIX}_prompts\" BQ_T_DATASETS = f\"{BQ_PREFIX}_datasets\" BQ_T_EVAL_RUN_DETAILS = f\"{BQ_PREFIX}_run_details\" BQ_T_EVAL_RUNS = f\"{BQ_PREFIX}_runs\" In\u00a0[7]: Copied! <pre>def setup_bigquery(bq_project_id, dataset_name, dataset_region, dry_run=False):\n    from google.cloud import bigquery\n\n    dataset_ref = f\"{bq_project_id}.{dataset_name}\"\n    job_config = bigquery.QueryJobConfig(dry_run=dry_run, default_dataset=dataset_ref)\n    client = bigquery.Client(\n        project=bq_project_id,\n        location=dataset_region,\n        default_query_job_config=job_config,\n    )\n    # create schema/dataset\n    try:\n        ddl = f\"\"\"\n            CREATE SCHEMA IF NOT EXISTS {dataset_name}\n            OPTIONS(\n                description=\"dataset for configuring Gemini evaluation tasks and storing evaluation results\",\n                {f\"location='{dataset_region}',\"if dataset_region else \"\"}\n                labels=[(\"tool\", \"vertexai-gemini-evals\")]\n            )\n        \"\"\"\n        print(\n            f\"Creating dataset {dataset_name} in project {bq_project_id}, if does not exists\"\n        )\n        print(ddl)\n        job = client.query(ddl)\n        results = job.result()\n        for result in results:\n            print(result)\n    except Exception as e:\n        print(\n            f\"Failed to create dataset {dataset_name} in project {bq_project_id} \\n{e}\"\n        )\n        raise e\n    # create tables\n    try:\n        print(f\"Creating tables in project {bq_project_id}, if does not exists.\")\n\n        ddl = open(BQ_TABLES_SQL_PATH).read()\n        print(ddl)\n\n        job = client.query(ddl)\n        result = job.result()\n        for result in results:\n            print(result)\n    except Exception as e:\n        print(f\"Failed to create tables in project {bq_project_id}\")\n        raise e\n</pre> def setup_bigquery(bq_project_id, dataset_name, dataset_region, dry_run=False):     from google.cloud import bigquery      dataset_ref = f\"{bq_project_id}.{dataset_name}\"     job_config = bigquery.QueryJobConfig(dry_run=dry_run, default_dataset=dataset_ref)     client = bigquery.Client(         project=bq_project_id,         location=dataset_region,         default_query_job_config=job_config,     )     # create schema/dataset     try:         ddl = f\"\"\"             CREATE SCHEMA IF NOT EXISTS {dataset_name}             OPTIONS(                 description=\"dataset for configuring Gemini evaluation tasks and storing evaluation results\",                 {f\"location='{dataset_region}',\"if dataset_region else \"\"}                 labels=[(\"tool\", \"vertexai-gemini-evals\")]             )         \"\"\"         print(             f\"Creating dataset {dataset_name} in project {bq_project_id}, if does not exists\"         )         print(ddl)         job = client.query(ddl)         results = job.result()         for result in results:             print(result)     except Exception as e:         print(             f\"Failed to create dataset {dataset_name} in project {bq_project_id} \\n{e}\"         )         raise e     # create tables     try:         print(f\"Creating tables in project {bq_project_id}, if does not exists.\")          ddl = open(BQ_TABLES_SQL_PATH).read()         print(ddl)          job = client.query(ddl)         result = job.result()         for result in results:             print(result)     except Exception as e:         print(f\"Failed to create tables in project {bq_project_id}\")         raise e In\u00a0[\u00a0]: Copied! <pre>setup_bigquery(\n    bq_project_id=PROJECT_ID,\n    dataset_name=BQ_DATASET_ID,\n    dataset_region=BQ_LOCATION,\n    dry_run=False,\n)\n\nprint(\n    \"Done! Created Bigquery dataset and tables to configure experiments and store eval results\"\n)\nprint(\"You are ready to run the evaluations!\")\n</pre> setup_bigquery(     bq_project_id=PROJECT_ID,     dataset_name=BQ_DATASET_ID,     dataset_region=BQ_LOCATION,     dry_run=False, )  print(     \"Done! Created Bigquery dataset and tables to configure experiments and store eval results\" ) print(\"You are ready to run the evaluations!\") In\u00a0[\u00a0]: Copied! <pre>from utils.config import save_config\n\nsave_config(\n    PROJECT_ID,\n    LOCATION,\n    STAGING_BUCKET,\n    STAGING_BUCKET_URI,\n    BQ_DATASET_ID,\n    BQ_LOCATION,\n    BQ_TABLES_SQL_PATH,\n    BQ_PREFIX,\n    BQ_T_EVAL_TASKS,\n    BQ_T_EXPERIMENTS,\n    BQ_T_PROMPTS,\n    BQ_T_DATASETS,\n    BQ_T_EVAL_RUN_DETAILS,\n    BQ_T_EVAL_RUNS,\n)\n</pre> from utils.config import save_config  save_config(     PROJECT_ID,     LOCATION,     STAGING_BUCKET,     STAGING_BUCKET_URI,     BQ_DATASET_ID,     BQ_LOCATION,     BQ_TABLES_SQL_PATH,     BQ_PREFIX,     BQ_T_EVAL_TASKS,     BQ_T_EXPERIMENTS,     BQ_T_PROMPTS,     BQ_T_DATASETS,     BQ_T_EVAL_RUN_DETAILS,     BQ_T_EVAL_RUNS, )"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#evals-playbook-prepare-your-environment","title":"Evals Playbook: Prepare your environment\u00b6","text":"<p>This notebook show you how to prepare the environment to run notebooks under Gemini Evals Playbook. The notebook performs following steps:</p> <ul> <li>Enable required APIs in Google Cloud project</li> <li>Required permissions and roles</li> <li>Install required libraries using Poetry</li> <li>Configure the necessary resources on Google Cloud such as BigQuery</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#before-you-begin","title":"Before you begin\u00b6","text":"<p>Before you run this notebook, review the README file of this repository to understand the features, architecture and data schema of Evals Playbook.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#getting-started","title":"\ud83c\udfac Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs</li> <li>Make sure that billing is enabled for your project</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API</li> <li>Enable the Cloud Storage API</li> <li>Enable the Cloud BigQuery API</li> <li>Enable the Cloud Resource Manager API</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, you will need to have the Owner role for your project. At minimum, you need the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> <li><code>roles/bigquery.user</code> and <code>roles/bigquery.dataViewer</code> to query BigQuery tables</li> <li><code>roles/bigquery.jobUser</code> to run BigQuery jobs</li> <li><code>roles/secretmanager.secretAccessor</code> to access secret versions in Cloud Secret Manager</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#install-dependencies","title":"Install dependencies\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.</p> <p>You may see the restart reported as a crash, but it is working as-intended -- you are merely restarting the runtime.</p> <p>The restart might take a minute or longer. After it's restarted, continue to the next step.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#set-google-cloud-project-information","title":"Set Google Cloud project information\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>LOCATION</code>unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#create-google-cloud-storage-bucket","title":"Create Google Cloud storage bucket\u00b6","text":"<p>Create or set Cloud Storage bucket name for Vertex AI staging and any other files relating to evals.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#enable-required-google-cloud-apis","title":"Enable required Google Cloud APIs\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#initialize-vertex-ai-sdk","title":"Initialize Vertex AI SDK\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#create-data-schema","title":"\u26c1 Create Data Schema\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#save-configuration-to-file","title":"\ud83d\udcbe Save Configuration to File\u00b6","text":"<p>Save the configurations set in this notebook to  <code>config.ini</code>. The parameters from this file are used in subsequent notebooks</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/0_gemini_evals_playbook_setup/#if-all-the-above-steps-are-executed-sucessfully-the-following-should-be-set-up","title":"\ud83e\udd41 If all the above steps are executed sucessfully, the following should be set up\u00b6","text":"<ul> <li><p>GCP project and APIs to run the eval pipeline</p> </li> <li><p>All the required IAM permissions</p> </li> <li><p>Environment to run the notebooks</p> </li> <li><p>Bigquery datasets and tables to track evaluation results</p> </li> </ul> <p>You can now proceed to run rest of the notebooks in Evals Playbook. Start with 1_gemini_evals_playbook_evaluate to design experiments, assess model performance on your generative AI tasks, and analyze evaluation results including side-by-side comparison of results across different experiments and runs.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/","title":"Evals Playbook: Experiment, Evaluate &amp; Analyze","text":"In\u00a0[1]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Vertex AI: Gemini Evaluations Playbook  Experiment, Evaluate, and Analyze  Run in Colab       Run in Colab Enterprise       View on GitHub       Open in Vertex AI Workbench      In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\n\nmodule_path = os.path.abspath(os.path.join(\"..\"))\nsys.path.append(module_path)\nprint(f\"module_path: {module_path}\")\n\n# Import all the parameters\nfrom utils.config import (LOCATION, PROJECT_ID, STAGING_BUCKET,\n                          STAGING_BUCKET_URI)\nfrom utils.evals_playbook import Evals, generate_uuid\n</pre> import os import sys  module_path = os.path.abspath(os.path.join(\"..\")) sys.path.append(module_path) print(f\"module_path: {module_path}\")  # Import all the parameters from utils.config import (LOCATION, PROJECT_ID, STAGING_BUCKET,                           STAGING_BUCKET_URI) from utils.evals_playbook import Evals, generate_uuid In\u00a0[4]: Copied! <pre>import datetime\nimport itertools\nimport re\n\nimport pandas as pd\nimport vertexai\nfrom datasets import Dataset, load_dataset\nfrom vertexai.evaluation import (EvalTask, PointwiseMetric,\n                                 PointwiseMetricPromptTemplate, constants)\nfrom vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,\n                                        HarmCategory, SafetySetting)\n</pre> import datetime import itertools import re  import pandas as pd import vertexai from datasets import Dataset, load_dataset from vertexai.evaluation import (EvalTask, PointwiseMetric,                                  PointwiseMetricPromptTemplate, constants) from vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,                                         HarmCategory, SafetySetting) In\u00a0[\u00a0]: Copied! <pre>vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)\n\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n\n# pandas display full column values\npd.set_option(\"display.max_colwidth\", None)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\n</pre> vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)  print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\")  # pandas display full column values pd.set_option(\"display.max_colwidth\", None) pd.set_option(\"display.max_rows\", None) pd.set_option(\"display.max_columns\", None) In\u00a0[6]: Copied! <pre># Initialize evals object\nevals = Evals()\n</pre> # Initialize evals object evals = Evals() In\u00a0[\u00a0]: Copied! <pre># create and log task\ntask_id = \"task_summarization\"\ntask = evals.Task(\n    task_id=task_id,\n    task_desc=\"summarize pubmed articles\",\n    tags=[\"pubmed\"],\n    create_datetime=datetime.datetime.now(),\n    update_datetime=datetime.datetime.now(),\n)\nevals.log_task(task)\n</pre> # create and log task task_id = \"task_summarization\" task = evals.Task(     task_id=task_id,     task_desc=\"summarize pubmed articles\",     tags=[\"pubmed\"],     create_datetime=datetime.datetime.now(),     update_datetime=datetime.datetime.now(), ) evals.log_task(task) <ul> <li>List all tasks available in the database (lists tasks sorted by task creation time in descending order)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_all_tasks()\n</pre> evals.get_all_tasks() \u26a0\ufe0f We recommend to create unique experiment id for each experiment to enable better tracking and experimentation. \u26a0\ufe0f In\u00a0[9]: Copied! <pre>experiment_id = \"Prompt with simple language summary and custom metrics\"\n# remove any special characters from experiment id\n_experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower())\nexperiment_desc = \"Update system instruction to generate a simple summary with bullets\"\ntags = [\"pubmed\"]\nmetadata = {}\n</pre> experiment_id = \"Prompt with simple language summary and custom metrics\" # remove any special characters from experiment id _experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower()) experiment_desc = \"Update system instruction to generate a simple summary with bullets\" tags = [\"pubmed\"] metadata = {} <ul> <li>Add system instructions to give the model additional context to understand the task, provide more customized responses, and adhere to specific guidelines over the full user interaction with the model.</li> </ul> In\u00a0[10]: Copied! <pre>system_instruction = \"\"\"Instruction: You are a medical researcher writing a plain language Summary of your Article for a layperson.\n\nTranslate any medical terms to simple english explanations.\nUse first-person 'We'.  Use short bullet points addressing following\n- Purpose: What was the purpose of the study?\n- Research: What did the researchers do?\n- Findings: What did they find?\n- Implications: What does this mean for me?\"\n\"\"\"\n</pre> system_instruction = \"\"\"Instruction: You are a medical researcher writing a plain language Summary of your Article for a layperson.  Translate any medical terms to simple english explanations. Use first-person 'We'.  Use short bullet points addressing following - Purpose: What was the purpose of the study? - Research: What did the researchers do? - Findings: What did they find? - Implications: What does this mean for me?\" \"\"\" <ul> <li>Define generation config and safety settings</li> </ul> In\u00a0[11]: Copied! <pre>generation_config = {\n    \"temperature\": 0.1,\n}\n\nsafety_settings = [\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n]\n</pre> generation_config = {     \"temperature\": 0.1, }  safety_settings = [     SafetySetting(         category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_HARASSMENT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ), ] In\u00a0[12]: Copied! <pre>model = GenerativeModel(\n    model_name=\"gemini-2.0-flash-001\",\n    generation_config=generation_config,\n    safety_settings=safety_settings,\n    system_instruction=system_instruction,\n    # TODO: Add tools and tool_config\n)\n</pre> model = GenerativeModel(     model_name=\"gemini-2.0-flash-001\",     generation_config=generation_config,     safety_settings=safety_settings,     system_instruction=system_instruction,     # TODO: Add tools and tool_config ) <ul> <li>Prepare a prompt template for the experiment</li> </ul> In\u00a0[\u00a0]: Copied! <pre>prompt_id = \"short bulleted list with format\"\nprompt_description = \"instruction with short bullets addressing specific questions\"\n\n# Prompt Template\nprompt_template = \"Article: {context} \\nSummary:\"\n\nevals.save_prompt_template(task_id, _experiment_id, prompt_id, prompt_template)\n</pre> prompt_id = \"short bulleted list with format\" prompt_description = \"instruction with short bullets addressing specific questions\"  # Prompt Template prompt_template = \"Article: {context} \\nSummary:\"  evals.save_prompt_template(task_id, _experiment_id, prompt_id, prompt_template) <ul> <li>Configure prompt id, description for tracking</li> </ul> In\u00a0[\u00a0]: Copied! <pre>prompt = evals.Prompt(\n    prompt_id=prompt_id,\n    prompt_description=prompt_description,\n    prompt_type=\"single-turn\",  # single-turn, chat,\n    is_multimodal=False,\n    system_instruction=system_instruction,\n    prompt_template=prompt_template,\n    create_datetime=datetime.datetime.now(),\n    update_datetime=datetime.datetime.now(),\n    tags=tags,\n)\nevals.log_prompt(prompt)\n</pre> prompt = evals.Prompt(     prompt_id=prompt_id,     prompt_description=prompt_description,     prompt_type=\"single-turn\",  # single-turn, chat,     is_multimodal=False,     system_instruction=system_instruction,     prompt_template=prompt_template,     create_datetime=datetime.datetime.now(),     update_datetime=datetime.datetime.now(),     tags=tags, ) evals.log_prompt(prompt) <ul> <li>Download sample dataset (10 rows) of PubMed articles for the task.</li> </ul> In\u00a0[15]: Copied! <pre># get sample dataset from PubMed articles\nds_stream = load_dataset(\n    \"ccdv/pubmed-summarization\", \"document\", split=\"test\", streaming=True\n)\nnum_rows = 10\ndataset = Dataset.from_list(list(itertools.islice(ds_stream, num_rows)))\n</pre> # get sample dataset from PubMed articles ds_stream = load_dataset(     \"ccdv/pubmed-summarization\", \"document\", split=\"test\", streaming=True ) num_rows = 10 dataset = Dataset.from_list(list(itertools.islice(ds_stream, num_rows))) <ul> <li>Pre-process and prepare dataset to use with the evaluator.</li> </ul> <p>Prepare the dataset as Pandas dataframe in the format expected by the Vertex AI Rapid Eval SDK.</p> <p>Dataset column names:</p> <ul> <li><code>reference</code>: The column name of ground truth in the dataset.</li> <li><code>context</code>: The column name containing article passed as the context.</li> <li><code>instruction</code>: System instruction configured to pass to the model</li> </ul> In\u00a0[16]: Copied! <pre># convert HuggingFace dataset to Pandas dataframe\neval_dataset = dataset.to_pandas()\n# rename columns as per Vertex AI Rapid Eval SDK defaults\neval_dataset.columns = [\"context\", \"reference\"]\n# add instruction for calculating metrics (not all metrics need instruction)\neval_dataset[\"instruction\"] = system_instruction\n# add prompt column\neval_dataset[\"prompt\"] = eval_dataset[\"context\"].apply(\n    lambda x: prompt_template.format(context=x)\n)\n# add prompt id for tracking\neval_dataset[\"dataset_row_id\"] = [f\"dataset_row_{i}\" for i in eval_dataset.index]\n</pre> # convert HuggingFace dataset to Pandas dataframe eval_dataset = dataset.to_pandas() # rename columns as per Vertex AI Rapid Eval SDK defaults eval_dataset.columns = [\"context\", \"reference\"] # add instruction for calculating metrics (not all metrics need instruction) eval_dataset[\"instruction\"] = system_instruction # add prompt column eval_dataset[\"prompt\"] = eval_dataset[\"context\"].apply(     lambda x: prompt_template.format(context=x) ) # add prompt id for tracking eval_dataset[\"dataset_row_id\"] = [f\"dataset_row_{i}\" for i in eval_dataset.index]  <ul> <li>Verify a few samples in the prepared evaluation dataset</li> </ul> In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of rows: {eval_dataset.shape}\")\neval_dataset.head(1)\n</pre> print(f\"Number of rows: {eval_dataset.shape}\") eval_dataset.head(1) <ul> <li>Optionally, save the dataset in Cloud Storage (or BigQuery) to reuse.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>file_name = \"pubmed_summary.csv\"\ngcs_file_path = f\"gs://{STAGING_BUCKET}/{task_id}/data/{file_name}\"\n# Save dataset to Cloud Storage\neval_dataset.to_csv(gcs_file_path, index=False)\nprint(f\"Dataset saved at {gcs_file_path} successfully!\")\n</pre> file_name = \"pubmed_summary.csv\" gcs_file_path = f\"gs://{STAGING_BUCKET}/{task_id}/data/{file_name}\" # Save dataset to Cloud Storage eval_dataset.to_csv(gcs_file_path, index=False) print(f\"Dataset saved at {gcs_file_path} successfully!\") <ul> <li>Define prebuilt/built-in metrics with Vertex GenAI Evaluation or bring your own metrics.</li> </ul> In\u00a0[19]: Copied! <pre># Creating custom metrics for Pointwise Evaluation;\n# You can define the metric following either a template of criteria and rating rubric\n# or using a free form prompt. One example for each is demonstrated below\n\n# Example 1: format adherence metric, to evaluate if the LLM strictly followed the required formatting\ncriteria = {\n    \"First-person We\": \"The text is written in first person 'we'\",\n    \"Format\": \"The output is formatted in bullets\",\n    \"Completeness\": \"All four sections, purpose, research, findings and implications are addressed in the output\",\n}\n\npointwise_rating_rubric = {\n    \"5\": \"Perfectly formatted: Text is in first person 'we', formatted in bullets and all four sections purpose, research, findings and implications are addressed in the output\",\n    \"4\": \"Mostly formatted: Content is formatted in bullets and all four sections purpose, research, findings and implications are addressed in the output, but failed to write in first person 'we' \",\n    \"3\": \"Somewhat formatted: Content is formatted in bullets and but failed to address one of the four sections purpose, research, findings and implications\",\n    \"2\": \"Poorly formatted : Content is may or may not be formatted in bullets and failed to address two out of the four sections purpose, research, findings and implications\",\n    \"1\": \"Very poorly formatted: Content is not formatted in bullets and failed to address two or more out of the four sections purpose, research, findings and implications\",\n}\n\n# The metric prompt template contains default prompts pre-defined for unspecified components.\nformat_adherence_metric_prompt_template = PointwiseMetricPromptTemplate(\n    criteria=criteria,\n    rating_rubric=pointwise_rating_rubric,\n    input_variables=[\"prompt\", \"reference\"],\n)\n\n# Display the assembled prompt template that will be sent to Gen AI Eval Service\n# along with the input data for model-based evaluation.\n# print(format_adherence_metric_prompt_template.prompt_data)\n\n# Register the custom \"format_adherence\" model-based metric.\nformat_adherence = PointwiseMetric(\n    metric=\"format_adherence\",\n    metric_prompt_template=format_adherence_metric_prompt_template,\n)\n\n\n# Example 2: text quality and relevance to layperson\nfree_form_pointwise_metric_prompt = \"\"\"\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\nWe will provide you with the user prompt and an AI-generated response.\nYou should first read the user prompt carefully for analyzing the task, and then evaluate the \nquality of the responses based on and Criteria provided in the Evaluation section below.\n\nYou will assign the response a score from 5, 4, 3, 2, 1, following the Rating Rubric and Evaluation Steps. \nGive step-by-step explanations for your scoring, and only choose scores from 5, 4, 3, 2, 1.\n\n# Evaluation\n## Metric Definition\nYou will be assessing Text Quality and relevance to layperson, which measures how effectively the text conveys\nclear, accurate, and engaging information that is easily understandable by a layperson and directly addresses \nthe user's prompt, considering factors like fluency, coherence, relevance, conciseness and free of \ncomplex medical language\n\n## Criteria\nCoherence: The response presents ideas in a logical and organized manner, with clear transitions and a consistent focus, making it easy to follow and understand.\nFluency: The text flows smoothly and naturally, adhering to grammatical rules and using appropriate vocabulary.\nRelevance to layperson: The response is easily understandable by a layperson as opposed to a medical professional\nGroundedness: The response contains information included only in the context. The response does not reference any outside information.\nVerbosity: The response is appropriately concise, providing sufficient detail without using complex language to thoroughly address the prompt without being overly wordy or excessively brief.\n\n## Rating Rubric\n5: (Very good). Exceptionally clear, coherent, fluent, and concise. Free of complex Medical language\n4: (Good). Well-written, coherent, and fluent. Easy to understand by a layperson. Minor room for improvement.\n3: (Ok). Adequate writing with decent coherence and fluency. May contain some medical jargon and minor ungrounded information. Could be more concise.\n2: (Bad). Poorly written, lacking coherence and fluency. Geared towards to medical professional as opposed to layperson. May include ungrounded information. \n1: (Very bad). Very poorly written, incoherent, and non-fluent. Geared towards to medical professional as opposed to layperson. Contains substantial ungrounded information. Severely lacking in conciseness.\n\n## Evaluation Steps\nSTEP 1: Assess the response in aspects of all criteria provided. Provide assessment according to each criterion.\nSTEP 2: Score based on the rating rubric. Give a brief rationale to explain your evaluation considering each individual criterion.\n\n# User Inputs and AI-generated Response\n## User Inputs\n### Prompt\n{prompt}\n\n## AI-generated Response\n{reference}\n\"\"\"\n\n# Register the custom \"text_quality_relevance_to_layperson\" model-based metric.\ntext_quality_relevance_to_layperson = PointwiseMetric(\n    metric=\"text_quality_relevance_to_layperson\",\n    metric_prompt_template=free_form_pointwise_metric_prompt,\n)\n</pre> # Creating custom metrics for Pointwise Evaluation; # You can define the metric following either a template of criteria and rating rubric # or using a free form prompt. One example for each is demonstrated below  # Example 1: format adherence metric, to evaluate if the LLM strictly followed the required formatting criteria = {     \"First-person We\": \"The text is written in first person 'we'\",     \"Format\": \"The output is formatted in bullets\",     \"Completeness\": \"All four sections, purpose, research, findings and implications are addressed in the output\", }  pointwise_rating_rubric = {     \"5\": \"Perfectly formatted: Text is in first person 'we', formatted in bullets and all four sections purpose, research, findings and implications are addressed in the output\",     \"4\": \"Mostly formatted: Content is formatted in bullets and all four sections purpose, research, findings and implications are addressed in the output, but failed to write in first person 'we' \",     \"3\": \"Somewhat formatted: Content is formatted in bullets and but failed to address one of the four sections purpose, research, findings and implications\",     \"2\": \"Poorly formatted : Content is may or may not be formatted in bullets and failed to address two out of the four sections purpose, research, findings and implications\",     \"1\": \"Very poorly formatted: Content is not formatted in bullets and failed to address two or more out of the four sections purpose, research, findings and implications\", }  # The metric prompt template contains default prompts pre-defined for unspecified components. format_adherence_metric_prompt_template = PointwiseMetricPromptTemplate(     criteria=criteria,     rating_rubric=pointwise_rating_rubric,     input_variables=[\"prompt\", \"reference\"], )  # Display the assembled prompt template that will be sent to Gen AI Eval Service # along with the input data for model-based evaluation. # print(format_adherence_metric_prompt_template.prompt_data)  # Register the custom \"format_adherence\" model-based metric. format_adherence = PointwiseMetric(     metric=\"format_adherence\",     metric_prompt_template=format_adherence_metric_prompt_template, )   # Example 2: text quality and relevance to layperson free_form_pointwise_metric_prompt = \"\"\" # Instruction You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models. We will provide you with the user prompt and an AI-generated response. You should first read the user prompt carefully for analyzing the task, and then evaluate the  quality of the responses based on and Criteria provided in the Evaluation section below.  You will assign the response a score from 5, 4, 3, 2, 1, following the Rating Rubric and Evaluation Steps.  Give step-by-step explanations for your scoring, and only choose scores from 5, 4, 3, 2, 1.  # Evaluation ## Metric Definition You will be assessing Text Quality and relevance to layperson, which measures how effectively the text conveys clear, accurate, and engaging information that is easily understandable by a layperson and directly addresses  the user's prompt, considering factors like fluency, coherence, relevance, conciseness and free of  complex medical language  ## Criteria Coherence: The response presents ideas in a logical and organized manner, with clear transitions and a consistent focus, making it easy to follow and understand. Fluency: The text flows smoothly and naturally, adhering to grammatical rules and using appropriate vocabulary. Relevance to layperson: The response is easily understandable by a layperson as opposed to a medical professional Groundedness: The response contains information included only in the context. The response does not reference any outside information. Verbosity: The response is appropriately concise, providing sufficient detail without using complex language to thoroughly address the prompt without being overly wordy or excessively brief.  ## Rating Rubric 5: (Very good). Exceptionally clear, coherent, fluent, and concise. Free of complex Medical language 4: (Good). Well-written, coherent, and fluent. Easy to understand by a layperson. Minor room for improvement. 3: (Ok). Adequate writing with decent coherence and fluency. May contain some medical jargon and minor ungrounded information. Could be more concise. 2: (Bad). Poorly written, lacking coherence and fluency. Geared towards to medical professional as opposed to layperson. May include ungrounded information.  1: (Very bad). Very poorly written, incoherent, and non-fluent. Geared towards to medical professional as opposed to layperson. Contains substantial ungrounded information. Severely lacking in conciseness.  ## Evaluation Steps STEP 1: Assess the response in aspects of all criteria provided. Provide assessment according to each criterion. STEP 2: Score based on the rating rubric. Give a brief rationale to explain your evaluation considering each individual criterion.  # User Inputs and AI-generated Response ## User Inputs ### Prompt {prompt}  ## AI-generated Response {reference} \"\"\"  # Register the custom \"text_quality_relevance_to_layperson\" model-based metric. text_quality_relevance_to_layperson = PointwiseMetric(     metric=\"text_quality_relevance_to_layperson\",     metric_prompt_template=free_form_pointwise_metric_prompt, ) <p>For a full list of built in metrics:</p> <ul> <li>Computation-based: https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#computation-based-metrics</li> <li>Model-based: https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics</li> </ul> In\u00a0[\u00a0]: Copied! <pre># List of built in metrics\nmetrics = [\n    constants.Metric.ROUGE_1,\n    constants.Metric.ROUGE_L_SUM,\n    constants.Metric.BLEU,\n    constants.Metric.FLUENCY,\n    constants.Metric.COHERENCE,\n    constants.Metric.SAFETY,\n    constants.Metric.GROUNDEDNESS,\n    constants.Metric.SUMMARIZATION_QUALITY,\n]\n\n# build a metric config object for tracking\n# Add built in metrics\nmetric_config = [\n    {\"metric_name\": metric, \"type\": \"prebuilt\", \"metric_scorer\": \"Vertex AI\"}\n    for metric in metrics\n]\n\n# Add custom metrics\nmetric_config.extend(\n    [\n        {\n            \"metric_name\": text_quality_relevance_to_layperson.metric_name,\n            \"type\": \"custom\",\n            \"metric_scorer\": \"Vertex AI\",\n        },\n        {\n            \"metric_name\": format_adherence.metric_name,\n            \"type\": \"custom\",\n            \"metric_scorer\": \"Vertex AI\",\n        },\n    ]\n)\n\nmetrics.extend([text_quality_relevance_to_layperson, format_adherence])\n\nprint(metric_config)\n</pre> # List of built in metrics metrics = [     constants.Metric.ROUGE_1,     constants.Metric.ROUGE_L_SUM,     constants.Metric.BLEU,     constants.Metric.FLUENCY,     constants.Metric.COHERENCE,     constants.Metric.SAFETY,     constants.Metric.GROUNDEDNESS,     constants.Metric.SUMMARIZATION_QUALITY, ]  # build a metric config object for tracking # Add built in metrics metric_config = [     {\"metric_name\": metric, \"type\": \"prebuilt\", \"metric_scorer\": \"Vertex AI\"}     for metric in metrics ]  # Add custom metrics metric_config.extend(     [         {             \"metric_name\": text_quality_relevance_to_layperson.metric_name,             \"type\": \"custom\",             \"metric_scorer\": \"Vertex AI\",         },         {             \"metric_name\": format_adherence.metric_name,             \"type\": \"custom\",             \"metric_scorer\": \"Vertex AI\",         },     ] )  metrics.extend([text_quality_relevance_to_layperson, format_adherence])  print(metric_config) In\u00a0[\u00a0]: Copied! <pre>experiment = evals.log_experiment(\n    task_id=task_id,\n    experiment_id=experiment_id,\n    experiment_desc=experiment_desc,\n    prompt=prompt,\n    model=model,\n    metric_config=metric_config,\n    tags=tags,\n)\n</pre> experiment = evals.log_experiment(     task_id=task_id,     experiment_id=experiment_id,     experiment_desc=experiment_desc,     prompt=prompt,     model=model,     metric_config=metric_config,     tags=tags, ) <ul> <li>You can view the experiment details</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_experiment(experiment_id=experiment_id)\n</pre> evals.get_experiment(experiment_id=experiment_id) <ul> <li>You can view the prompt and system instruction if set.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_prompt(prompt_id=prompt_id)\n</pre> evals.get_prompt(prompt_id=prompt_id) <ul> <li>List all experiments available</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_all_experiments()\n</pre> evals.get_all_experiments() <ul> <li>Define Vertex AI Rapid Eval Task. Evaluation tasks must contain an evaluation dataset, and a list of metrics to evaluate.</li> </ul> In\u00a0[25]: Copied! <pre>_experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower())\neval_task = EvalTask(dataset=eval_dataset, metrics=metrics, experiment=_experiment_id)\n</pre> _experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower()) eval_task = EvalTask(dataset=eval_dataset, metrics=metrics, experiment=_experiment_id) <ul> <li>Run the evaluation task with a run name, model and prompt template. This step may take a few minutes depending on the size of evaluation dataset.</li> </ul> \u26a0\ufe0f A unique experiment run name is auto-generated based on experiment id. \u26a0\ufe0f In\u00a0[\u00a0]: Copied! <pre>experiment_run_name = generate_uuid(_experiment_id)\neval_result = eval_task.evaluate(\n    model=model,\n    prompt_template=prompt_template,\n    experiment_run_name=experiment_run_name,\n)\n</pre> experiment_run_name = generate_uuid(_experiment_id) eval_result = eval_task.evaluate(     model=model,     prompt_template=prompt_template,     experiment_run_name=experiment_run_name, ) <ul> <li>After the evaluation task is completed, Vertex AI Rapid Eval SDK returns the result of the  run including summary metrics and a detailed metrics table with per-instance (that is per example) metrics.</li> </ul> In\u00a0[27]: Copied! <pre>summary_metrics = eval_result.summary_metrics\nreport_df = eval_result.metrics_table\n</pre> summary_metrics = eval_result.summary_metrics report_df = eval_result.metrics_table In\u00a0[\u00a0]: Copied! <pre>report_df.head(1)\n</pre> report_df.head(1) In\u00a0[\u00a0]: Copied! <pre>summary_metrics\n</pre> summary_metrics <ul> <li>Log the run metrics (both summary and detail) to analyze or compare them in subsequent iterations.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>run_path = f\"{task_id}/prompts/{_experiment_id}/{experiment_run_name}\"\nevals.log_eval_run(\n    experiment_run_id=experiment_run_name,\n    experiment=experiment,\n    eval_result=eval_result,\n    run_path=run_path,\n    tags=tags,\n    metadata=metadata,\n)\n</pre> run_path = f\"{task_id}/prompts/{_experiment_id}/{experiment_run_name}\" evals.log_eval_run(     experiment_run_id=experiment_run_name,     experiment=experiment,     eval_result=eval_result,     run_path=run_path,     tags=tags,     metadata=metadata, ) <ul> <li>View all evaluation runs for an experiment</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_eval_runs(experiment_id=experiment_id)\n</pre> evals.get_eval_runs(experiment_id=experiment_id) <ul> <li>View all evaluation runs in the system across experiments</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_all_eval_runs()\n</pre> evals.get_all_eval_runs() <ul> <li>Define <code>Evals</code> object to access helper functions</li> </ul> In\u00a0[33]: Copied! <pre>evals = Evals()\n</pre> evals = Evals() <ul> <li>Get all experiments</li> </ul> In\u00a0[\u00a0]: Copied! <pre>evals.get_all_experiments()\n</pre> evals.get_all_experiments() <ul> <li>Get a specific experiment using <code>experiment_id</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>experiment_id = \"Prompt with simple language summary\"\nevals.get_experiment(experiment_id=experiment_id)\n</pre> experiment_id = \"Prompt with simple language summary\" evals.get_experiment(experiment_id=experiment_id) In\u00a0[\u00a0]: Copied! <pre>evals.get_eval_runs(experiment_id=experiment_id)\n</pre> evals.get_eval_runs(experiment_id=experiment_id) In\u00a0[\u00a0]: Copied! <pre># Replace  \nexperiment_run_id = \"[your-run_id]\"\nevals.get_eval_run_detail(experiment_run_id=experiment_run_id)\n</pre> # Replace   experiment_run_id = \"[your-run_id]\" evals.get_eval_run_detail(experiment_run_id=experiment_run_id) In\u00a0[\u00a0]: Copied! <pre>run_ids = [\n    \"[your-run_id1]\",\n    \"[your-run_id2]\",\n]\n# list of run ids - strings\nevals.compare_eval_runs(run_ids)\n</pre> run_ids = [     \"[your-run_id1]\",     \"[your-run_id2]\", ] # list of run ids - strings evals.compare_eval_runs(run_ids) <ul> <li>Fetch run details for two experiment run ids you would like to compare.</li> </ul>  Use <code>evals.get_all_eval_runs()</code> or <code>evals.get_eval_runs(experiment_id=experiment_id)</code> to get run ids. In\u00a0[43]: Copied! <pre># Prepare run details to compare\n# @markdown ### Enter experiment run id 1\nrun_1 = \"[your-run_id1]\"  # @param {type:\"string\"}\nrun_1_details = evals.get_eval_run_detail(experiment_run_id=run_1)\nrun_1_details = run_1_details[\n    [\"run_id\", \"dataset_row_id\", \"input_prompt_gcs_uri\", \"output_text\"]\n]\n\n# @markdown ### Enter experiment run id 2\nrun_2 = \"[your-run_id2]\"  # @param {type:\"string\"}\nrun_2_details = evals.get_eval_run_detail(experiment_run_id=run_2)\nrun_2_details = run_2_details[\n    [\"run_id\", \"dataset_row_id\", \"input_prompt_gcs_uri\", \"output_text\"]\n]\n\nrun1_run2 = pd.merge(\n    run_1_details,\n    run_2_details,\n    how=\"outer\",\n    on=[\"dataset_row_id\"],\n    suffixes=(\"_1\", \"_2\"),\n)\nrun1_run2 = run1_run2.rename(\n    columns={\n        \"input_prompt_gcs_uri_1\": \"prompt\",\n        \"output_text_1\": \"response_a\",\n        \"output_text_2\": \"response_b\",\n    }\n)\n</pre> # Prepare run details to compare # @markdown ### Enter experiment run id 1 run_1 = \"[your-run_id1]\"  # @param {type:\"string\"} run_1_details = evals.get_eval_run_detail(experiment_run_id=run_1) run_1_details = run_1_details[     [\"run_id\", \"dataset_row_id\", \"input_prompt_gcs_uri\", \"output_text\"] ]  # @markdown ### Enter experiment run id 2 run_2 = \"[your-run_id2]\"  # @param {type:\"string\"} run_2_details = evals.get_eval_run_detail(experiment_run_id=run_2) run_2_details = run_2_details[     [\"run_id\", \"dataset_row_id\", \"input_prompt_gcs_uri\", \"output_text\"] ]  run1_run2 = pd.merge(     run_1_details,     run_2_details,     how=\"outer\",     on=[\"dataset_row_id\"],     suffixes=(\"_1\", \"_2\"), ) run1_run2 = run1_run2.rename(     columns={         \"input_prompt_gcs_uri_1\": \"prompt\",         \"output_text_1\": \"response_a\",         \"output_text_2\": \"response_b\",     } ) <ul> <li>Prepare pairwise comparison file to visualize using LLM Comparator</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from llm_comparator import (comparison, llm_judge_runner, model_helper,\n                            rationale_bullet_generator,\n                            rationale_cluster_generator)\n\ninputs = run1_run2.to_dict(orient=\"records\")\n\ncustom_fields_schema = [\n    {\"name\": \"prompt_id\", \"type\": \"string\"},\n]\n\n# Initialize the models-calling classes.\ngenerator = model_helper.VertexGenerationModelHelper(model_name=\"gemini-2.0-flash-001\")\nembedder = model_helper.VertexEmbeddingModelHelper()\n\n# Initialize the instances that run work on the models.\njudge = llm_judge_runner.LLMJudgeRunner(generator)\nbulletizer = rationale_bullet_generator.RationaleBulletGenerator(generator)\nclusterer = rationale_cluster_generator.RationaleClusterGenerator(generator, embedder)\n\n# Configure and run the comparative evaluation.\ncomparison_result = comparison.run(\n    inputs, judge, bulletizer, clusterer, judge_opts={\"num_repeats\": 2}\n)\n\n# Write the results to a JSON file that can be loaded in\n# https://pair-code.github.io/llm-comparator\nfile_path = \"assets/run1_run2_compare.json\"\ncomparison.write(comparison_result, file_path)\n</pre> from llm_comparator import (comparison, llm_judge_runner, model_helper,                             rationale_bullet_generator,                             rationale_cluster_generator)  inputs = run1_run2.to_dict(orient=\"records\")  custom_fields_schema = [     {\"name\": \"prompt_id\", \"type\": \"string\"}, ]  # Initialize the models-calling classes. generator = model_helper.VertexGenerationModelHelper(model_name=\"gemini-2.0-flash-001\") embedder = model_helper.VertexEmbeddingModelHelper()  # Initialize the instances that run work on the models. judge = llm_judge_runner.LLMJudgeRunner(generator) bulletizer = rationale_bullet_generator.RationaleBulletGenerator(generator) clusterer = rationale_cluster_generator.RationaleClusterGenerator(generator, embedder)  # Configure and run the comparative evaluation. comparison_result = comparison.run(     inputs, judge, bulletizer, clusterer, judge_opts={\"num_repeats\": 2} )  # Write the results to a JSON file that can be loaded in # https://pair-code.github.io/llm-comparator file_path = \"assets/run1_run2_compare.json\" comparison.write(comparison_result, file_path) <ul> <li>You can now upload this file on LLM Comparator tool/app at https://pair-code.github.io/llm-comparator/ and analyze the results. Refer to documentation on how to use the tool.</li> </ul> <p></p> <p>Based on the analysis, you can identify loss patterns and seed idea for next experiment. For example, changing prompt template, system instruction or model configuration. Add a new experiment and run evaluations until you meet the success criteria for the evaluation task.</p> In\u00a0[\u00a0]: Copied! <pre># # Delete BigQuery Dataset using bq utility\n# ! bq rm -r -f -d {BQ_DATASET_ID}\n\n# # Delete GCS bucket\n# ! gcloud storage rm --recursive {STAGING_BUCKET_URI}\n</pre> # # Delete BigQuery Dataset using bq utility # ! bq rm -r -f -d {BQ_DATASET_ID}  # # Delete GCS bucket # ! gcloud storage rm --recursive {STAGING_BUCKET_URI}"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#evals-playbook-experiment-evaluate-analyze","title":"Evals Playbook: Experiment, Evaluate &amp; Analyze\u00b6","text":"<p>This notebook shows you how to define experiments, run evaluations to assess model performance, and analyze evaluation results including side-by-side comparison of results across different experiments and runs. The notebook performs following steps:</p> <ul> <li>Define the evaluation task</li> <li>Prepare evaluation dataset</li> <li>Define an experiment by:<ul> <li>Configuring the model</li> <li>Setting prompt and system instruction</li> <li>Establishing evaluation criteria (metrics)</li> </ul> </li> <li>Run evaluations using Vertex AI Rapid Eval SDK</li> <li>Log detailed results and summarizing through aggregated metrics.</li> <li>Side-by-side comparison of evaluation runs for a comprehensive analysis.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#0-pre-requisites","title":"\ud83d\udea7 0. Pre-requisites\u00b6","text":"<p>Make sure that you have prepared the environment following steps in 0_gemini_evals_playbook_setup.ipynb. If the 0_gemini_evals_playbook_setup notebook has been run successfully, the following are set up:</p> <ul> <li>GCP project and APIs to run the eval pipeline</li> <li>All the required IAM permissions</li> <li>Environment to run the notebooks</li> <li>Bigquery datasets and tables to track evaluation results</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#read-configurations","title":"Read configurations\u00b6","text":"<p>The configuration saved previously in 0_gemini_evals_playbook_setup.ipynb will be used for initializing variables.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#initialize-vertex-ai-sdk","title":"Initialize Vertex AI SDK\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#define-evals-object","title":"Define <code>Evals</code> object\u00b6","text":"<p><code>Evals</code> is a helper class helps to define tasks, experiments and log evaluation results. Define an instance of <code>Evals</code> class to use in the rest of the notebook.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#1-define-and-configure-evaluation-task-and-experiment","title":"\ud83d\udee0\ufe0f 1. Define and configure evaluation task and experiment\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#define-evaluation-task","title":"Define Evaluation Task\u00b6","text":"<p>An evaluation task defines the task model(s) will be evaluated on. The <code>task_id</code> is analogous to a workspace to group experiments and corresponding evaluation runs. This notebook premises on summarization of PubMed articles as the task.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#define-experiment","title":"Define Experiment\u00b6","text":"<p>An experiment in Evals Playbook is defined by configuring</p> <ul> <li>Dataset</li> <li>Model and model configuration</li> <li>Prompt</li> </ul> <p>Each experiment has an <code>experiment_id</code> and associated with a <code>task_id</code>. This sectio defines the required components.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#configure-model","title":"Configure Model\u00b6","text":"<p>Define the Gemini model you want to evaluate your task on including name, configuration settings such as temperature and safety settings.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#prepare-prompt","title":"Prepare Prompt\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#prepare-evaluation-dataset","title":"Prepare evaluation dataset\u00b6","text":"<p>This notebook uses a sample of PubMed articles that are hosted on HuggingFace.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#configure-metrics","title":"Configure Metrics\u00b6","text":"<p>In this section, you configure the evaluation criteria for your task. You can choose from the built-in metrics (or metric bundles) from Vertex AI Rapid Eval SDK or define a custom metric.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#add-experiment","title":"Add Experiment\u00b6","text":"<p>Now that you have defined model, prompt, dataset and eval criteria (metrics), let's add them to an experiment and start logging.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#2-run-experiments-for-an-evaluation-task","title":"\ud83d\ude80 2. Run experiment(s) for an evaluation task\u00b6","text":"<p>The experiment is now ready to run an evaluation task using the model, prompt, dataset and metrics configured.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#3-analyze-results","title":"\ud83d\udcca 3. Analyze results\u00b6","text":"<p>This section shows a few ways to analyze and compare results. Since the results are stored in BigQuery tables, there are multiple ways to analyze them</p> <ol> <li>Use BigQuery SQL queries</li> <li>Use Pandas dataframe and BigQuery</li> <li>Build Looker dashboards</li> <li>Use tools such as LLM Comparator from Google's PAIR team</li> </ol> <ul> <li>and more ...</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#get-experiments-runs-and-run-details","title":"Get experiments, runs and run details\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#basic-analysis","title":"Basic analysis\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#summary-metrics","title":"Summary metrics\u00b6","text":"<p>Compare all runs for a given experiment at a summary level. This can be useful, when you run the same experiment at different time snapshots and allow you to see if there is any variance or change in eval metrics (how robust the model is).</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#detailed-metrics","title":"Detailed metrics\u00b6","text":"<p>You can get a detail eval result for a given experiment run at example level. This helps you to analyze and identify any loss patterns. To find run_id for previous runs, see gemini_evals_plapbook(schema) &gt;&gt; eval_runs(table) &gt;&gt; run_id (column) on bigquery</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#compare-eval-runs-across-experiments","title":"Compare eval runs across experiments\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#compare-eval-runs-at-summary-level","title":"Compare eval runs at summary level\u00b6","text":"<p>You can compare summary metrics for multiple runs side-by-side even across different experiments. For example, you can compare eval runs</p> <ul> <li>For the same prompt at different temperature settings</li> <li>Same model setting but different prompt templates or system instruction</li> </ul> <p>Pass a list of experiment run ids and compare them side-by-side</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#llm-comparator-for-analyzing-side-by-side-llm-evaluation-results","title":"LLM Comparator for analyzing side-by-side LLM evaluation results\u00b6","text":"<p>To visualize model responses from different runs, we use LLM Comparator Python Library from Google PAIR team to compare model responses from two runs side-by-side. The tool coordinates the three phases of comparative evaluation: judging, bulletizing, and clustering and the results can be uploaded on LLM Comparator app to view and analyze further.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/1_gemini_evals_playbook_evaluate/#cleaning-up","title":"\ud83e\uddf9 Cleaning up\u00b6","text":"<p>Uncomment the following cells to clean up resources created as part of the Evals Playbook.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/","title":"Evals Playbook: Optimize with grid search of experiments","text":"In\u00a0[1]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Vertex AI: Gemini Evaluations Playbook  Optimize with grid search of experiments  Run in Colab       Run in Colab Enterprise       View on GitHub       Open in Vertex AI Workbench      In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\n\nmodule_path = os.path.abspath(os.path.join(\"..\"))\nsys.path.append(module_path)\nprint(f\"module_path: {module_path}\")\n\n# Import all the parameters\nfrom utils.config import (LOCATION, PROJECT_ID, STAGING_BUCKET,\n                          STAGING_BUCKET_URI)\nfrom utils.evals_playbook import Evals, generate_uuid\n</pre> import os import sys  module_path = os.path.abspath(os.path.join(\"..\")) sys.path.append(module_path) print(f\"module_path: {module_path}\")  # Import all the parameters from utils.config import (LOCATION, PROJECT_ID, STAGING_BUCKET,                           STAGING_BUCKET_URI) from utils.evals_playbook import Evals, generate_uuid In\u00a0[4]: Copied! <pre>import datetime\nimport itertools\nimport re\n\nimport pandas as pd\nimport vertexai\nfrom datasets import Dataset, load_dataset\nfrom vertexai.evaluation import EvalTask, constants\nfrom vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,\n                                        HarmCategory, SafetySetting)\n</pre> import datetime import itertools import re  import pandas as pd import vertexai from datasets import Dataset, load_dataset from vertexai.evaluation import EvalTask, constants from vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,                                         HarmCategory, SafetySetting) In\u00a0[\u00a0]: Copied! <pre>vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)\n\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n\n# pandas display full column values\npd.set_option(\"display.max_colwidth\", None)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\n</pre> vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)  print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\")  # pandas display full column values pd.set_option(\"display.max_colwidth\", None) pd.set_option(\"display.max_rows\", None) pd.set_option(\"display.max_columns\", None) In\u00a0[6]: Copied! <pre># Define eval object\nevals = Evals()\n</pre> # Define eval object evals = Evals() In\u00a0[7]: Copied! <pre>param_grid = {\n    \"prompt\": [  # Format: (prompt_id, prompt_description, prompt_template)\n        (\n            \"prompt_template_1\",\n            \"Single Sentence\",\n            \"Summarize this PubMed article: {context}\",\n        ),\n        (\"prompt_template_2\", \"Structured\", \"Article: {context}. Summary:\"),\n    ],\n    \"temperature\": [0.0, 0.1, 0.2],\n}\n</pre> param_grid = {     \"prompt\": [  # Format: (prompt_id, prompt_description, prompt_template)         (             \"prompt_template_1\",             \"Single Sentence\",             \"Summarize this PubMed article: {context}\",         ),         (\"prompt_template_2\", \"Structured\", \"Article: {context}. Summary:\"),     ],     \"temperature\": [0.0, 0.1, 0.2], } In\u00a0[8]: Copied! <pre>system_instruction = \"\"\"Instruction: You are a medical researcher writing a plain language Summary of your Article for a layperson.\n\nTranslate any medical terms to simple english explanations.\nUse first-person 'We'.  Use short bullet points addressing following\n- Purpose: What was the purpose of the study?\n- Research: What did the researchers do?\n- Findings: What did they find?\n- Implications: What does this mean for me?\"\n\"\"\"\n\n#\nsafety_settings = [\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n    SafetySetting(\n        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n        threshold=HarmBlockThreshold.BLOCK_NONE,\n    ),\n]\n\n#\nmodel_name = \"gemini-2.0-flash-001\"\n</pre> system_instruction = \"\"\"Instruction: You are a medical researcher writing a plain language Summary of your Article for a layperson.  Translate any medical terms to simple english explanations. Use first-person 'We'.  Use short bullet points addressing following - Purpose: What was the purpose of the study? - Research: What did the researchers do? - Findings: What did they find? - Implications: What does this mean for me?\" \"\"\"  # safety_settings = [     SafetySetting(         category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_HARASSMENT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ),     SafetySetting(         category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,         threshold=HarmBlockThreshold.BLOCK_NONE,     ), ]  # model_name = \"gemini-2.0-flash-001\" In\u00a0[9]: Copied! <pre>#\nmetrics = [\n    constants.Metric.ROUGE_1,\n    constants.Metric.ROUGE_L_SUM,\n    constants.Metric.BLEU,\n    constants.Metric.FLUENCY,\n    constants.Metric.COHERENCE,\n    constants.Metric.SAFETY,\n    constants.Metric.GROUNDEDNESS,\n    constants.Metric.SUMMARIZATION_QUALITY,\n]\n\n# build a metric config object for tracking\nmetric_config = [\n    {\"metric_name\": metric, \"type\": \"prebuilt\", \"metric_scorer\": \"Vertex AI\"}\n    for metric in metrics\n]\n</pre> # metrics = [     constants.Metric.ROUGE_1,     constants.Metric.ROUGE_L_SUM,     constants.Metric.BLEU,     constants.Metric.FLUENCY,     constants.Metric.COHERENCE,     constants.Metric.SAFETY,     constants.Metric.GROUNDEDNESS,     constants.Metric.SUMMARIZATION_QUALITY, ]  # build a metric config object for tracking metric_config = [     {\"metric_name\": metric, \"type\": \"prebuilt\", \"metric_scorer\": \"Vertex AI\"}     for metric in metrics ] In\u00a0[10]: Copied! <pre># Prompt Template\nprompt_template = \"Article: {context} \\nSummary:\"\n</pre> # Prompt Template prompt_template = \"Article: {context} \\nSummary:\" In\u00a0[11]: Copied! <pre>from google.cloud import storage\n\n# # OPTION 1:\n# # Load prepared dataset from GCS\n# # Path to your CSV file in GCS\n# file_name = \"pubmed_summary.csv\"\n# file_path = f\"gs://{STAGING_BUCKET}/{file_name}\"\n\n# # Read the CSV file into pandas DataFrame\n# eval_dataset = pd.read_csv(file_path)\n\n\n# OPTION 2:\n# Load and prepare public dataset from HuggingFace\nds_stream = load_dataset(\n    \"ccdv/pubmed-summarization\", \"document\", split=\"test\", streaming=True\n)\nnum_rows = 10\ndataset = Dataset.from_list(list(itertools.islice(ds_stream, num_rows)))\n\n# convert HuggingFace dataset to Pandas dataframe\neval_dataset = dataset.to_pandas()\n# rename columns as per Vertex AI Rapid Eval SDK defaults\neval_dataset.columns = [\"context\", \"reference\"]\n# add instruction for calculating metrics (not all metrics need instruction)\neval_dataset[\"instruction\"] = system_instruction\n# add prompt column\neval_dataset[\"prompt\"] = eval_dataset[\"context\"].apply(\n    lambda x: prompt_template.format(context=x)\n)\n# add prompt id for tracking\neval_dataset[\"dataset_row_id\"] = [f\"dataset_row_{i}\" for i in eval_dataset.index]\n</pre> from google.cloud import storage  # # OPTION 1: # # Load prepared dataset from GCS # # Path to your CSV file in GCS # file_name = \"pubmed_summary.csv\" # file_path = f\"gs://{STAGING_BUCKET}/{file_name}\"  # # Read the CSV file into pandas DataFrame # eval_dataset = pd.read_csv(file_path)   # OPTION 2: # Load and prepare public dataset from HuggingFace ds_stream = load_dataset(     \"ccdv/pubmed-summarization\", \"document\", split=\"test\", streaming=True ) num_rows = 10 dataset = Dataset.from_list(list(itertools.islice(ds_stream, num_rows)))  # convert HuggingFace dataset to Pandas dataframe eval_dataset = dataset.to_pandas() # rename columns as per Vertex AI Rapid Eval SDK defaults eval_dataset.columns = [\"context\", \"reference\"] # add instruction for calculating metrics (not all metrics need instruction) eval_dataset[\"instruction\"] = system_instruction # add prompt column eval_dataset[\"prompt\"] = eval_dataset[\"context\"].apply(     lambda x: prompt_template.format(context=x) ) # add prompt id for tracking eval_dataset[\"dataset_row_id\"] = [f\"dataset_row_{i}\" for i in eval_dataset.index] In\u00a0[\u00a0]: Copied! <pre># create and log task\ntask_id = \"task_summarization\"\ntask = evals.Task(\n    task_id=task_id,\n    task_desc=\"summarize pubmed articles\",\n    create_datetime=datetime.datetime.now(),\n    update_datetime=datetime.datetime.now(),\n    tags=[\"pubmed\"],\n)\nevals.log_task(task)\n\n#\nevals.get_all_tasks()\n</pre> # create and log task task_id = \"task_summarization\" task = evals.Task(     task_id=task_id,     task_desc=\"summarize pubmed articles\",     create_datetime=datetime.datetime.now(),     update_datetime=datetime.datetime.now(),     tags=[\"pubmed\"], ) evals.log_task(task)  # evals.get_all_tasks() In\u00a0[\u00a0]: Copied! <pre># Note thar this cell can take time to finish!\nfrom sklearn.model_selection import ParameterGrid\n\ngrid = ParameterGrid(param_grid)\nexperiment_run_ids = []\n\n# print(list(grid))\n\nfor indx, params in enumerate(grid):\n\n    prompt_id, prompt_description, prompt_template = params[\"prompt\"]\n    temperature = params[\"temperature\"]\n\n    # Print above parameters, one in each line\n    # print(f'prompt_id: {prompt_id}\\nprompt_description: {prompt_description}\\nprompt_template: {prompt_template}\\ntemperature: {temperature}\\n')\n\n    # Track status\n    print(\"Running ........\")\n    print(f\"{indx+1}. {params}\")\n\n    # Set up the experiment\n    experiment_id = f\"prompt-{prompt_id}-{temperature}\"\n    experiment_desc = f\"Simple language summary with prompt {prompt_id} and temperature {temperature} \"\n    tags = [\"pubmed\"]\n    metadata = {}\n\n    # print(experiment_id, experiment_desc)\n\n    generation_config = {\"temperature\": temperature}\n\n    model = GenerativeModel(\n        model_name=model_name,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        system_instruction=system_instruction,\n        # TODO: Add tools and tool_config\n    )\n\n    # Configure and log prompt\n    prompt = evals.Prompt(\n        prompt_id=prompt_id,\n        prompt_description=prompt_description,\n        prompt_type=\"single-turn\",  # single-turn, chat,\n        is_multimodal=False,\n        system_instruction=system_instruction,\n        prompt_template=prompt_template,\n        create_datetime=datetime.datetime.now(),\n        update_datetime=datetime.datetime.now(),\n        tags=tags,\n    )\n    evals.log_prompt(prompt)\n\n    # Configure and log experiment\n    experiment = evals.log_experiment(\n        task_id=task_id,\n        experiment_id=experiment_id,\n        experiment_desc=experiment_desc,\n        prompt=prompt,\n        model=model,\n        metric_config=metric_config,\n        tags=tags,\n    )\n\n    # Run Experiment\n    _experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower())\n    eval_task = EvalTask(\n        dataset=eval_dataset, metrics=metrics, experiment=_experiment_id\n    )\n\n    experiment_run_name = generate_uuid(_experiment_id)\n    experiment_run_ids.append(experiment_run_name)\n    eval_result = eval_task.evaluate(\n        model=model,\n        prompt_template=prompt_template,\n        experiment_run_name=experiment_run_name,\n    )\n\n    run_path = f\"{task_id}/prompts/{_experiment_id}/{experiment_run_name}\"\n    evals.log_eval_run(\n        experiment_run_id=experiment_run_name,\n        experiment=experiment,\n        eval_result=eval_result,\n        run_path=run_path,\n        tags=tags,\n        metadata=metadata,\n    )\n</pre> # Note thar this cell can take time to finish! from sklearn.model_selection import ParameterGrid  grid = ParameterGrid(param_grid) experiment_run_ids = []  # print(list(grid))  for indx, params in enumerate(grid):      prompt_id, prompt_description, prompt_template = params[\"prompt\"]     temperature = params[\"temperature\"]      # Print above parameters, one in each line     # print(f'prompt_id: {prompt_id}\\nprompt_description: {prompt_description}\\nprompt_template: {prompt_template}\\ntemperature: {temperature}\\n')      # Track status     print(\"Running ........\")     print(f\"{indx+1}. {params}\")      # Set up the experiment     experiment_id = f\"prompt-{prompt_id}-{temperature}\"     experiment_desc = f\"Simple language summary with prompt {prompt_id} and temperature {temperature} \"     tags = [\"pubmed\"]     metadata = {}      # print(experiment_id, experiment_desc)      generation_config = {\"temperature\": temperature}      model = GenerativeModel(         model_name=model_name,         generation_config=generation_config,         safety_settings=safety_settings,         system_instruction=system_instruction,         # TODO: Add tools and tool_config     )      # Configure and log prompt     prompt = evals.Prompt(         prompt_id=prompt_id,         prompt_description=prompt_description,         prompt_type=\"single-turn\",  # single-turn, chat,         is_multimodal=False,         system_instruction=system_instruction,         prompt_template=prompt_template,         create_datetime=datetime.datetime.now(),         update_datetime=datetime.datetime.now(),         tags=tags,     )     evals.log_prompt(prompt)      # Configure and log experiment     experiment = evals.log_experiment(         task_id=task_id,         experiment_id=experiment_id,         experiment_desc=experiment_desc,         prompt=prompt,         model=model,         metric_config=metric_config,         tags=tags,     )      # Run Experiment     _experiment_id = re.sub(\"[^0-9a-zA-Z]\", \"-\", experiment_id.lower())     eval_task = EvalTask(         dataset=eval_dataset, metrics=metrics, experiment=_experiment_id     )      experiment_run_name = generate_uuid(_experiment_id)     experiment_run_ids.append(experiment_run_name)     eval_result = eval_task.evaluate(         model=model,         prompt_template=prompt_template,         experiment_run_name=experiment_run_name,     )      run_path = f\"{task_id}/prompts/{_experiment_id}/{experiment_run_name}\"     evals.log_eval_run(         experiment_run_id=experiment_run_name,         experiment=experiment,         eval_result=eval_result,         run_path=run_path,         tags=tags,         metadata=metadata,     ) <ul> <li>Fetch run details</li> </ul>  Use <code>evals.get_all_eval_runs()</code> or <code>evals.get_eval_runs(experiment_id=experiment_id)</code> to get run ids. In\u00a0[\u00a0]: Copied! <pre>evals.get_all_eval_runs()\n</pre> evals.get_all_eval_runs() In\u00a0[\u00a0]: Copied! <pre># To find run_id for previous runs, see \n# gemini_evals_plapbook(schema) &gt;&gt; eval_runs(table) &gt;&gt; run_id (column) on bigquery\nevals.get_eval_run_detail(\n    experiment_run_id=\"[your_run_id]\"\n)\n</pre> # To find run_id for previous runs, see  # gemini_evals_plapbook(schema) &gt;&gt; eval_runs(table) &gt;&gt; run_id (column) on bigquery evals.get_eval_run_detail(     experiment_run_id=\"[your_run_id]\" ) In\u00a0[16]: Copied! <pre># Set the task_id to perform the search\ntask_id = \"task_summarization\"\n\n# Metrics to be used for grid search\nopt_metrics = [\n    \"ROUGE_1\",\n    \"BLEU\",\n]  # Options: \"ROUGE_1\", \"ROUGE_L_SUM\", \"BLEU\", \"FLUENCY\", \"COHERENCE\", \"SAFETY\", \"GROUNDEDNESS\", \"SUMMARIZATION_QUALITY\", \"SUMMARIZATION_VERBOSITY\", \"SUMMARIZATION_HELPFULNESS\"\n\n# Paramaters to be retrieved from grid search\nopt_params = [\n    \"prompt_template\",\n    \"temperature\",\n]  # Options: \"experiment_desc\", \"prompt_template\", \"temperature\", \"system_instruction\", \"model_name\"\n\n# Use run_ids collected during grid search: experiment_run_ids\n</pre> # Set the task_id to perform the search task_id = \"task_summarization\"  # Metrics to be used for grid search opt_metrics = [     \"ROUGE_1\",     \"BLEU\", ]  # Options: \"ROUGE_1\", \"ROUGE_L_SUM\", \"BLEU\", \"FLUENCY\", \"COHERENCE\", \"SAFETY\", \"GROUNDEDNESS\", \"SUMMARIZATION_QUALITY\", \"SUMMARIZATION_VERBOSITY\", \"SUMMARIZATION_HELPFULNESS\"  # Paramaters to be retrieved from grid search opt_params = [     \"prompt_template\",     \"temperature\", ]  # Options: \"experiment_desc\", \"prompt_template\", \"temperature\", \"system_instruction\", \"model_name\"  # Use run_ids collected during grid search: experiment_run_ids In\u00a0[\u00a0]: Copied! <pre># Comparision of runs in experiment grid\nevals.compare_eval_runs(experiment_run_ids)\n</pre> # Comparision of runs in experiment grid evals.compare_eval_runs(experiment_run_ids) In\u00a0[\u00a0]: Copied! <pre># Outcome of gridsearch\nevals.grid_search(\n    task_id=task_id,\n    experiment_run_ids=experiment_run_ids,\n    opt_metrics=opt_metrics,\n    opt_params=opt_params,\n)\n</pre> # Outcome of gridsearch evals.grid_search(     task_id=task_id,     experiment_run_ids=experiment_run_ids,     opt_metrics=opt_metrics,     opt_params=opt_params, ) In\u00a0[19]: Copied! <pre># # Delete BigQuery Dataset using bq utility\n# ! bq rm -r -f -d {BQ_DATASET_ID}\n\n# # Delete GCS bucket\n# ! gcloud storage rm --recursive {STAGING_BUCKET_URI}\n</pre> # # Delete BigQuery Dataset using bq utility # ! bq rm -r -f -d {BQ_DATASET_ID}  # # Delete GCS bucket # ! gcloud storage rm --recursive {STAGING_BUCKET_URI}"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#evals-playbook-optimize-with-grid-search-of-experiments","title":"Evals Playbook: Optimize with grid search of experiments\u00b6","text":"<p>This notebook shows you systematically exploring different experiment configurations  by testing various prompt templates or model settings (like temperature), or combinations of these using a grid-search style approach. The notebook performs following steps:</p> <ul> <li>Define the evaluation task</li> <li>Prepare evaluation dataset</li> <li>Define an experiment by:<ul> <li>Configuring the model</li> <li>Setting prompt and system instruction</li> <li>Establishing evaluation criteria (metrics)</li> </ul> </li> <li>Run evaluations using Vertex AI Rapid Eval SDK</li> <li>Log detailed results and summarizing through aggregated metrics.</li> <li>Side-by-side comparison of evaluation runs for a comprehensive analysis.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#0-pre-requisites","title":"\ud83d\udea7 0. Pre-requisites\u00b6","text":"<p>Make sure that you have completed the initial setup process using 0_gemini_evals_playbook_setup.ipynb. If the 0_gemini_evals_playbook_setup notebook has been run successfully, the following are set up:</p> <ul> <li><p>GCP project and APIs to run the eval pipeline</p> </li> <li><p>All the required IAM permissions</p> </li> <li><p>Environment to run the notebooks</p> </li> <li><p>Bigquery datasets and tables to track evaluation results</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#read-configurations","title":"Read configurations\u00b6","text":"<p>The configuration saved previously in 0_gemini_evals_playbook_setup.ipynb will be used for initializing variables.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#initialize-vertex-ai-sdk","title":"Initialize Vertex AI SDK\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#define-evals-object","title":"Define <code>Evals</code> object\u00b6","text":"<p><code>Evals</code> is a helper class helps to define tasks, experiments and log evaluation results. Define an instance of <code>Evals</code> class to use in the rest of the notebook.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#1-configure-parameter-grid-to-run-experiments","title":"\ud83d\udee0\ufe0f 1. Configure parameter grid to run experiments\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#define-exploration-space-as-grid","title":"Define exploration space as grid\u00b6","text":"<p>Define a dictionary with parameters names (str) as keys such as prompt template or temperature. For each key, specify a list of settings to try as values, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. This is similar to defining grid search in ML.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#configure-model","title":"Configure Model\u00b6","text":"<p>Define the Gemini model you want to evaluate your task on including name, configuration settings such as temperature and safety settings.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#configure-metrics","title":"Configure Metrics\u00b6","text":"<p>In this section, you configure the evaluation criteria for your task. You can choose from the built-in metrics (or metric bundles) from Vertex AI Rapid Eval SDK or define a custom metric.</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#prepare-evaluation-dataset","title":"Prepare evaluation dataset\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#define-evaluation-task","title":"Define Evaluation task\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#2-run-experiments-on-the-grid","title":"\u23f3 2. Run experiments on the grid\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#3-grid-search","title":"\ud83d\udd0d 3. Grid search\u00b6","text":"<p>Search the grid for optimal configuration with respect to metrics of choice</p>"},{"location":"genai-on-vertex-ai/gemini/evals_playbook/notebooks/2_gemini_evals_playbook_gridsearch/#cleaning-up","title":"\ud83e\uddf9 Cleaning up\u00b6","text":"<p>Uncomment the following cells to clean up resources created as part of the Evals Playbook.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/","title":"Eval Recipes for upgrading to Gemini 2","text":"<p>This directory contains Eval Recipes for common GenAI tasks.  The goal is to accelerate the process of upgrading to the latest version of Gemini and minimize the risk of regression. - An Eval Recipe is a minimalistic example of an automated evaluation that includes a prompt template and an evaluation dataset.  - The included evaluation datasets are very small, which makes it possible to run each Eval Recipe in less than 1 minute. - Eval Recipes are lightweight and easy to learn - the entire configuration for most tasks fits on one screen. - Each Eval Recipe includes 3 alternative implementations:     - Colab notebook based on Vertex GenAI Evaluation Service     - Command line script based on Vertex GenAI Evaluation Service     - Command line script based on Promptfoo - Eval Recipes can be customized by replacing the prompt template and the evaluation dataset.</p> Eval Recipe Vertex AI Colab Vertex AI Script Promptfoo Document QnA view view view Summarization view view view Text Classification view view view Multi-turn Chat view view view Instruction Following view view view Image-Prompt Alignment view view view RAG Embeddings view view"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/promptfoo/","title":"Document Question Answering","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a Document Question Answering prompt with Gemini 1.0 and Gemini 2.0 using a labeled dataset and open source evaluation tool Promptfoo.</p> <p></p> <ul> <li> <p>Use case: answer questions based on information from the given document.</p> </li> <li> <p>The Evaluation Dataset is based on SQuAD2.0. It includes 6 documents stored as plain text files, and a JSONL file that provides ground truth labels: <code>dataset.jsonl</code>. Each record in this file includes 3 attributes wrapped in the <code>vars</code> object. This structure allows Promptfoo to specify the variables needed to populate prompt templates (document and question), as well as the ground truth label required to score the accuracy of model responses:</p> <ul> <li><code>document</code>: relative path to the plain text document file</li> <li><code>question</code>: the question that we want to ask about this particular document</li> <li><code>answer</code>: expected correct answer or special code <code>ANSWER_NOT_FOUND</code> used to verify that the model does not hallucinate answers when the document does not provide enough information to answer the given question.</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with two prompt variables (<code>document</code> and <code>question</code>) that are automatically populated from our dataset.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all Promptfoo configuration:</p> <ul> <li><code>providers</code>: list of models that will be evaluated</li> <li><code>prompts</code>: location of the prompt template file</li> <li><code>tests</code>: location of the labeled dataset file</li> <li><code>defaultTest</code>: defines the scoring logic:<ol> <li><code>type: factuality</code> uses an Autorater (aka LLM Judge) to compare the model answer with our ground truth label and rate its correctness</li> <li><code>value: \"{{answer}}\"</code> instructs Promptfoo to use the dataset attribute \"answer\" as the ground truth label</li> </ol> </li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li> <p>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</p> <pre><code>cd document_qna/promptfoo\npromptfoo eval\n</code></pre> </li> <li> <p>Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the configuration file <code>promptfooconfig.yaml</code> to a new folder.</li> <li>Add your labeled dataset file with JSONL schema similar to <code>dataset.jsonl</code>. </li> <li>Save your prompt template to <code>prompt_template.txt</code> and make sure that the template variables map to the variables defined in your dataset.</li> <li>That's it! You are ready to run <code>promptfoo eval</code>. If needed, add alternative prompt templates or additional metrics to promptfooconfig.yaml as explained here.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_colab/document_qna_eval/","title":"Document qna eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     https://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"as is\" basis,\n# without warranties or conditions of any kind, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at #     https://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"as is\" basis, # without warranties or conditions of any kind, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: answer questions based on information from the given document.</p> </li> <li><p>Metric: this eval uses an Autorater (LLM Judge) to rate Question Answering Quality.</p> </li> <li><p>Evaluation Dataset is based on SQuAD2.0. It includes 6 documents stored as plain text files, and a JSONL file <code>dataset.jsonl</code> that provides ground truth labels. Each record in this file includes 3 attributes:</p> <ul> <li><code>document_uri</code>: relative path to the plain text document file</li> <li><code>question</code>: the question that we want to ask about this particular document</li> <li><code>reference</code>: expected correct answer or special code <code>ANSWER_NOT_FOUND</code> used to verify that the model does not hallucinate answers when the document does not provide enough information to answer the given question.</li> </ul> </li> <li><p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with two prompt variables (<code>document</code> and <code>question</code>) that are automatically populated from our dataset.</p> </li> </ul> <p>Step 1 of 4: Configure eval settings</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id            # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=eval-document-qna     # Creates Vertex AI Experiment to track the eval runs\nMODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model\nMODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model\nDATASET_URI=\"gs://gemini_assets/document_qna/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=gs://gemini_assets/document_qna/prompt_template.txt  # Text file in Google Cloud Storage\n</pre> %%writefile .env PROJECT_ID=your-project-id            # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=eval-document-qna     # Creates Vertex AI Experiment to track the eval runs MODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model MODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model DATASET_URI=\"gs://gemini_assets/document_qna/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=gs://gemini_assets/document_qna/prompt_template.txt  # Text file in Google Cloud Storage <p>Step 2 of 4: Install Python libraries</p> In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) <p>Step 3 of 4: Authenticate to Google Cloud (requires permission to open a popup window)</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nload_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> import os import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) <p>Step 4 of 4: Run the eval on both models and compare the Accuracy scores</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport pandas as pd\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory\n\ndef load_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_dataset(dataset_uri: str):\n    jsonl = load_file(dataset_uri)\n    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n    df = pd.DataFrame(samples)\n    df['document_text'] = df['document_uri'].apply(lambda document_uri: load_file(document_uri))\n    return df[['question', 'reference', 'document_text']]\n\ndef run_eval(model: str) -&gt; EvalResult:\n  timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n  return EvalTask(\n      dataset=load_dataset(os.getenv(\"DATASET_URI\")),\n      metrics=[MetricPromptTemplateExamples.Pointwise.QUESTION_ANSWERING_QUALITY],\n      experiment=os.getenv('EXPERIMENT_NAME')\n  ).evaluate(\n      model=GenerativeModel(model),\n      prompt_template=load_file(os.getenv(\"PROMPT_TEMPLATE_URI\")),\n      experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"\n  )\n\nbaseline_results = run_eval(os.getenv(\"MODEL_BASELINE\"))\ncandidate_results = run_eval(os.getenv(\"MODEL_CANDIDATE\"))\nclear_output()\nprint(f\"Baseline model score: {baseline_results.summary_metrics['question_answering_quality/mean']*20:.1f}%\")\nprint(f\"Candidate model score: {candidate_results.summary_metrics['question_answering_quality/mean']*20:.1f}%\")\n</pre> import json import pandas as pd from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory  def load_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_dataset(dataset_uri: str):     jsonl = load_file(dataset_uri)     samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]     df = pd.DataFrame(samples)     df['document_text'] = df['document_uri'].apply(lambda document_uri: load_file(document_uri))     return df[['question', 'reference', 'document_text']]  def run_eval(model: str) -&gt; EvalResult:   timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()   return EvalTask(       dataset=load_dataset(os.getenv(\"DATASET_URI\")),       metrics=[MetricPromptTemplateExamples.Pointwise.QUESTION_ANSWERING_QUALITY],       experiment=os.getenv('EXPERIMENT_NAME')   ).evaluate(       model=GenerativeModel(model),       prompt_template=load_file(os.getenv(\"PROMPT_TEMPLATE_URI\")),       experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"   )  baseline_results = run_eval(os.getenv(\"MODEL_BASELINE\")) candidate_results = run_eval(os.getenv(\"MODEL_CANDIDATE\")) clear_output() print(f\"Baseline model score: {baseline_results.summary_metrics['question_answering_quality/mean']*20:.1f}%\") print(f\"Candidate model score: {candidate_results.summary_metrics['question_answering_quality/mean']*20:.1f}%\")  <p>You can view all prompts and model responses by calling <code>candidate_results.metrics_table</code></p> <p>Please use our documentation to learn about all available metrics and customization options.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_colab/document_qna_eval/#document-question-answering-eval-recipe","title":"Document Question Answering Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare performance of two models on a document question answering prompt using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_script/","title":"Document Question Answering","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a document question answering prompt with Gemini 1.0 and Gemini 2.0 using  Vertex AI Evaluation Service.</p> <p></p> <ul> <li>Use case: answer questions based on information from the given document.</li> <li> <p>The Evaluation Dataset is based on SQuAD2.0. It includes 6 documents stored as plain text files, and a JSONL file that provides ground truth labels: <code>dataset.jsonl</code>.  Each record in this file includes 3 attributes:</p> <ul> <li><code>document_path</code>: relative path to the plain text document file</li> <li><code>question</code>: the question that we want to ask about this particular document</li> <li><code>reference</code>: expected correct answer or special code <code>ANSWER_NOT_FOUND</code> used to verify that the model does not hallucinate answers when the document does not provide enough information to answer the given question.</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with two prompt variables (<code>document</code> and <code>question</code>) that are automatically populated from our dataset.</p> </li> <li> <p>Python script <code>eval.py</code> configures the evaluation:</p> <ul> <li><code>run_eval</code>: configures the evaluation task, runs it on the 2 models and prints the results.</li> <li><code>load_dataset</code>: loads the dataset including the contents of all documents.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_script/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd document_qna/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output. </p> </li> <li>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics scores.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/document_qna/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>baseline_model</code> to the model that is currently used by your application</li> <li>set the parameter <code>candidate_model</code> to the model that you want to compare with your current model</li> <li>configure a unique <code>experiment_name</code> for tracking purposes</li> </ul> </li> <li>Replace the contents of <code>dataset.jsonl</code> with your custom data in the same format.</li> <li>Replace the contents of <code>prompt_template.txt</code> with your custom prompt template. Make sure that prompt template variables map to the dataset attributes.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/promptfoo/","title":"Image-Prompt Alignment","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to use a prompt alignment autorater to compare image generation quality of two models (Imagen2 and Imagen3) with the open source evaluation tool Promptfoo.</p> <p></p> <ul> <li> <p>Use case: Image Generation</p> </li> <li> <p>We use an unlabeled dataset with 5 image generation prompts stored in a JSONL file <code>dataset.jsonl</code> and JPEG images generated by Imagen2 and Imagen3 based on these prompts. Each record in the dataset includes 3 attributes wrapped in the <code>vars</code> object so that Promptfoo can inject this data into the autorater prompt.</p> <ul> <li><code>prompt</code>: full text of the image generation prompt</li> <li><code>imagen2</code>: local path to the JPG image generated by Imagen 2 based on this prompt</li> <li><code>imagen3</code>: local path to the JPG image generated by Imagen 3 based on this prompt</li> </ul> </li> <li> <p>All instructions for our prompt alignment autorater are stored in <code>autorater_instructions.txt</code>. These instructions are imported into the final multiodal prompt templates <code>prompt_imagen2.yaml</code> and <code>prompt_imagen3.yaml</code> that combine the images from our dataset with the autorater instructions.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all  configuration:</p> <ul> <li><code>providers</code>: defines the LLM judge model</li> <li><code>prompts</code>: autorater prompt templates for Imagen2 and Imagen3</li> <li><code>tests</code>: location of the dataset file</li> <li><code>defaultTest</code>: loads the autorater instructions into the shared variable <code>autorater_instructions</code>, and configures the custom prompt alignment metric defined in <code>metrics.py</code>. This metric parses the JSON response from our autorater and returns the percentage score along with the list of gaps detected by the autorater (each gap describes a prompt requirement that is not satisfied by the image).</li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li> <p>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</p> <pre><code>cd image_prompt_alignment/promptfoo\npromptfoo eval\n</code></pre> </li> <li> <p>Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the eval recipe folder (<code>promptfoo</code>) to your environment.</li> <li>Create a list of image generation prompts.</li> <li>Use your baseline and candidate models to generate images based on the image generation prompts, and save them to the <code>images</code> folder.</li> <li>Put your image generation prompts into the dataset file <code>dataset.jsonl</code> and make sure that each record points to the right images.</li> <li>That's it! You are ready to run <code>promptfoo eval</code> and view the results using <code>promptfoo view</code>.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_colab/image_prompt_alignment_eval/","title":"Image prompt alignment eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     https://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"as is\" basis,\n# without warranties or conditions of any kind, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at #     https://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"as is\" basis, # without warranties or conditions of any kind, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: Image Generation</p> </li> <li><p>Dataset: This eval recipe uses two JSONL dataset files that are based on the same set of prompts and map the prompts to the images generated by Imagen 2 and Imagen 3</p> </li> <li><p>Metric: we use an autorater inspired by Gecko that generates questions about all visually groundable aspects of the image, answers these questions, assigns the prompt alignment score based on the answers, and generates an explanation for all identified gaps.</p> </li> </ul> <p>Step 1 of 4: Configure eval settings</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id            # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=eval-image-prompt-alignment    # Creates Vertex AI Experiment to track the eval runs\nMODEL_JUDGE=gemini-2.0-flash-001  # This model will run the autorater prompt\nDATASET_URI_IMAGEN2=\"gs://gemini_assets/image_prompt_alignment/dataset_imagen2.jsonl\"  # Evaluation dataset for Imagen 2\nDATASET_URI_IMAGEN3=\"gs://gemini_assets/image_prompt_alignment/dataset_imagen3.jsonl\"  # Evaluation dataset for Imagen 3\n</pre> %%writefile .env PROJECT_ID=your-project-id            # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=eval-image-prompt-alignment    # Creates Vertex AI Experiment to track the eval runs MODEL_JUDGE=gemini-2.0-flash-001  # This model will run the autorater prompt DATASET_URI_IMAGEN2=\"gs://gemini_assets/image_prompt_alignment/dataset_imagen2.jsonl\"  # Evaluation dataset for Imagen 2 DATASET_URI_IMAGEN3=\"gs://gemini_assets/image_prompt_alignment/dataset_imagen3.jsonl\"  # Evaluation dataset for Imagen 3 <p>Step 2 of 4: Install Python libraries</p> In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) <p>Step 3 of 4: Authenticate to Google Cloud (requires permission to open a popup window)</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport pandas as pd\nimport vertexai\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom google import genai\nfrom google.cloud import storage\nfrom google.genai.types import Content, Part\nfrom vertexai.evaluation import EvalTask, CustomMetric\n\nload_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\n\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n_gemini_client = genai.Client(vertexai=True, project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> import os import sys import pandas as pd import vertexai from datetime import datetime from dotenv import load_dotenv from google import genai from google.cloud import storage from google.genai.types import Content, Part from vertexai.evaluation import EvalTask, CustomMetric  load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user()  vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) _gemini_client = genai.Client(vertexai=True, project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) <p>Step 4 of 4: Evaluate images from Baseline and Candidate models and print the alignment scores</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport os\nimport pandas as pd\nimport vertexai\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples\n\n_AUTORATER_PROMPT_TEMPLATE = '''\nYou are an expert image analyst with a keen eye for detail and a deep understanding of linguistics and human perception.\n\n# Definitions\n- **Visually Groundable Requirement:** A specific claim or requirement within the image description that can be verified or refuted by examining the visual content of the image. This includes descriptions of objects (existence and attributes like color, size, shape, or text on the object), spatial relationships between objects, actions depicted, or overall scene characteristics like lighting conditions.\n- **Gap:** A visually groundable requirement that is either contradicted by the image or cannot be directly confirmed based on the image.\n\n# Instructions\nReview the image and a description of that image located in the IMAGE_DESCRIPTION tag below.\nYour goal is to rate the accuracy of the image description on the scale of 0 to 10.\nYou must use the following 6-step process and provide brief written notes for each step:\n- Step 1. Identify all Visually Groundable Requirements contained in IMAGE_DESCRIPTION and save them to a numbered list.\n- Step 2. Write a numbered list of true/false questions that should be asked about each of the identified requirements in order to verify whether each requirement is satisfied by the image or not.\n- Step 3. For each of the questions created in Step 2 write a brief analysis of the most relevant information in the provided image and then write the final answer:\n    - True only if the image contains a clear positive answer to this question.\n    - False if the image clearly justifies a negative answer to this question OR does not have enough information to answer this question.\n- Step 4. Calculate the number of questions that received the answer \"True\" in step 3.\n- Step 5. Calculate the final accuracy score as the percentage of positively answered questions out of the total questions answered in Step 3, rounded to the nearest integer.\n- Step 6. Write the final answer as a Markdown codeblock containing a single JSON object with two attributes:\n    - \"score\" with the integer value of the final accuracy score calculated in Step 5.\n    - \"gaps\" with a JSON array of strings that describe each gap (question that got a negative answer in Step 3). The description should be a one sentence statement that combines key information from the question and the analysis of relevant information from Step 3.\n\n&lt;IMAGE_DESCRIPTION&gt;\n{image_description}\n&lt;/IMAGE_DESCRIPTION&gt;\n'''\n\ndef load_text_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_image(gcs_uri: str) -&gt; bytes:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_bytes()\n\ndef load_dataset(dataset_uri: str):\n    '''Convert the dataset to a Pandas DataFrame and load all images into the \"image\" column.'''\n    lines = load_text_file(dataset_uri).splitlines()\n    data = [json.loads(line) for line in lines if line.strip()]\n    df = pd.DataFrame(data)\n    df['image'] = df['image_uri'].apply(lambda image_uri: load_image(image_uri))\n    return df[['image_uri', 'prompt', 'image']]\n\ndef image_prompt_alignment_autorater(record: dict) -&gt; dict:\n    '''Custom metric function for scoring prompt alignment between the image and prompt from the given dataset record.'''\n    response = _gemini_client.models.generate_content(\n        model=os.getenv('MODEL_JUDGE'),\n        contents=[\n            Content(role='user', parts=[Part(text=_AUTORATER_PROMPT_TEMPLATE.format(image_description=record['prompt']))]),\n            Content(role='user', parts=[Part.from_bytes(data=record['image'], mime_type='image/jpeg')])\n        ]\n    )\n    json_output = json.loads(response.text.split('```json\\n')[1].split('\\n```')[0])\n    return {\n        \"image_prompt_alignment\": json_output['score'],\n        \"explanation\": '\\n'.join(json_output['gaps'])\n    }\n\ndef print_scores_and_explanations(title: str, eval_result: EvalResult) -&gt; None:\n    print(f'\\n{\"-\"*80}\\nRESULTS FOR {title}:')\n    for i, row in eval_result.metrics_table.iterrows():\n        gaps = row[\"image_prompt_alignment/explanation\"]\n        gaps = f', GAPS: {gaps}' if gaps else ''\n        print(f'{row[\"image_uri\"]}: SCORE={row[\"image_prompt_alignment/score\"]}%{gaps}')\n\ndef run_eval(model: str, dataset_uri: str, experiment_name:str):\n    '''Rate the alignment between image generation prompts and the generated images and identify gaps using a custom autorater.'''\n    timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n    dataset=load_dataset(dataset_uri)\n    task = EvalTask(\n        dataset=dataset,\n        metrics=[CustomMetric(name=\"image_prompt_alignment\", metric_function=image_prompt_alignment_autorater)],\n        experiment=experiment_name\n    )\n    return task.evaluate(experiment_run_name=f\"{timestamp}-{model.lower().replace('.', '-')}\")\n\ndef compare_models(project_id: str, location: str, experiment_name: str, model_a: str, dataset_uri_a: str, model_b: str, dataset_uri_b: str) -&gt; None:\n    global _gemini_client\n    _gemini_client = genai.Client(vertexai=True, project=project_id, location=location)\n    vertexai.init(project=project_id, location=location)\n    results_a = run_eval(model_a, dataset_uri_a, experiment_name)\n    results_b = run_eval(model_b, dataset_uri_b, experiment_name)\n    clear_output()\n    print_scores_and_explanations(model_a, results_a)\n    print_scores_and_explanations(model_b, results_b)\n    print(f\"\\n{model_a} average alignment score = {results_a.summary_metrics['image_prompt_alignment/mean']:.1f}%\")\n    print(f\"{model_b} average alignment score = {results_b.summary_metrics['image_prompt_alignment/mean']:.1f}%\")\n\ncompare_models(\n    project_id=os.getenv('PROJECT_ID'),\n    location=os.getenv('LOCATION'),\n    experiment_name=os.getenv('EXPERIMENT_NAME'),\n    model_a=\"IMAGEN2\",\n    dataset_uri_a=os.getenv('DATASET_URI_IMAGEN2'),\n    model_b=\"IMAGEN3\",\n    dataset_uri_b=os.getenv('DATASET_URI_IMAGEN3')\n)\n</pre> import json import os import pandas as pd import vertexai from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples  _AUTORATER_PROMPT_TEMPLATE = ''' You are an expert image analyst with a keen eye for detail and a deep understanding of linguistics and human perception.  # Definitions - **Visually Groundable Requirement:** A specific claim or requirement within the image description that can be verified or refuted by examining the visual content of the image. This includes descriptions of objects (existence and attributes like color, size, shape, or text on the object), spatial relationships between objects, actions depicted, or overall scene characteristics like lighting conditions. - **Gap:** A visually groundable requirement that is either contradicted by the image or cannot be directly confirmed based on the image.  # Instructions Review the image and a description of that image located in the IMAGE_DESCRIPTION tag below. Your goal is to rate the accuracy of the image description on the scale of 0 to 10. You must use the following 6-step process and provide brief written notes for each step: - Step 1. Identify all Visually Groundable Requirements contained in IMAGE_DESCRIPTION and save them to a numbered list. - Step 2. Write a numbered list of true/false questions that should be asked about each of the identified requirements in order to verify whether each requirement is satisfied by the image or not. - Step 3. For each of the questions created in Step 2 write a brief analysis of the most relevant information in the provided image and then write the final answer:     - True only if the image contains a clear positive answer to this question.     - False if the image clearly justifies a negative answer to this question OR does not have enough information to answer this question. - Step 4. Calculate the number of questions that received the answer \"True\" in step 3. - Step 5. Calculate the final accuracy score as the percentage of positively answered questions out of the total questions answered in Step 3, rounded to the nearest integer. - Step 6. Write the final answer as a Markdown codeblock containing a single JSON object with two attributes:     - \"score\" with the integer value of the final accuracy score calculated in Step 5.     - \"gaps\" with a JSON array of strings that describe each gap (question that got a negative answer in Step 3). The description should be a one sentence statement that combines key information from the question and the analysis of relevant information from Step 3.   {image_description}  '''  def load_text_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_image(gcs_uri: str) -&gt; bytes:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_bytes()  def load_dataset(dataset_uri: str):     '''Convert the dataset to a Pandas DataFrame and load all images into the \"image\" column.'''     lines = load_text_file(dataset_uri).splitlines()     data = [json.loads(line) for line in lines if line.strip()]     df = pd.DataFrame(data)     df['image'] = df['image_uri'].apply(lambda image_uri: load_image(image_uri))     return df[['image_uri', 'prompt', 'image']]  def image_prompt_alignment_autorater(record: dict) -&gt; dict:     '''Custom metric function for scoring prompt alignment between the image and prompt from the given dataset record.'''     response = _gemini_client.models.generate_content(         model=os.getenv('MODEL_JUDGE'),         contents=[             Content(role='user', parts=[Part(text=_AUTORATER_PROMPT_TEMPLATE.format(image_description=record['prompt']))]),             Content(role='user', parts=[Part.from_bytes(data=record['image'], mime_type='image/jpeg')])         ]     )     json_output = json.loads(response.text.split('```json\\n')[1].split('\\n```')[0])     return {         \"image_prompt_alignment\": json_output['score'],         \"explanation\": '\\n'.join(json_output['gaps'])     }  def print_scores_and_explanations(title: str, eval_result: EvalResult) -&gt; None:     print(f'\\n{\"-\"*80}\\nRESULTS FOR {title}:')     for i, row in eval_result.metrics_table.iterrows():         gaps = row[\"image_prompt_alignment/explanation\"]         gaps = f', GAPS: {gaps}' if gaps else ''         print(f'{row[\"image_uri\"]}: SCORE={row[\"image_prompt_alignment/score\"]}%{gaps}')  def run_eval(model: str, dataset_uri: str, experiment_name:str):     '''Rate the alignment between image generation prompts and the generated images and identify gaps using a custom autorater.'''     timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()     dataset=load_dataset(dataset_uri)     task = EvalTask(         dataset=dataset,         metrics=[CustomMetric(name=\"image_prompt_alignment\", metric_function=image_prompt_alignment_autorater)],         experiment=experiment_name     )     return task.evaluate(experiment_run_name=f\"{timestamp}-{model.lower().replace('.', '-')}\")  def compare_models(project_id: str, location: str, experiment_name: str, model_a: str, dataset_uri_a: str, model_b: str, dataset_uri_b: str) -&gt; None:     global _gemini_client     _gemini_client = genai.Client(vertexai=True, project=project_id, location=location)     vertexai.init(project=project_id, location=location)     results_a = run_eval(model_a, dataset_uri_a, experiment_name)     results_b = run_eval(model_b, dataset_uri_b, experiment_name)     clear_output()     print_scores_and_explanations(model_a, results_a)     print_scores_and_explanations(model_b, results_b)     print(f\"\\n{model_a} average alignment score = {results_a.summary_metrics['image_prompt_alignment/mean']:.1f}%\")     print(f\"{model_b} average alignment score = {results_b.summary_metrics['image_prompt_alignment/mean']:.1f}%\")  compare_models(     project_id=os.getenv('PROJECT_ID'),     location=os.getenv('LOCATION'),     experiment_name=os.getenv('EXPERIMENT_NAME'),     model_a=\"IMAGEN2\",     dataset_uri_a=os.getenv('DATASET_URI_IMAGEN2'),     model_b=\"IMAGEN3\",     dataset_uri_b=os.getenv('DATASET_URI_IMAGEN3') ) <p>Learn more about Vertex AI GenAI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_colab/image_prompt_alignment_eval/#image-prompt-alignment","title":"Image-Prompt Alignment\u00b6","text":"<p>This Eval Recipe demonstrates how to use a prompt alignment autorater to compare image generation quality of two models (Imagen2 and Imagen3) using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_script/","title":"Image-Prompt Alignment","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to use a prompt alignment autorater to compare image generation quality of two models (Imagen2 and Imagen3) using Vertex AI Evaluation Service.</p> <p></p> <ul> <li> <p>Use case: Image Generation</p> </li> <li> <p>We use a separate unlabeled dataset in JSONL format for each model: <code>dataset_imagen2.jsonl</code> and <code>dataset_imagen3.jsonl</code>. Each record in these datasets includes 2 attributes:</p> <ul> <li><code>prompt</code>: full text of the image generation prompt</li> <li><code>image_path</code>: local path to the JPG image generated based on this prompt</li> </ul> </li> <li> <p>The autorater instructions are stored in <code>autorater_instructions.txt</code>. These instructions are imported into the multiodal prompt templates <code>prompt_imagen2.yaml</code> and <code>prompt_imagen3.yaml</code> that combine the images from our dataset with the autorater instructions.</p> </li> <li> <p>The evaluation is configured in <code>eval.py</code>:</p> <ul> <li><code>load_dataset</code>: loads the dataset including the images referenced from the JSONL file.</li> <li><code>image_prompt_alignment_autorater</code>: assembles the autorater prompt and runs inference in order to identify gaps between the prompt and the generated image and calculate the prompt alignment score.</li> <li><code>run_eval</code>: executes evalulation based on one dataset file.</li> <li><code>compare_models</code>: orchestrates evaluations for both models and prints the results.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_script/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd image_prompt_alignment/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output.</p> </li> <li>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final scores.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/image_prompt_alignment/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>configure a unique <code>experiment_name</code> for tracking purposes.</li> <li>(optional) you can customize the autorater prompt located in the global variable <code>_AUTORATER_PROMPT_TEMPLATE</code>.</li> </ul> </li> <li>Replace the contenx of the two JSONL files with your image generation prompts.</li> <li>Replace the generated images located in the <code>images</code> folder with images generated based on your prompts.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/promptfoo/","title":"Instruction Following","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of an Instruction Following prompt with Gemini 1.5 Flash and Gemini 2.0 Flash using an unlabeled dataset and open source evaluation tool Promptfoo.</p> <p>Instruction-Following Eval (IFEval) is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of \"verifiable instructions\" such as \"write in more than 400 words\", \"write in bullet points\", etc. </p> <ul> <li> <p>Use case: Instruction Following </p> </li> <li> <p>Evaluation Dataset is based on Instruction Following Evaluation Dataset. It includes 10 randomly sampled prompts in a JSONL file <code>dataset.jsonl</code>. Each record in this file includes 1 attribute wrapped in the <code>vars</code> object. This structure allows Promptfoo to specify the variables needed to populate prompt templates (document and question), as well as the ground truth label required to score the accuracy of model responses:</p> <ul> <li><code>prompt</code>: The task with specific instructions provided</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with one prompt variable (<code>prompt</code>) that is automatically populated from our dataset.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all Promptfoo configuration:</p> <ul> <li><code>providers</code>: list of models that will be evaluated</li> <li><code>prompts</code>: location of the prompt template file</li> <li><code>tests</code>: location of the labeled dataset file</li> <li><code>defaultTest</code>: defines the scoring logic:     <code>type: answer-relevance</code> The answer-relevance assertion evaluates whether an LLM's output is relevant to the original query. It uses a combination of embedding similarity and LLM evaluation to determine relevance..     <code>value: \"Check if the response adheres to the instructions in the prompt\"</code> instructs Promptfoo to verify and score based on how well the generated response is aligned with the original prompt.     <code>threshold: 0.5</code> Mark any responses with a score below 0.5 as a failure.</li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</li> </ul> <p><pre><code>cd genai-on-vertex-ai/gemini/model_upgrades/instruction_following/promptfoo\npromptfoo eval\n</code></pre> 1. Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the configuration file <code>promptfooconfig.yaml</code> to a new folder.</li> <li>Add your labeled dataset file with JSONL schema similar to <code>dataset.jsonl</code>. </li> <li>Save your prompt template to <code>prompt_template.txt</code> and make sure that the template variables map to the variables defined in your dataset.</li> <li>That's it! You are ready to run <code>promptfoo eval</code>. If needed, add alternative prompt templates or additional metrics to promptfooconfig.yaml as explained here.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/","title":"Instruction following eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software  # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: Instruction Following</p> </li> <li><p>Metric: This eval uses a Pairwise Instruction Following template to evaluate the responses and pick a model as the winner.</p> </li> <li><p>Evaluation Dataset is based on Instruction Following Evaluation Dataset. It includes 10 randomly sampled prompts in a JSONL file <code>dataset.jsonl</code> with the following structure:</p> <ul> <li><code>prompt</code>: The task with specific instructions provided</li> </ul> </li> <li><p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with one prompt variable (<code>prompt</code>) that is automatically populated from our dataset.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id          # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=instructionfollowing-eval-recipe-demo      # Creates Vertex AI Experiment to track the eval runs\nMODEL_BASELINE=gemini-1.5-flash  # Name of your current model\nMODEL_CANDIDATE=gemini-2.0-flash # This model will be compared to the baseline model\nDATASET_URI=\"gs://gemini_assets/instruction_following/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=\"gs://gemini_assets/instruction_following/prompt_template.txt\"  # Text file in Google Cloud Storage\nMETRIC_NAME = \"pairwise_instruction_following\"\n</pre> %%writefile .env PROJECT_ID=your-project-id          # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=instructionfollowing-eval-recipe-demo      # Creates Vertex AI Experiment to track the eval runs MODEL_BASELINE=gemini-1.5-flash  # Name of your current model MODEL_CANDIDATE=gemini-2.0-flash # This model will be compared to the baseline model DATASET_URI=\"gs://gemini_assets/instruction_following/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=\"gs://gemini_assets/instruction_following/prompt_template.txt\"  # Text file in Google Cloud Storage METRIC_NAME = \"pairwise_instruction_following\" In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport pandas as pd\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, PairwiseMetric,  MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel\n</pre> import os import json import pandas as pd import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, PairwiseMetric,  MetricPromptTemplateExamples from vertexai.generative_models import GenerativeModel In\u00a0[\u00a0]: Copied! <pre>load_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:  \n    from google.colab import auth  \n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:       from google.colab import auth       auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) In\u00a0[\u00a0]: Copied! <pre>def load_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_dataset(dataset_uri: str):\n    jsonl = load_file(dataset_uri)\n    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n    df = pd.DataFrame(samples)\n    return df\n\ndef load_prompt_template() -&gt; str:\n    blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef run_eval(model: str) -&gt; EvalResult:\n  timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n  return EvalTask(\n      dataset=os.getenv(\"DATASET_URI\"),\n      metrics= [\n        PairwiseMetric(\n            metric=os.getenv('METRIC_NAME'),\n            metric_prompt_template=MetricPromptTemplateExamples.Pairwise.INSTRUCTION_FOLLOWING.metric_prompt_template,\n            # Baseline model for pairwise comparison\n            baseline_model=GenerativeModel(os.getenv(\"MODEL_BASELINE\")),\n        ),\n    ],\n      experiment=os.getenv('EXPERIMENT_NAME')\n  ).evaluate(\n      model=GenerativeModel(os.getenv(\"MODEL_CANDIDATE\")),\n      prompt_template=load_prompt_template(),\n      experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"\n  )\n\n#baseline = run_eval(os.getenv(\"MODEL_BASELINE\"))\nmetrics = run_eval(os.getenv(\"MODEL_CANDIDATE\"))\nclear_output()\nprint(\"Baseline model win rate:\", round(metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/baseline_model_win_rate'],3))\nprint(\"Candidate model win rate:\", round(metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/candidate_model_win_rate'],3))\n</pre> def load_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_dataset(dataset_uri: str):     jsonl = load_file(dataset_uri)     samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]     df = pd.DataFrame(samples)     return df  def load_prompt_template() -&gt; str:     blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())     return blob.download_as_string().decode('utf-8')  def run_eval(model: str) -&gt; EvalResult:   timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()   return EvalTask(       dataset=os.getenv(\"DATASET_URI\"),       metrics= [         PairwiseMetric(             metric=os.getenv('METRIC_NAME'),             metric_prompt_template=MetricPromptTemplateExamples.Pairwise.INSTRUCTION_FOLLOWING.metric_prompt_template,             # Baseline model for pairwise comparison             baseline_model=GenerativeModel(os.getenv(\"MODEL_BASELINE\")),         ),     ],       experiment=os.getenv('EXPERIMENT_NAME')   ).evaluate(       model=GenerativeModel(os.getenv(\"MODEL_CANDIDATE\")),       prompt_template=load_prompt_template(),       experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"   )  #baseline = run_eval(os.getenv(\"MODEL_BASELINE\")) metrics = run_eval(os.getenv(\"MODEL_CANDIDATE\")) clear_output() print(\"Baseline model win rate:\", round(metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/baseline_model_win_rate'],3)) print(\"Candidate model win rate:\", round(metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/candidate_model_win_rate'],3))"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/#instruction-following-eval-recipe","title":"Instruction Following Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare performance of two models on a Instruction Following dataset using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/#configure-eval-settings","title":"Configure Eval Settings\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/#install-python-libraries","title":"Install Python Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/#authenticate-to-google-cloud-requires-permission-to-open-a-popup-window","title":"Authenticate to Google Cloud (requires permission to open a popup window)\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_colab/instruction_following_eval/#run-the-eval-on-both-models-on-the-pairwise-autorater","title":"Run the eval on both models on the Pairwise Autorater\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_script/","title":"Instruction Following","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of an Instruction Following prompt with Gemini 1.5 Flash and Gemini 2.0 Flash using  Vertex AI Evaluation Service.</p> <p>Instruction-Following Eval (IFEval) is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of \"verifiable instructions\" such as \"write in more than 400 words\", \"write in bullet points\", etc. </p> <ul> <li> <p>Use case: Instruction Following </p> </li> <li> <p>Evaluation Dataset is based on Instruction Following Evaluation Dataset. It includes 10 randomly sampled prompts in a JSONL file <code>dataset.jsonl</code> with the follwing structure:</p> <ul> <li><code>prompt</code>: The task with specific instructions provided</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with one prompt variable (<code>prompt</code>) that is automatically populated from our dataset.</p> </li> <li> <p>Python script <code>eval.py</code> configures the evaluation:</p> <ul> <li><code>run_eval</code>: configures the evaluation task, runs it on the 2 models and prints the results.</li> <li><code>load_dataset</code>: loads the dataset including the contents of all documents.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd instruction_following/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output. </p> </li> <li> <p>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics scores.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/instruction_following/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>baseline_model</code> to the model that is currently used by your application</li> <li>set the parameter <code>candidate_model</code> to the model that you want to compare with your current model</li> <li>configure a unique <code>experiment_name</code> for tracking purposes</li> </ul> </li> <li>Replace the contents of <code>dataset.jsonl</code> with your custom data in the same format.</li> <li>Replace the contents of <code>prompt_template.txt</code> with your custom prompt template. Make sure that prompt template variables map to the dataset attributes.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/promptfoo/","title":"Multi-Turn Conversation (Chat)","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to evaluate a multi-turn conversation (chat) on Gemini 1.0 and Gemini 2.0 using the open source evaluation tool Promptfoo.</p> <p></p> <ul> <li> <p>Use case: multi-turn conversation</p> </li> <li> <p>Evaluation Dataset is based on Multi-turn Prompts Dataset. It includes 5 conversations: <code>dataset.jsonl</code>. Each record in this file links to a JSON file with the system instruction followed by a few messages from the User and responses from the Assistant. This dataset does not include any ground truth labels.</p> </li> <li> <p>Prompt Template located in <code>prompt_template.txt</code> injects the <code>chat</code> variable from our dataset which represents the conversation history.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all Promptfoo configuration:</p> <ul> <li><code>providers</code>: list of models that will be evaluated</li> <li><code>prompts</code>: location of the prompt template file</li> <li><code>tests</code>: location of the dataset file</li> <li><code>defaultTest</code>: configures the evaluation metric:<ol> <li><code>type: select-best</code> auto-rater that decides which of the two models configured above generated the best response.</li> <li><code>providers</code> configures the judge model</li> <li><code>value</code> configures the custom criteria that is evaluated by the <code>select-best</code> auto-rater</li> </ol> </li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li> <p>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</p> <pre><code>cd multiturn_chat/promptfoo\npromptfoo eval\n</code></pre> </li> <li> <p>Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the configuration file <code>promptfooconfig.yaml</code> to a new folder.</li> <li>Add your labeled dataset file with JSONL schema similar to <code>dataset.jsonl</code>. </li> <li>Save your prompt template to <code>prompt_template.txt</code> and make sure that the template variables map to the variables defined in your dataset.</li> <li>That's it! You are ready to run <code>promptfoo eval</code>. If needed, add alternative prompt templates or additional metrics to promptfooconfig.yaml as explained here.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_colab/multiturn_chat_eval/","title":"Multiturn chat eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     https://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"as is\" basis,\n# without warranties or conditions of any kind, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at #     https://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"as is\" basis, # without warranties or conditions of any kind, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: multi-turn conversation (Chat)</p> </li> <li><p>Metric: this eval uses a Pairwise Autorater (LLM Judge) to compare the quality of model responses.</p> </li> <li><p>Evaluation Dataset is a subset of Multi-turn Prompts Dataset. Each record in the dataset.jsonl file links to a JSON file with the history of conversation between the User and the Model. This dataset does not include any ground truth labels.</p> </li> </ul> <p>Step 1 of 4: Configure all necessary parameters</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id            # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=eval-multiturn-chat   # Creates Vertex AI Experiment to track the eval runs\nMODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model\nMODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model\nDATASET_URI=\"gs://gemini_assets/multiturn_chat/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=gs://gemini_assets/multiturn_chat/prompt_template.txt  # Text file in Google Cloud Storage\n</pre> %%writefile .env PROJECT_ID=your-project-id            # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=eval-multiturn-chat   # Creates Vertex AI Experiment to track the eval runs MODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model MODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model DATASET_URI=\"gs://gemini_assets/multiturn_chat/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=gs://gemini_assets/multiturn_chat/prompt_template.txt  # Text file in Google Cloud Storage <p>Step 2 of 4: Install all required Python libraries if not already installed.</p> In\u00a0[\u00a0]: Copied! <pre>try: # Skip installation and kernel restart if this cell has been executed.\n  import dotenv\nexcept:\n  %pip install --upgrade --user --quiet python-dotenv google-genai google-cloud-aiplatform[evaluation]\n  import IPython\n  # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\n  IPython.Application.instance().kernel.do_shutdown(True)\n</pre> try: # Skip installation and kernel restart if this cell has been executed.   import dotenv except:   %pip install --upgrade --user --quiet python-dotenv google-genai google-cloud-aiplatform[evaluation]   import IPython   # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.   IPython.Application.instance().kernel.do_shutdown(True) <p>Step 3 of 4: Authenticate to Google Cloud (requires permission to open a popup window)</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nload_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> import os import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) <p>Step 4 of 4: Run the eval on both models and compare the Accuracy scores</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport pandas as pd\nfrom datetime import datetime\nfrom google import genai\nfrom google.genai.types import Content, Part\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory\n\ndef load_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_dataset(dataset_uri: str):\n    jsonl = load_file(dataset_uri)\n    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n    df = pd.DataFrame(samples)\n    df['history'] = df['chat_uri'].apply(lambda document_uri: load_file(document_uri))\n    return df[['history']]\n\ndef generate_chat_responses(project_id: str, location:str, model: str, dataset: pd.DataFrame, response_column_name: str) -&gt; None:\n    '''Generate the final model response for each conversation in the dataset using the specified model.'''\n    client = genai.Client(vertexai=True, project=project_id, location=location)\n    responses = []\n    user_prompts = []\n    for i, record in dataset.iterrows():\n        print(f'Generating chat completion #{i+1} with {model}')\n        messages = json.loads(record.get('history'))\n        last_user_message = messages.pop()\n        history = [\n            Content(\n                role=message['role'],\n                parts=[Part(text=message['content'])],\n            )\n            for message in messages\n        ]\n        chat = client.chats.create(model=model, history=history)\n        response = chat.send_message(message=[Part(text=last_user_message['content'])])\n        user_prompts.append(last_user_message)\n        responses.append( response.candidates[0].content.parts[0].text )\n    dataset['prompt'] = user_prompts  # The last user message is required by the Autorater\n    dataset[response_column_name] = responses\n    print(f'{len(responses)} responses from model {model} are stored in dataset column \"{response_column_name}\"')\n\ndef run_eval(project_id: str, location:str, experiment_name: str, baseline_model: str, candidate_model: str, dataset_uri: str):\n    vertexai.init(project=project_id, location=location)\n    timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n    dataset=load_dataset(dataset_uri)\n    generate_chat_responses(project_id, location, baseline_model, dataset, 'baseline_model_response')\n    generate_chat_responses(project_id, location, candidate_model, dataset, 'response')\n    task = EvalTask(\n        dataset=dataset,\n        metrics=[MetricPromptTemplateExamples.Pairwise.MULTI_TURN_CHAT_QUALITY],\n        experiment=experiment_name\n    )\n    eval_results = task.evaluate(\n        experiment_run_name=f\"{timestamp}-{baseline_model.replace('.', '-')}\"\n    )\n    clear_output()\n    print(f\"Baseline model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/baseline_model_win_rate']:.2f}\")\n    print(f\"Candidate model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/candidate_model_win_rate']:.2f}\")\n\nrun_eval(\n    project_id=os.getenv('PROJECT_ID'),\n    location=os.getenv('LOCATION'),\n    experiment_name=os.getenv('EXPERIMENT_NAME'),\n    baseline_model=os.getenv('MODEL_BASELINE'),\n    candidate_model=os.getenv('MODEL_CANDIDATE'),\n    dataset_uri=os.getenv('DATASET_URI')\n)\n</pre> import json import pandas as pd from datetime import datetime from google import genai from google.genai.types import Content, Part from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory  def load_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_dataset(dataset_uri: str):     jsonl = load_file(dataset_uri)     samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]     df = pd.DataFrame(samples)     df['history'] = df['chat_uri'].apply(lambda document_uri: load_file(document_uri))     return df[['history']]  def generate_chat_responses(project_id: str, location:str, model: str, dataset: pd.DataFrame, response_column_name: str) -&gt; None:     '''Generate the final model response for each conversation in the dataset using the specified model.'''     client = genai.Client(vertexai=True, project=project_id, location=location)     responses = []     user_prompts = []     for i, record in dataset.iterrows():         print(f'Generating chat completion #{i+1} with {model}')         messages = json.loads(record.get('history'))         last_user_message = messages.pop()         history = [             Content(                 role=message['role'],                 parts=[Part(text=message['content'])],             )             for message in messages         ]         chat = client.chats.create(model=model, history=history)         response = chat.send_message(message=[Part(text=last_user_message['content'])])         user_prompts.append(last_user_message)         responses.append( response.candidates[0].content.parts[0].text )     dataset['prompt'] = user_prompts  # The last user message is required by the Autorater     dataset[response_column_name] = responses     print(f'{len(responses)} responses from model {model} are stored in dataset column \"{response_column_name}\"')  def run_eval(project_id: str, location:str, experiment_name: str, baseline_model: str, candidate_model: str, dataset_uri: str):     vertexai.init(project=project_id, location=location)     timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()     dataset=load_dataset(dataset_uri)     generate_chat_responses(project_id, location, baseline_model, dataset, 'baseline_model_response')     generate_chat_responses(project_id, location, candidate_model, dataset, 'response')     task = EvalTask(         dataset=dataset,         metrics=[MetricPromptTemplateExamples.Pairwise.MULTI_TURN_CHAT_QUALITY],         experiment=experiment_name     )     eval_results = task.evaluate(         experiment_run_name=f\"{timestamp}-{baseline_model.replace('.', '-')}\"     )     clear_output()     print(f\"Baseline model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/baseline_model_win_rate']:.2f}\")     print(f\"Candidate model win rate: {eval_results.summary_metrics['pairwise_multi_turn_chat_quality/candidate_model_win_rate']:.2f}\")  run_eval(     project_id=os.getenv('PROJECT_ID'),     location=os.getenv('LOCATION'),     experiment_name=os.getenv('EXPERIMENT_NAME'),     baseline_model=os.getenv('MODEL_BASELINE'),     candidate_model=os.getenv('MODEL_CANDIDATE'),     dataset_uri=os.getenv('DATASET_URI') ) <p>You can access all prompts and model responses in <code>candidate_results.metrics_table</code></p> <p>Please use our documentation to learn about all available metrics and customization options.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_colab/multiturn_chat_eval/#multi-turn-chat-eval-recipe","title":"Multi-turn Chat Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare quality of chat responses from two versions of Gemini using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_script/","title":"Multi-turn Chat","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a Multi-turn conversation (Chat) with Gemini 1.0 and Gemini 2.0 using  Vertex AI Evaluation Service.</p> <p></p> <ul> <li> <p>Use case: multi-turn conversation (Chat)</p> </li> <li> <p>Evaluation Dataset is based on Multi-turn Prompts Dataset. It includes 5 conversations: <code>dataset.jsonl</code>. Each record in this file links to a JSON file with the conversation history between the User and the Model. This dataset does not include any ground truth labels.</p> </li> <li> <p>Python script <code>eval.py</code> executes the evaluation:</p> <ul> <li><code>load_dataset</code>: loads all conversation histories to a Pandas Dataframe.</li> <li><code>generate_chat_responses</code>: runs an inference for each conversation in the dataset and saves the model responses.</li> <li><code>run_eval</code>: passes the dataset including responses from the Baseline and Candidate models to a pairwise autorater in order to calculate the win rate.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_script/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd multiturn_chat/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output. </p> </li> <li>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/multiturn_chat/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>baseline_model</code> to the model that is currently used by your application</li> <li>set the parameter <code>candidate_model</code> to the model that you want to compare with your current model</li> <li>configure a unique <code>experiment_name</code> for tracking purposes</li> </ul> </li> <li>Replace the dataset with your custom data in the same format.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/","title":"Rag embeddings eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software  # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: RAG retrieval</p> </li> <li><p>Metric: This eval uses a Pointwise Retrieval quality template to evaluate the responses and pick an embedding model as the winner. We will define <code>retrieval quality</code> as the metric here. It checks whether the <code>retrieved_context</code> contains all the key information present in <code>reference</code>.</p> </li> <li><p>Evaluation Datasets are based on RAG Dataset in compliance with the following license. They include 8 randomly sampled prompts in JSONL files <code>baseline_dataset.jsonl</code> and <code>candidate_dataset.jsonl</code> with the following structure:</p> <ul> <li><code>question</code>: User inputted question</li> <li><code>reference</code>: The golden truth answer for the question</li> <li><code>retrieved_context</code>: The context retrieved from the model</li> </ul> </li> <li><p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with two prompt variables ( <code>reference</code> and <code>retrieved_context</code>) that are automatically populated from our dataset.</p> </li> <li><p>This eval recipe uses an LLM judge model(gemini-2.0-flash) to evaluate the retrieval quality of the embedding models.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id        # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=rag-embeddings-eval-recipe-demo      # Creates Vertex AI Experiment to track the eval runs\nBASELINE_EMBEDDING_MODEL=text-embedding-004\nCANDIDATE_EMBEDDING_MODEL=text-embedding-005\nMODEL=gemini-2.0-flash # This model will be the judge for performing evaluations\nBASELINE_DATASET_URI=\"gs://gemini_assets/rag_embeddings/baseline_dataset.jsonl\"  # Baseline embedding model dataset in Google Cloud Storage\nCANDIDATE_DATASET_URI=\"gs://gemini_assets/rag_embeddings/candidate_dataset.jsonl\"  # Candidate embedding model dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=\"gs://gemini_assets/rag_embeddings/prompt_template.txt\"  # Text file in Google Cloud Storage\nMETRIC_NAME=\"retrieval_quality\"\n</pre> %%writefile .env PROJECT_ID=your-project-id        # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=rag-embeddings-eval-recipe-demo      # Creates Vertex AI Experiment to track the eval runs BASELINE_EMBEDDING_MODEL=text-embedding-004 CANDIDATE_EMBEDDING_MODEL=text-embedding-005 MODEL=gemini-2.0-flash # This model will be the judge for performing evaluations BASELINE_DATASET_URI=\"gs://gemini_assets/rag_embeddings/baseline_dataset.jsonl\"  # Baseline embedding model dataset in Google Cloud Storage CANDIDATE_DATASET_URI=\"gs://gemini_assets/rag_embeddings/candidate_dataset.jsonl\"  # Candidate embedding model dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=\"gs://gemini_assets/rag_embeddings/prompt_template.txt\"  # Text file in Google Cloud Storage METRIC_NAME=\"retrieval_quality\" In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport pandas as pd\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, PointwiseMetric\n</pre> import os import json import pandas as pd import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, PointwiseMetric In\u00a0[\u00a0]: Copied! <pre>load_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:  \n    from google.colab import auth  \n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:       from google.colab import auth       auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) In\u00a0[\u00a0]: Copied! <pre>def load_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_dataset(dataset_uri: str):\n    jsonl = load_file(dataset_uri)\n    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n    df = pd.DataFrame(samples)\n    return df\n\ndef load_prompt_template() -&gt; str:\n    blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef run_eval(model: str, embedding_model: str, dataset_uri: str) -&gt; EvalResult:\n  timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n  return EvalTask(\n      dataset=dataset_uri,\n      metrics=[PointwiseMetric(\n               metric=os.getenv('METRIC_NAME'),\n               metric_prompt_template= load_prompt_template()\n               )   \n               ],\n      experiment=os.getenv('EXPERIMENT_NAME')\n  ).evaluate(\n      response_column_name= 'retrieved_context',\n      experiment_run_name=f\"{timestamp}-{embedding_model}-{model.replace('.', '-')}\"\n  )\n\n\nbaseline_metrics = run_eval(os.getenv(\"MODEL\"), os.getenv(\"BASELINE_EMBEDDING_MODEL\"), os.getenv(\"BASELINE_DATASET_URI\"))\ncandidate_metrics = run_eval(os.getenv(\"MODEL\"), os.getenv(\"CANDIDATE_EMBEDDING_MODEL\"), os.getenv(\"CANDIDATE_DATASET_URI\"))\nclear_output()\nprint(\"Average score for baseline model retrieval quality:\", round(baseline_metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/mean'],3))\nprint(\"Average score for candidate model retrieval quality:\", round(candidate_metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/mean'],3))\n</pre> def load_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_dataset(dataset_uri: str):     jsonl = load_file(dataset_uri)     samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]     df = pd.DataFrame(samples)     return df  def load_prompt_template() -&gt; str:     blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())     return blob.download_as_string().decode('utf-8')  def run_eval(model: str, embedding_model: str, dataset_uri: str) -&gt; EvalResult:   timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()   return EvalTask(       dataset=dataset_uri,       metrics=[PointwiseMetric(                metric=os.getenv('METRIC_NAME'),                metric_prompt_template= load_prompt_template()                )                   ],       experiment=os.getenv('EXPERIMENT_NAME')   ).evaluate(       response_column_name= 'retrieved_context',       experiment_run_name=f\"{timestamp}-{embedding_model}-{model.replace('.', '-')}\"   )   baseline_metrics = run_eval(os.getenv(\"MODEL\"), os.getenv(\"BASELINE_EMBEDDING_MODEL\"), os.getenv(\"BASELINE_DATASET_URI\")) candidate_metrics = run_eval(os.getenv(\"MODEL\"), os.getenv(\"CANDIDATE_EMBEDDING_MODEL\"), os.getenv(\"CANDIDATE_DATASET_URI\")) clear_output() print(\"Average score for baseline model retrieval quality:\", round(baseline_metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/mean'],3)) print(\"Average score for candidate model retrieval quality:\", round(candidate_metrics.summary_metrics[f'{os.getenv(\"METRIC_NAME\")}/mean'],3))"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#rag-embeddings-retrieval-eval-recipe","title":"RAG Embeddings Retrieval Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare performance of two embedding models on a RAG dataset using Vertex AI Evaluation Service.</p> <p>We will be looking at <code>text-embedding-004</code> as our baseline model and <code>text-embedding-005</code> as our candidate model. Please follow the documentation here to get an understanding of the various text embedding models.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#prerequisite","title":"Prerequisite\u00b6","text":"<p>This recipe assumes that the user has already created datasets for the baseline embedding model and the candidate embedding model. The user needs to generate the datasets for the baseline(text-embedding-004) and candidate(text-embedding-005) embedding models. Please refer to RAG Engine generation notebook to create two separate RAG engines and set up corresponding datasets. The <code>retrieved_context</code> column in the dataset is the context retrieved from the respective RAG engine for each one of the questions.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#configure-eval-settings","title":"Configure Eval Settings\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#install-python-libraries","title":"Install Python Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#authenticate-to-google-cloud-requires-permission-to-open-a-popup-window","title":"Authenticate to Google Cloud (requires permission to open a popup window)\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_colab/rag_embeddings_eval/#run-the-eval-on-both-models-on-the-pairwise-autorater","title":"Run the eval on both models on the Pairwise Autorater\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_script/","title":"RAG Retrieval","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of two embedding models on a RAG dataset using Vertex AI Evaluation Service.</p> <p>We will be looking at <code>text-embedding-004</code> as our baseline model and <code>text-embedding-005</code> as our candidate model. Please follow the documentation here to get an understanding of the various text embedding models. </p> <ul> <li> <p>Use case: RAG retrieval</p> </li> <li> <p>Metric: This eval uses a Pointwise Retrieval quality template to evaluate the responses and pick a model as the winner. We will define <code>retrieval quality</code> as the metric here. It checks whether the <code>retrieved_context</code> contains all the key information present in <code>reference</code>.</p> </li> <li> <p>Evaluation Datasets are based on RAG Dataset in compliance with the following license. They include 8 randomly sampled prompts in JSONL files <code>baseline_dataset.jsonl</code> and <code>candidate_dataset.jsonl</code> with the following structure:</p> <ul> <li><code>question</code>: User inputted question </li> <li><code>reference</code>: The golden truth answer for the question</li> <li><code>retrieved_context</code>: The context retrieved from the model.</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with two prompt variables ( <code>reference</code> and <code>retrieved_context</code>) that are automatically populated from our dataset.</p> </li> <li> <p>This eval recipe uses an LLM judge model(gemini-2.0-flash) to evaluate the retrieval quality of the embedding models. </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_script/#prerequisite","title":"Prerequisite","text":"<p>This recipe assumes that the user has already created datasets for the baseline embedding model and the candidate embedding model. The user needs to generate the datasets for the baseline(text-embedding-004) and candidate(text-embedding-005) embedding models. Please refer to RAG Engine generation notebook to create two separate RAG engines and set up corresponding datasets. The <code>retrieved_context</code> column in the dataset is the context retrieved from the respective RAG engine for each one of the questions.</p> <ul> <li> <p>Python script <code>eval.py</code> configures the evaluation:</p> <ul> <li><code>run_eval</code>: configures the evaluation task, runs it on the 2 models and prints the results.</li> <li><code>load_dataset</code>: loads the dataset including the contents of all documents.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd rag_embeddings/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output. </p> </li> <li> <p>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics scores.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/rag_embeddings/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<p>We will have two runs, one for the baseline model and the candidate model</p> <ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>model</code>  in the run_eval calls (e.g., 'gemini-2.0-flash') to the LLM you want to use for performing the evaluation task.</li> <li>set the parameter <code>embedding_model</code> to the model that you want to run the evaluation for</li> <li>configure a unique <code>experiment_name</code> for tracking purposes</li> <li>set the parameter <code>dataset_local_path</code> to the file you are running the evaluations for </li> </ul> </li> <li>Replace the contents of <code>prompt_template.txt</code> with your custom prompt template. Make sure that prompt template variables map to the dataset attributes.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/promptfoo/","title":"Summarization","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a Summarization prompt with Gemini 1.0 and Gemini 2.0 using a labeled dataset and open source evaluation tool Promptfoo.</p> <p></p> <ul> <li> <p>Use case: summarize a news article.</p> </li> <li> <p>The Evaluation Dataset<sup>1</sup> includes 5 news articles stored as plain text files, and a JSONL file with ground truth labels: <code>dataset.jsonl</code>. Each record in this file includes 2 attributes wrapped in the <code>vars</code> object. This structure allows Promptfoo to inject the article text into the prompt template, and find ground truth label required to score the quality of model-generated summaries:</p> <ul> <li><code>document</code>: relative path to the plain text file containing the news article</li> <li><code>summary</code>: ground truth label (short summary of the article)</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with variable <code>document</code> that gets populated from the corresponding dataset attribute.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all Promptfoo configuration:</p> <ul> <li><code>providers</code>: list of models that will be evaluated</li> <li><code>prompts</code>: location of the prompt template file</li> <li><code>tests</code>: location of the labeled dataset file</li> <li><code>defaultTest</code>: defines the scoring logic:<ol> <li><code>type: rouge-n</code> rates similarity between the model response and the ground truth label</li> <li><code>value: \"{{summary}}\"</code> instructs Promptfoo to use the \"summary\" dataset attribute as the ground truth label</li> </ol> </li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li> <p>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</p> <pre><code>cd summarization/promptfoo\npromptfoo eval\n</code></pre> </li> <li> <p>Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the configuration file <code>promptfooconfig.yaml</code> to a new folder.</li> <li>Add your labeled dataset file with JSONL schema similar to <code>dataset.jsonl</code>. </li> <li>Save your prompt template to <code>prompt_template.txt</code> and make sure that the template variables map to the variables defined in your dataset.</li> <li>That's it! You are ready to run <code>promptfoo eval</code>. If needed, add alternative prompt templates or additional metrics to promptfooconfig.yaml as explained here.</li> </ol> <ol> <li> <p>Dataset (XSum) citation:  @InProceedings{xsum-emnlp,   author    = {Shashi Narayan and Shay B. Cohen and Mirella Lapata},   title     = {Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization},   booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},   year      = {2018},   address   = {Brussels, Belgium}, }\u00a0\u21a9</p> </li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_colab/summarization_eval/","title":"Summarization eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     https://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"as is\" basis,\n# without warranties or conditions of any kind, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at #     https://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"as is\" basis, # without warranties or conditions of any kind, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: summarize a news article.</p> </li> <li><p>Metric: this eval uses an Autorater (LLM Judge) to rate Summarization Quality.</p> </li> <li><p>Evaluation Dataset is based on XSum. It includes 5 news articles stored as plain text files, and a JSONL file with ground truth labels: <code>dataset.jsonl</code>. Each record in this file includes 2 attributes:</p> <ul> <li><code>document</code>: relative path to the plain text file containing the news article</li> <li><code>reference</code>: ground truth label (short summary of the article)</li> </ul> </li> <li><p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with variable <code>document</code> that gets populated from the corresponding dataset attribute.</p> </li> </ul> <p>Step 1 of 4: Configure eval settings</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id            # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=eval-summarization    # Creates Vertex AI Experiment to track the eval runs\nMODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model\nMODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model\nDATASET_URI=\"gs://gemini_assets/summarization/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=gs://gemini_assets/summarization/prompt_template.txt  # Text file in Google Cloud Storage\n</pre> %%writefile .env PROJECT_ID=your-project-id            # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=eval-summarization    # Creates Vertex AI Experiment to track the eval runs MODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model MODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model DATASET_URI=\"gs://gemini_assets/summarization/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=gs://gemini_assets/summarization/prompt_template.txt  # Text file in Google Cloud Storage <p>Step 2 of 4: Install Python libraries</p> In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) <p>Step 3 of 4: Authenticate to Google Cloud (requires permission to open a popup window)</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nload_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> import os import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) <p>Step 4 of 4: Run the eval on both models and compare the Accuracy scores</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport pandas as pd\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory\n\ndef load_file(gcs_uri: str) -&gt; str:\n    blob = storage.Blob.from_string(gcs_uri, storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef load_dataset(dataset_uri: str):\n    jsonl = load_file(dataset_uri)\n    samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]\n    df = pd.DataFrame(samples)\n    df['document_text'] = df['document_uri'].apply(lambda document_uri: load_file(document_uri))\n    return df[['document_text', 'reference']]\n\ndef run_eval(model: str) -&gt; EvalResult:\n  timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n  return EvalTask(\n      dataset=load_dataset(os.getenv(\"DATASET_URI\")),\n      metrics=[MetricPromptTemplateExamples.Pointwise.SUMMARIZATION_QUALITY],\n      experiment=os.getenv('EXPERIMENT_NAME')\n  ).evaluate(\n      model=GenerativeModel(model),\n      prompt_template=load_file(os.getenv(\"PROMPT_TEMPLATE_URI\")),\n      experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"\n  )\n\nbaseline_results = run_eval(os.getenv(\"MODEL_BASELINE\"))\ncandidate_results = run_eval(os.getenv(\"MODEL_CANDIDATE\"))\nclear_output()\nprint(f\"Baseline model score: {baseline_results.summary_metrics['summarization_quality/mean']:.2f}\")\nprint(f\"Candidate model score: {candidate_results.summary_metrics['summarization_quality/mean']:.2f}\")\n</pre> import json import pandas as pd from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, MetricPromptTemplateExamples from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory  def load_file(gcs_uri: str) -&gt; str:     blob = storage.Blob.from_string(gcs_uri, storage.Client())     return blob.download_as_string().decode('utf-8')  def load_dataset(dataset_uri: str):     jsonl = load_file(dataset_uri)     samples = [json.loads(line) for line in jsonl.splitlines() if line.strip()]     df = pd.DataFrame(samples)     df['document_text'] = df['document_uri'].apply(lambda document_uri: load_file(document_uri))     return df[['document_text', 'reference']]  def run_eval(model: str) -&gt; EvalResult:   timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()   return EvalTask(       dataset=load_dataset(os.getenv(\"DATASET_URI\")),       metrics=[MetricPromptTemplateExamples.Pointwise.SUMMARIZATION_QUALITY],       experiment=os.getenv('EXPERIMENT_NAME')   ).evaluate(       model=GenerativeModel(model),       prompt_template=load_file(os.getenv(\"PROMPT_TEMPLATE_URI\")),       experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"   )  baseline_results = run_eval(os.getenv(\"MODEL_BASELINE\")) candidate_results = run_eval(os.getenv(\"MODEL_CANDIDATE\")) clear_output() print(f\"Baseline model score: {baseline_results.summary_metrics['summarization_quality/mean']:.2f}\") print(f\"Candidate model score: {candidate_results.summary_metrics['summarization_quality/mean']:.2f}\")  <p>You can access all prompts and model responses in <code>candidate_results.metrics_table</code></p> <p>Dataset (XSum) citation: @InProceedings{xsum-emnlp, author    = {Shashi Narayan and Shay B. Cohen and Mirella Lapata}, title     = {Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization}, booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, year      = {2018} }</p> <p>Please use our documentation to learn about all available metrics and customization options.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_colab/summarization_eval/#summarization-eval-recipe","title":"Summarization Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare performance of two models on a summarization task using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_script/","title":"Summarization","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a summarization prompt with Gemini 1.0 and Gemini 2.0 using  Vertex AI Evaluation Service.</p> <p></p> <ul> <li> <p>Use case: summarize a news article.</p> </li> <li> <p>The Evaluation Dataset<sup>1</sup> includes 5 news articles stored as plain text files, and a JSONL file with ground truth labels: <code>dataset.jsonl</code>. Each record in this file includes 2 attributes:</p> <ul> <li><code>document</code>: relative path to the plain text file containing the news article</li> <li><code>reference</code>: ground truth label (short summary of the article)</li> </ul> </li> <li> <p>Prompt Template is a zero-shot prompt located in <code>prompt_template.txt</code> with variable <code>document</code> that gets populated from the corresponding dataset attribute.</p> </li> <li> <p>Python script <code>eval.py</code> configures the evaluation:</p> <ul> <li><code>run_eval</code>: configures the evaluation task, runs it on the 2 models and prints the results.</li> <li><code>load_dataset</code>: loads the dataset including the contents of all documents.</li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_script/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd summarization/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting metrics will be displayed in the script output. You can find the prompts and model responses stored in <code>candidate_results.metrics_table</code>.</p> </li> <li>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics scores.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/summarization/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>baseline_model</code> to the model that is currently used by your application</li> <li>set the parameter <code>candidate_model</code> to the model that you want to compare with your current model</li> <li>configure a unique <code>experiment_name</code> for tracking purposes</li> </ul> </li> <li>Replace the contents of <code>dataset.jsonl</code> with your custom data in the same format.</li> <li>Replace the contents of <code>prompt_template.txt</code> with your custom prompt template. Make sure that prompt template variables map to the dataset attributes.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe.</li> </ol> <ol> <li> <p>Dataset (XSum) citation:  @InProceedings{xsum-emnlp,   author    = {Shashi Narayan and Shay B. Cohen and Mirella Lapata},   title     = {Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization},   booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},   year      = {2018},   address   = {Brussels, Belgium}, }\u00a0\u21a9</p> </li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/promptfoo/","title":"Text Classification","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/promptfoo/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a text classification prompt with Gemini 1.0 and Gemini 2.0 using a labeled dataset and open source evaluation tool Promptfoo.</p> <p></p> <ul> <li> <p>Use case: given a Product Description find the most relevant Product Category from a predefined list of categories.</p> </li> <li> <p>Metric: this eval uses a single deterministic metric \"Accuracy\" calculated by comparing model responses with ground truth labels. </p> </li> <li> <p>Labeled evaluation dataset (<code>dataset.jsonl</code>) is based on MAVE dataset from Google Research. It includes 6 records that represent products from different categories. Each record provides two attributes which are wrapped in the <code>vars</code> object. This dataset structure allows Promptfoo to recognize variables that are needed to populate prompt templates, and ground truth labels used for scoring:</p> <ul> <li><code>product</code>: product name and description</li> <li><code>category</code>: the name of correct product category which serves as the ground truth label</li> </ul> </li> <li> <p>Prompt template is a zero-shot prompt located in <code>prompt_template.txt</code> with just one prompt variable <code>product</code> that maps to the <code>product</code> attribute in the dataset.</p> </li> <li> <p><code>promptfooconfig.yaml</code> contains all Promptfoo configuration:</p> <ul> <li><code>providers</code>: list of models that will be evaluated</li> <li><code>prompts</code>: location of the prompt template file</li> <li><code>tests</code>: location of the labeled dataset file</li> <li><code>defaultTest</code>: defines how we calculate the Accuracy:<ol> <li><code>transform: output.trim()</code> removes any white space around the model response</li> <li><code>type: equals</code> scores model responses based on exact match with ground truth labels</li> <li><code>value: \"{{category}}\"</code> instructs Promptfoo to use the \"category\" attribute in our dataset as the ground truth label</li> </ol> </li> </ul> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/promptfoo/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Install Promptfoo using these instructions.</p> </li> <li> <p>Navigate to the Eval Recipe directory in terminal and run the command <code>promptfoo eval</code>.</p> <pre><code>cd text_classification/promptfoo\npromptfoo eval\n</code></pre> </li> <li> <p>Run <code>promptfoo view</code> to analyze the eval results. You can switch the Display option to <code>Show failures only</code> in order to investigate any underperforming prompts.</p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/promptfoo/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Copy the configuration file <code>promptfooconfig.yaml</code> to a new folder.</li> <li>Add your labeled dataset file with JSONL schema similar to <code>dataset.jsonl</code>. </li> <li>Save your prompt template to <code>prompt_template.txt</code> and make sure that the template variables map to the variables defined in your dataset.</li> <li>That's it! You are ready to run <code>promptfoo eval</code>. If needed, add alternative prompt templates or additional metrics to promptfooconfig.yaml as explained here.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_colab/text_classification_eval/","title":"Text classification eval","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2025 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2025 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <ul> <li><p>Use case: given a Product Description find the most relevant Product Category from a predefined list of categories.</p> </li> <li><p>Metric: this eval uses a single deterministic metric \"Accuracy\" calculated by comparing model responses with ground truth labels.</p> </li> <li><p>Labeled evaluation dataset dataset.jsonl is based MAVE dataset from Google Research. It includes 6 records that represent products from different categories. Each record includes two attributes:</p> <ul> <li><code>product</code>: product name and description</li> <li><code>reference</code>: correct product category name which serves as the ground truth label</li> </ul> </li> <li><p>Prompt template is a zero-shot prompt located in prompt_template.txt with just one prompt variable <code>product</code> that maps to the <code>product</code> attribute in the dataset.</p> </li> </ul> <p>Step 1 of 4: Configure eval settings</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile .env\nPROJECT_ID=your-project-id            # Google Cloud Project ID\nLOCATION=us-central1                  # Region for all required Google Cloud services\nEXPERIMENT_NAME=eval-classification   # Creates Vertex AI Experiment to track the eval runs\nMODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model\nMODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model\nDATASET_URI=\"gs://gemini_assets/classification_vertex/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage\nPROMPT_TEMPLATE_URI=gs://gemini_assets/classification_vertex/prompt_template.txt  # Text file in Google Cloud Storage\n</pre> %%writefile .env PROJECT_ID=your-project-id            # Google Cloud Project ID LOCATION=us-central1                  # Region for all required Google Cloud services EXPERIMENT_NAME=eval-classification   # Creates Vertex AI Experiment to track the eval runs MODEL_BASELINE=gemini-1.5-flash-002   # Name of your current model MODEL_CANDIDATE=gemini-2.0-flash-001  # This model will be compared to the baseline model DATASET_URI=\"gs://gemini_assets/classification_vertex/dataset.jsonl\"  # Evaluation dataset in Google Cloud Storage PROMPT_TEMPLATE_URI=gs://gemini_assets/classification_vertex/prompt_template.txt  # Text file in Google Cloud Storage <p>Step 2 of 4: Install Python libraries</p> In\u00a0[\u00a0]: Copied! <pre>%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv\n# The error \"session crashed\" is expected. Please ignore it and proceed to the next cell.\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)\n</pre> %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] python-dotenv # The error \"session crashed\" is expected. Please ignore it and proceed to the next cell. import IPython IPython.Application.instance().kernel.do_shutdown(True) <p>Step 3 of 4: Authenticate and initialize Vertex AI</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nimport vertexai\nfrom dotenv import load_dotenv\nfrom google.cloud import storage\n\nload_dotenv(override=True)\nif os.getenv(\"PROJECT_ID\") == \"your-project-id\":\n    raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\")\nif \"google.colab\" in sys.modules:  \n    from google.colab import auth  \n    auth.authenticate_user()\nvertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION'))\n</pre> import os import sys import vertexai from dotenv import load_dotenv from google.cloud import storage  load_dotenv(override=True) if os.getenv(\"PROJECT_ID\") == \"your-project-id\":     raise ValueError(\"Please configure your Google Cloud Project ID in the first cell.\") if \"google.colab\" in sys.modules:       from google.colab import auth       auth.authenticate_user() vertexai.init(project=os.getenv('PROJECT_ID'), location=os.getenv('LOCATION')) <p>Step 4 of 4: Run the eval on both models and compare the Accuracy scores</p> In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\nfrom IPython.display import clear_output\nfrom vertexai.evaluation import EvalTask, EvalResult, CustomMetric, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory\n\ndef case_insensitive_match(record: dict[str, str]) -&gt; dict[str, float]:\n    response = record[\"response\"].strip().lower()\n    label = record[\"reference\"].strip().lower()\n    return {\"accuracy\": 1.0 if label == response else 0.0}\n\ndef load_prompt_template() -&gt; str:\n    blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())\n    return blob.download_as_string().decode('utf-8')\n\ndef run_eval(model: str) -&gt; EvalResult:\n  timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()\n  return EvalTask(\n      dataset=os.getenv(\"DATASET_URI\"),\n      metrics=[CustomMetric(name=\"accuracy\", metric_function=case_insensitive_match)],\n      experiment=os.getenv('EXPERIMENT_NAME')\n  ).evaluate(\n      model=GenerativeModel(model),\n      prompt_template=load_prompt_template(),\n      experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"\n  )\n\nbaseline = run_eval(os.getenv(\"MODEL_BASELINE\"))\ncandidate = run_eval(os.getenv(\"MODEL_CANDIDATE\"))\nclear_output()\nprint(\"Baseline model accuracy:\", baseline.summary_metrics[\"accuracy/mean\"])\nprint(\"Candidate model accuracy:\", candidate.summary_metrics[\"accuracy/mean\"])\n</pre> from datetime import datetime from IPython.display import clear_output from vertexai.evaluation import EvalTask, EvalResult, CustomMetric, MetricPromptTemplateExamples from vertexai.generative_models import GenerativeModel, HarmBlockThreshold, HarmCategory  def case_insensitive_match(record: dict[str, str]) -&gt; dict[str, float]:     response = record[\"response\"].strip().lower()     label = record[\"reference\"].strip().lower()     return {\"accuracy\": 1.0 if label == response else 0.0}  def load_prompt_template() -&gt; str:     blob = storage.Blob.from_string(os.getenv(\"PROMPT_TEMPLATE_URI\"), storage.Client())     return blob.download_as_string().decode('utf-8')  def run_eval(model: str) -&gt; EvalResult:   timestamp = f\"{datetime.now().strftime('%b-%d-%H-%M-%S')}\".lower()   return EvalTask(       dataset=os.getenv(\"DATASET_URI\"),       metrics=[CustomMetric(name=\"accuracy\", metric_function=case_insensitive_match)],       experiment=os.getenv('EXPERIMENT_NAME')   ).evaluate(       model=GenerativeModel(model),       prompt_template=load_prompt_template(),       experiment_run_name=f\"{timestamp}-{model.replace('.', '-')}\"   )  baseline = run_eval(os.getenv(\"MODEL_BASELINE\")) candidate = run_eval(os.getenv(\"MODEL_CANDIDATE\")) clear_output() print(\"Baseline model accuracy:\", baseline.summary_metrics[\"accuracy/mean\"]) print(\"Candidate model accuracy:\", candidate.summary_metrics[\"accuracy/mean\"]) <p>This Eval Recipe is intended to be a simple starting point. Please use our documentation to learn about all available metrics and customization options.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_colab/text_classification_eval/#text-classification-eval-recipe","title":"Text Classification Eval Recipe\u00b6","text":"<p>This Eval Recipe demonstrates how to compare performance of two models on a text classification prompt using Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_script/","title":"Text Classification","text":""},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_script/#eval-recipe-for-model-migration","title":"Eval Recipe for model migration","text":"<p>This Eval Recipe demonstrates how to compare performance of a text classification prompt with Gemini 1.0 and Gemini 2.0 using  Vertex AI Evaluation Service.</p> <p></p> <ul> <li> <p>Use case: given a Product Description find the most relevant Product Category from a predefined list of categories.</p> </li> <li> <p>Metric: this eval uses a single deterministic metric \"Accuracy\" calculated by comparing model responses with ground truth labels. </p> </li> <li> <p>Labeled evaluation dataset (<code>dataset.jsonl</code>) is based on MAVE dataset from Google Research. It includes 6 records that represent products from different categories. Each record provides two attributes which are wrapped in the <code>vars</code> object. This dataset structure allows Promptfoo to recognize variables that are needed to populate prompt templates, and ground truth labels used for scoring:</p> <ul> <li><code>product</code>: product name and description</li> <li><code>reference</code>: the name of correct product category which serves as the ground truth label</li> </ul> </li> <li> <p>Prompt template is a zero-shot prompt located in <code>prompt_template.txt</code> with just one prompt variable <code>product</code> that maps to the <code>product</code> attribute in the dataset.</p> </li> <li> <p>Python script <code>eval.py</code> configures the evaluation:</p> <ul> <li><code>run_eval</code>: configures the evaluation task, runs it on the 2 models and prints the results.</li> <li><code>case_insensitive_match</code>: scores the accuracy of model responses by comparing them to ground truth labels.  </li> </ul> </li> <li> <p>Shell script <code>run.sh</code> installs the required Python libraries and runs <code>eval.py</code> </p> </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_script/#how-to-run-this-eval-recipe","title":"How to run this Eval Recipe","text":"<ul> <li> <p>Google Cloud Shell is the easiest option as it automatically clones our Github repo:</p> <p> </p> </li> <li> <p>Alternatively, you can use the following command to clone this repo to any Linux environment with configured Google Cloud Environment:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git &amp;&amp; \\\ncd applied-ai-engineering-samples &amp;&amp; \\\ngit sparse-checkout init &amp;&amp; \\\ngit sparse-checkout set genai-on-vertex-ai/gemini/model_upgrades &amp;&amp; \\\ngit pull origin main\ncd genai-on-vertex-ai/gemini/model_upgrades\n</code></pre> </li> <li> <p>Navigate to the Eval Recipe directory in terminal, set your Google Cloud Project ID and run the shell script <code>run.sh</code>.</p> <pre><code>cd text_classification/vertex_script\nexport PROJECT_ID=\"[your-project-id]\"\n./run.sh\n</code></pre> </li> <li> <p>The resulting scores will be displayed in the script output. </p> </li> <li>You can use Vertex AI Experiments to view the history of evaluations for each experiment, including the final metrics scores.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/model_upgrades/text_classification/vertex_script/#how-to-customize-this-eval-recipe","title":"How to customize this Eval Recipe:","text":"<ol> <li>Edit the Python script <code>eval.py</code>:<ul> <li>set the <code>project</code> parameter of vertexai.init to your Google Cloud Project ID.</li> <li>set the parameter <code>baseline_model</code> to the model that is currently used by your application</li> <li>set the parameter <code>candidate_model</code> to the model that you want to compare with your current model</li> <li>configure a unique <code>experiment_name</code> for each template for tracking purposes</li> </ul> </li> <li>Replace the contents of <code>dataset.jsonl</code> with your custom data in the same format.</li> <li>Replace the contents of <code>prompt_template.txt</code> with your custom prompt template. Make sure that prompt template variables have the same names as dataset attributes.</li> <li>Please refer to our documentation if you want to further customize your evaluation. Vertex AI Evaluation Service has a lot of features that are not included in this recipe, including LLM-based autoraters that can provide valuable metrics even without ground truth labels.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/","title":"Needle In A Haystack - Pressure Testing LLMs","text":"<p>This repository is a fork of Greg Kamradt's needle-in-a-haystack repository. </p> <p>Original Repository: https://github.com/gkamradt/LLMTest_NeedleInAHaystack</p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#what-is-the-needle-in-a-haystack-test","title":"What is the 'Needle in a Haystack' Test?","text":"<p>The 'Needle in a Haystack' test is a common technique used to evaluate the performance of a language model on long context windows. The basic idea is to place a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack') and then ask the model to retrieve this statement. By varying the depth of the needle within the haystack and the length of the context window, we can measure how well the model can retrieve the correct statement.</p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#steps","title":"Steps","text":"<ol> <li>Place a random fact or statement (the 'needle') in the middle of a long context window (the 'haystack')</li> <li>Ask the model to retrieve this statement</li> <li>Compare the model's response to the actual needle (we use an LLM to score the similarity)</li> <li>Iterate over various document depths (where the needle is placed) and context lengths to measure performance robustly</li> </ol> <p>This test has been used in a number of papers to evaluate the performance of LLMs on long context windows, including the Gemini 1.5 paper.</p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#differences-from-original-repository","title":"Differences from Original Repository","text":"<p>This fork adds support for using Vertex AI Gemini (and only Vertex AI Gemini) models.</p> <p>It also differs in that the needle behavior matches that used in the Gemini 1.5 paper. Quoting from the paper:</p> <p>We insert a needle at linearly spaced intervals from the beginning to the end of the context, where the needle is i.e., \u201cThe special magic {city} number is: {number}\u201d where the city and number are varied for each query, and query the model to return the magic number for a specific city.</p> <p>This dynamic needle is to prevent false positives due to caching. To revert to the original static needle behavior use <code>--dynamic_needle=False</code></p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#how-to-use","title":"How to Use","text":""},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#setup-google-cloud-project","title":"Setup Google Cloud Project","text":"<ol> <li>Make sure that billing is enabled for your Google Cloud project.</li> <li>Make sure that the Vertex AI API is enabled for your project.</li> </ol> <p>You can run the following steps in Cloud Shell or Vertex AI Workbench or Colab Enterprise.</p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#clone-repository","title":"Clone Repository","text":"<p>You may run the following commands from your local terminal or from Google Cloud Shell. We tested the code using Python 3.11 but other versions may work.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git\ncd applied-ai-engineering-samples/genai-on-vertex-ai/gemini/needle_in_a_haystack\n</code></pre>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#setup-virtual-environment","title":"Setup Virtual Environment","text":"<p>We recommend setting up a virtual environment to isolate Python dependencies, ensuring project-specific packages without conflicting with system-wide installations. </p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#install-requirements","title":"Install Requirements","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#quickstart-configurations","title":"Quickstart Configurations","text":"<p>Run a test with a single context length and document depth. <pre><code>needlehaystack.run_test --gcp_project_id &lt;YOUR_PROJECT_ID&gt;  --document_depth_percents \"[50]\" --context_lengths \"[200]\"\n</code></pre></p> <p>Run a test with 11 different context lengths ranging from 4,000 to 2,000,000 tokens and 11 evenly spaced document depths for each context length. This will result in 11*11 = 121 tests and can take a while to run. <pre><code>needlehaystack.run_test --gcp_project_id &lt;YOUR_PROJECT_ID&gt; --document_depth_percent_intervals 11 --context_lengths \"[4000,8000,16000,32000,64000,128000,256000,512000,1000000,1500000,2000000]\"\n</code></pre></p> <p>In the event of an individual test failure due to rate limits or other issues, the program will print the exception and move on to the next test. If you re-run the command at a later time it will check the <code>results/</code> directory and skip any tests that have already been completed.</p>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#output-and-interpretation","title":"Output and Interpretation","text":"<p>The results will be saved to the <code>results/</code> directory, one .json file per test.</p> <p>You may generate a visualization of these results using the notebook niah_visualize.ipynb that will look like the below:</p> <p></p> <ul> <li>x-axis - Size of context window (tokens)</li> <li>y-axis - Depth of needle placement within context. 0 = beginning, 100 = end.</li> <li>colors - The agreement of the ground truth needle vs the model retrieved needle. Green corresponds to 10/10 agreement, Red corresponds to 1/10 agreement, in between colors indicate partial agreement. </li> </ul>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#configuration-options","title":"Configuration Options","text":"<p>You may modify your test configuration with the following options:</p> <ul> <li><code>gcp_project_id</code> - The GCP project ID used to run the test. </li> <li><code>model_name</code> - Model name of the language model accessible by the provider. Defaults to <code>gemini-2.0-flash-001</code></li> <li><code>evaluator_model_name</code> - Model name of the language model accessible by the evaluator. Defaults to <code>gemini-2.0-flash-001</code></li> <li><code>dynamic_needle</code> - Whether to use the dynamic needle or not. Defaults to <code>True</code></li> <li><code>needle</code> - The statement or fact which will be placed in your context. Only used if <code>dynamic_needle=False</code></li> <li><code>haystack_dir</code> - The directory which contains the text files to load as background context. Only text files are supported</li> <li><code>retrieval_question</code> - The question with which to retrieve your needle in the background context</li> <li><code>results_version</code> - You may want to run your test multiple times for the same combination of length/depth, change the version number if so</li> <li><code>num_concurrent_requests</code> - Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.</li> <li><code>save_results</code> - Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False. If <code>save_results = True</code>, then this script will populate a <code>result/</code> directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.</li> <li><code>save_contexts</code> - Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False</li> <li><code>final_context_length_buffer</code> - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.</li> <li><code>context_lengths_min</code> - The starting point of your context lengths list to iterate</li> <li><code>context_lengths_max</code> - The ending point of your context lengths list to iterate</li> <li><code>context_lengths_num_intervals</code> - The number of intervals between your min/max to iterate through</li> <li><code>context_lengths</code> - A custom set of context lengths. This will override the values set for <code>context_lengths_min</code>, max, and intervals if set</li> <li><code>document_depth_percent_min</code> - The starting point of your document depths. Should be int &gt; 0</li> <li><code>document_depth_percent_max</code> - The ending point of your document depths. Should be int &lt; 100</li> <li><code>document_depth_percent_intervals</code> - The number of iterations to do between your min/max points</li> <li><code>document_depth_percents</code> - A custom set of document depths lengths. This will override the values set for <code>document_depth_percent_min</code>, max, and intervals if set</li> <li><code>document_depth_percent_interval_type</code> - Determines the distribution of depths to iterate over. 'linear' or 'sigmoid</li> <li><code>seconds_to_sleep_between_completions</code> - Default: None, set # of seconds if you'd like to slow down your requests</li> <li><code>print_ongoing_status</code> - Default: True, whether or not to print the status of test as they complete</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/needle_in_a_haystack/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. Use of this software requires attribution to the original author and project, as detailed in the license.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/","title":"Overview","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/#vertex-ai-exploring-geminis-multimodal-and-long-context-window-capabilities","title":"Vertex AI: Exploring Gemini's multimodal and long context window capabilities","text":"<p>This folder contains code samples and guidance for how to leverage Gemini's unique multimodal and long context window capabilities on Vertex AI.</p> Recipe Description <code>multimodal/</code>        Multimodal prompting with Gemini      <code>pdf_processing/</code>        Analyzing large PDF files with Gemini      <code>spatial_reasoning/</code>      Multimodal analysis and reasoning use cases for spatial reasoning"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/","title":"Overview","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/#multimodal-prompting-with-gemini-15","title":"Multimodal prompting with Gemini 1.5","text":"<p>Gemini 1.5 Pro and 1.5 Flash models supports adding image, audio, video, and PDF files in text or chat prompts to generate a text or code response. Gemini 1.5 Pro supports up to 2 Million input tokens, making it possible to analyze long videos and audio files in a single prompt. This folder has examples to demonstrate multimodal capabilities of Gemini 1.5 and how to effectively write prompts for better results. </p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/#multimodal-prompting-for-images","title":"Multimodal Prompting for Images","text":"<p>Demonstrate prompting recipes and strategies for working with Gemini on images: - Image Understanding - Using system instruction - Structuring prompt with images - Adding few-shot examples the image prompt - Document understanding - Math understanding</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/#multimodal-prompting-for-audio","title":"Multimodal Prompting for Audio","text":"<p>Demonstrate prompting recipes and strategies for working with Gemini on audio files: - Audio Understanding - Effective prompting - Key event detection - Using System instruction - Generating structured output</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/#multimodal-prompting-for-videos","title":"Multimodal Prompting for Videos","text":"<p>Demonstrate prompting recipes and strategies for working with Gemini on video files: - Video Understanding - Key event detection - Using System instruction - Analyzing videos with step-by-step reasoning - Generating structured output - Using context caching for repeated queries</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/","title":"Multimodal Prompting with Gemini: Working with Audio","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Run in Colab   Open in Colab Enterprise       Open in Vertex AI Workbench   View on GitHub  Author(s) Michael Chertushkin Reviewer(s) Rajesh Thallam, Skander Hannachi Last updated 2024-09-16 In\u00a0[\u00a0]: Copied! <pre>! pip install google-cloud-aiplatform --upgrade --quiet --user\n</pre> ! pip install google-cloud-aiplatform --upgrade --quiet --user In\u00a0[\u00a0]: Copied! <pre># Restart kernel after installs so that your environment can access the new packages\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> # Restart kernel after installs so that your environment can access the new packages import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") In\u00a0[\u00a0]: Copied! <pre>import vertexai\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"  # @param {type:\"string\"}\n\nvertexai.init(project=PROJECT_ID, location=REGION)\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n</pre> import vertexai  PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} REGION = \"us-central1\"  # @param {type:\"string\"}  vertexai.init(project=PROJECT_ID, location=REGION) print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") In\u00a0[\u00a0]: Copied! <pre>from vertexai.generative_models import (GenerationConfig, GenerativeModel,\n                                        HarmBlockThreshold, HarmCategory, Part)\n</pre> from vertexai.generative_models import (GenerationConfig, GenerativeModel,                                         HarmBlockThreshold, HarmCategory, Part) In\u00a0[\u00a0]: Copied! <pre>import http.client\nimport textwrap\nimport typing\nimport urllib.request\n\nfrom google.cloud import storage\nfrom IPython import display\nfrom IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\ndef wrap(string, max_width=80):\n    return textwrap.fill(string, max_width)\n\n\ndef get_bytes_from_url(url: str) -&gt; bytes:\n    with urllib.request.urlopen(url) as response:\n        response = typing.cast(http.client.HTTPResponse, response)\n        bytes = response.read()\n    return bytes\n\n\ndef get_bytes_from_gcs(gcs_path: str):\n    bucket_name = gcs_path.split(\"/\")[2]\n    object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(object_prefix)\n    return blob.download_as_bytes()\n\n\ndef display_image(image_url: str, width: int = 300, height: int = 200):\n    if image_url.startswith(\"gs://\"):\n        image_bytes = get_bytes_from_gcs(image_url)\n    else:\n        image_bytes = get_bytes_from_url(image_url)\n    display.display(display.Image(data=image_bytes, width=width, height=height))\n\n\ndef display_video(video_url: str, width: int = 300, height: int = 200):\n    if video_url.startswith(\"gs://\"):\n        video_bytes = get_bytes_from_gcs(video_url)\n    else:\n        video_bytes = get_bytes_from_url(video_url)\n    display.display(\n        display.Video(\n            data=video_bytes,\n            width=width,\n            height=height,\n            embed=True,\n            mimetype=\"video/mp4\",\n        )\n    )\n\n\ndef display_audio(audio_url: str, width: int = 300, height: int = 200):\n    if audio_url.startswith(\"gs://\"):\n        audio_bytes = get_bytes_from_gcs(audio_url)\n    else:\n        audio_bytes = get_bytes_from_url(audio_url)\n    display.display(display.Audio(data=audio_bytes, embed=True))\n\n\ndef print_prompt(contents: list[str | Part]):\n    for content in contents:\n        if isinstance(content, Part):\n            if content.mime_type.startswith(\"image\"):\n                display_image(image_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"video\"):\n                display_video(video_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"audio\"):\n                display_audio(audio_url=content.file_data.file_uri)\n            else:\n                print(content)\n        else:\n            print(content)\n</pre> import http.client import textwrap import typing import urllib.request  from google.cloud import storage from IPython import display from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"   def wrap(string, max_width=80):     return textwrap.fill(string, max_width)   def get_bytes_from_url(url: str) -&gt; bytes:     with urllib.request.urlopen(url) as response:         response = typing.cast(http.client.HTTPResponse, response)         bytes = response.read()     return bytes   def get_bytes_from_gcs(gcs_path: str):     bucket_name = gcs_path.split(\"/\")[2]     object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])     storage_client = storage.Client()     bucket = storage_client.bucket(bucket_name)     blob = bucket.get_blob(object_prefix)     return blob.download_as_bytes()   def display_image(image_url: str, width: int = 300, height: int = 200):     if image_url.startswith(\"gs://\"):         image_bytes = get_bytes_from_gcs(image_url)     else:         image_bytes = get_bytes_from_url(image_url)     display.display(display.Image(data=image_bytes, width=width, height=height))   def display_video(video_url: str, width: int = 300, height: int = 200):     if video_url.startswith(\"gs://\"):         video_bytes = get_bytes_from_gcs(video_url)     else:         video_bytes = get_bytes_from_url(video_url)     display.display(         display.Video(             data=video_bytes,             width=width,             height=height,             embed=True,             mimetype=\"video/mp4\",         )     )   def display_audio(audio_url: str, width: int = 300, height: int = 200):     if audio_url.startswith(\"gs://\"):         audio_bytes = get_bytes_from_gcs(audio_url)     else:         audio_bytes = get_bytes_from_url(audio_url)     display.display(display.Audio(data=audio_bytes, embed=True))   def print_prompt(contents: list[str | Part]):     for content in contents:         if isinstance(content, Part):             if content.mime_type.startswith(\"image\"):                 display_image(image_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"video\"):                 display_video(video_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"audio\"):                 display_audio(audio_url=content.file_data.file_uri)             else:                 print(content)         else:             print(content) In\u00a0[\u00a0]: Copied! <pre># Gemini Config\nGENERATION_CONFIG = {\n    \"max_output_tokens\": 8192,\n    \"temperature\": 0.1,\n    \"top_p\": 0.95,\n}\n\nSAFETY_CONFIG = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n}\n\ngemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\")\naudio_path_prefix = (\n    \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/audio\"\n)\n\n\ndef generate(\n    model,\n    contents,\n    safety_settings=SAFETY_CONFIG,\n    generation_config=GENERATION_CONFIG,\n    as_markdown=False,\n):\n    responses = model.generate_content(\n        contents=contents,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        stream=False,\n    )\n    if isinstance(responses, list):\n        for response in responses:\n            if as_markdown:\n                display.display(display.Markdown(response.text))\n            else:\n                print(wrap(response.text), end=\"\")\n    else:\n        if as_markdown:\n            display.display(display.Markdown(responses.text))\n        else:\n            print(wrap(responses.text), end=\"\")\n</pre> # Gemini Config GENERATION_CONFIG = {     \"max_output_tokens\": 8192,     \"temperature\": 0.1,     \"top_p\": 0.95, }  SAFETY_CONFIG = {     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH, }  gemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\") audio_path_prefix = (     \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/audio\" )   def generate(     model,     contents,     safety_settings=SAFETY_CONFIG,     generation_config=GENERATION_CONFIG,     as_markdown=False, ):     responses = model.generate_content(         contents=contents,         generation_config=generation_config,         safety_settings=safety_settings,         stream=False,     )     if isinstance(responses, list):         for response in responses:             if as_markdown:                 display.display(display.Markdown(response.text))             else:                 print(wrap(response.text), end=\"\")     else:         if as_markdown:             display.display(display.Markdown(responses.text))         else:             print(wrap(responses.text), end=\"\") In\u00a0[\u00a0]: Copied! <pre>display_audio(\n    audio_url=\"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/audio/sound_1.mp3\"\n)\n</pre> display_audio(     audio_url=\"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/audio/sound_1.mp3\" ) In\u00a0[\u00a0]: Copied! <pre>audio_path = f\"{audio_path_prefix}/sound_1.mp3\"\naudio_content = Part.from_uri(uri=audio_path, mime_type=\"audio/mp3\")\nprompt = \"\"\"Provide a description of the audio.\nThe description should also contain anything important which people say in the audio.\"\"\"\n\ncontents = [audio_content, prompt]\n# print_prompt(contents)\n</pre> audio_path = f\"{audio_path_prefix}/sound_1.mp3\" audio_content = Part.from_uri(uri=audio_path, mime_type=\"audio/mp3\") prompt = \"\"\"Provide a description of the audio. The description should also contain anything important which people say in the audio.\"\"\"  contents = [audio_content, prompt] # print_prompt(contents) In\u00a0[6]: Copied! <pre>generate(gemini, contents, as_markdown=True)\n</pre> generate(gemini, contents, as_markdown=True) <p>This is an audio recording of CD2, an audio program to accompany English in Action 1, second edition, by Barbara H. Foley and Elizabeth R. Neblet. The copyright is 2018, National Geographic Learning, a part of Cengage Learning. The audio contains a section titled \"A. Listen and Repeat.\" The audio then lists the following sentences:</p> <ol> <li>He is eating.</li> <li>He is washing the car.</li> <li>She is listening to the radio.</li> <li>They are studying.</li> <li>He is cooking.</li> <li>She is sleeping.</li> <li>He is reading.</li> <li>She is drinking.</li> <li>They are talking.</li> <li>They are watching TV.</li> <li>He is doing his homework.</li> <li>She is cleaning the house.</li> <li>She is driving.</li> <li>They are walking.</li> <li>She is making lunch.</li> <li>He is doing the laundry.</li> </ol> <p>As we see the model correctly picked that this is a lesson in English, however we can improve the level of details.</p> In\u00a0[7]: Copied! <pre>prompt = \"\"\"You are an audio analyzer. You receive an audio and produce the \ndetailed description about what happens in the audio.\n\n&lt;INSTRUCTIONS&gt;\n- Determine what happens in the audio\n- Understand the hidden meaning of the audio\n- If there are dialogues, identify the talking personas\n- Make sure the description is clear and helpful\n&lt;/INSTRUCTIONS&gt;\n\nNow analyse the following audio\n\"\"\"\n\ncontents = [audio_content, prompt]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt = \"\"\"You are an audio analyzer. You receive an audio and produce the  detailed description about what happens in the audio.   - Determine what happens in the audio - Understand the hidden meaning of the audio - If there are dialogues, identify the talking personas - Make sure the description is clear and helpful   Now analyse the following audio \"\"\"  contents = [audio_content, prompt] generate(gemini, contents, as_markdown=True) <p>Okay, here is a detailed description of the audio:</p> <p>The audio is an audio program to accompany English in Action 1, second edition, by Barbara H. Foley and Elizabeth R. Neblet. It is copyrighted in 2018 by National Geographic Learning, a part of Cengage Learning.</p> <p>The audio contains a series of sentences that are read aloud. The sentences describe various actions that people are doing. The listener is instructed to listen and repeat each sentence.</p> <p>Here is a list of the sentences that are read aloud:</p> <ol> <li>He is eating.</li> <li>He is washing the car.</li> <li>She is listening to the radio.</li> <li>They are studying.</li> <li>He is cooking.</li> <li>She is sleeping.</li> <li>He is reading.</li> <li>She is drinking.</li> <li>They are talking.</li> <li>They are watching TV.</li> <li>He is doing his homework.</li> <li>She is cleaning the house.</li> <li>She is driving.</li> <li>They are walking.</li> <li>She is making lunch.</li> <li>He is doing the laundry.</li> </ol> <p>With the updated prompt, we are able to capture much more details, although this prompt is rather generic and can be used for other audio files. Now let's add these changes as system instruction and see.</p> In\u00a0[8]: Copied! <pre>system_prompt = \"\"\"You are an audio analyzer. You receive an audio and produce \nthe detailed description about what happens in the audio.\n\n&lt;INSTRUCTIONS&gt;\n- Determine what happens in the audio\n- Understand the hidden meaning of the audio\n- If there are dialogues, identify the talking personas\n- Make sure the description is clear and helpful\n&lt;/INSTRUCTIONS&gt;\n\"\"\"\n\nprompt = \"Now analyze the audio\"\n</pre> system_prompt = \"\"\"You are an audio analyzer. You receive an audio and produce  the detailed description about what happens in the audio.   - Determine what happens in the audio - Understand the hidden meaning of the audio - If there are dialogues, identify the talking personas - Make sure the description is clear and helpful  \"\"\"  prompt = \"Now analyze the audio\" In\u00a0[9]: Copied! <pre>gemini_si = GenerativeModel(\n    model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt\n)\n\ncontents = [audio_content, prompt]\ngenerate(gemini_si, contents, as_markdown=True)\n</pre> gemini_si = GenerativeModel(     model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt )  contents = [audio_content, prompt] generate(gemini_si, contents, as_markdown=True) <p>Okay, here is the analysis of the audio:</p> <p>General Description: The audio is an educational recording designed to accompany the \"English in Action 1\" textbook, second edition. It seems to be focused on teaching basic English vocabulary and grammar, specifically related to actions and present continuous tense.</p> <p>Content Breakdown:</p> <ul> <li>Introduction: A narrator introduces the audio program, mentioning the textbook it accompanies, the authors (Barbara H. Foley and Elizabeth R. Neblet), and the copyright information (2018, National Geographic Learning, a part of Cengage Learning).</li> <li>Instructions: A voice instructs the listener to \"Listen and repeat.\"</li> <li>Vocabulary/Grammar Practice: A series of numbered sentences are presented, each describing an action. The sentences are simple and use the present continuous tense (e.g., \"He is eating,\" \"She is washing the car\").</li> </ul> <p>Talking Personas:</p> <ul> <li>Narrator: A voice introduces the audio program and provides copyright information.</li> <li>Instructor: A voice gives instructions to the listener (e.g., \"Listen and repeat\").</li> <li>Speakers: Different voices (male and female) read the sentences describing the actions.</li> </ul> <p>Hidden Meaning/Purpose: The audio aims to help learners:</p> <ul> <li>Improve their listening comprehension skills.</li> <li>Practice pronunciation by repeating the sentences.</li> <li>Learn and reinforce vocabulary related to everyday actions.</li> <li>Understand and use the present continuous tense correctly.</li> </ul> <p>Overall: The audio is a straightforward and practical tool for English language learners, particularly beginners. It focuses on building foundational skills in listening, speaking, and grammar through repetition and simple sentence structures.</p> In\u00a0[10]: Copied! <pre>response_schema = {\n    \"type\": \"ARRAY\",\n    \"items\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"harmfulness_reasoning\": {\n                \"type\": \"STRING\",\n                \"description\": \"Step-by-step detailed reasoning about how harmful is the audio\",\n            },\n            \"harmfulness_score\": {\n                \"type\": \"INTEGER\",\n                \"description\": \"Number between 0 and 5 indicating how harmful is the audio\",\n            },\n        },\n        \"required\": [\"harmfulness_reasoning\", \"harmfulness_score\"],\n    },\n}\n</pre> response_schema = {     \"type\": \"ARRAY\",     \"items\": {         \"type\": \"OBJECT\",         \"properties\": {             \"harmfulness_reasoning\": {                 \"type\": \"STRING\",                 \"description\": \"Step-by-step detailed reasoning about how harmful is the audio\",             },             \"harmfulness_score\": {                 \"type\": \"INTEGER\",                 \"description\": \"Number between 0 and 5 indicating how harmful is the audio\",             },         },         \"required\": [\"harmfulness_reasoning\", \"harmfulness_score\"],     }, } In\u00a0[11]: Copied! <pre>structured_prompt = \"\"\"You are an audio analyzer. You receive an audio and \nproduce harmfulness score - how harmful this audio can be for kids.\"\"\"\n\ncontents = [audio_content, structured_prompt]\n\ngenerate(\n    gemini,\n    contents,\n    generation_config=GenerationConfig(\n        response_mime_type=\"application/json\", response_schema=response_schema\n    ),\n)\n</pre> structured_prompt = \"\"\"You are an audio analyzer. You receive an audio and  produce harmfulness score - how harmful this audio can be for kids.\"\"\"  contents = [audio_content, structured_prompt]  generate(     gemini,     contents,     generation_config=GenerationConfig(         response_mime_type=\"application/json\", response_schema=response_schema     ), ) <pre>[   {     \"harmfulness_reasoning\": \"The audio primarily contains instructional\ncontent for English language learning, featuring clear and neutral speech. There\nare no elements that could be considered harmful to children; it lacks any\noffensive language, violence, or suggestive themes. The overall tone is\neducational and safe for children of all ages.\",     \"harmfulness_score\": 1   }\n]</pre> <p>The model returned the correct score for the audio by asking the model to output \"reasoning\" along with the score. Adding \"reasoning\" field before the \"score\" gives a consistent and correct score. The intuition is  that LLM can generate \"reasoning\" first and rely on the thoughts to properly produce the score.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#multimodal-prompting-with-gemini-working-with-audio","title":"Multimodal Prompting with Gemini: Working with Audio\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#overview","title":"Overview\u00b6","text":"<p>Gemini 2.0 models supports adding image, audio, video, and PDF files in text or chat prompts for a text or code response. Gemini 2.0 Flash supports up to 1 Million input tokens with up to 8.4 hours length of audio per prompt. You can add audio to Gemini requests to perform audio analysis tasks such as transcribing audio, audio chapterization (or localization), key event detection, audio translation and more.</p> <p>In this notebook we cover prompting recipes and strategies for working with Gemini on audio files and show some examples on the way. This notebook is organized as follows:</p> <ul> <li>Audio Understanding</li> <li>Effective prompting</li> <li>Key event detection</li> <li>Using System instruction</li> <li>Generating structured output</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#getting-started","title":"Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.</li> <li>Make sure that billing is enabled for your project.</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API.</li> <li>Enable the Cloud Storage API.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, including the optional section, you will need to have the Owner role for your project.</p> <p>If you want to skip the optional section, you need at least the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#install-vertex-ai-sdk-for-python-and-other-dependencies-if-needed","title":"Install Vertex AI SDK for Python and other dependencies (If Needed)\u00b6","text":"<p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.## Install Vertex AI SDK for Python and other dependencies (If Needed)</p> <p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and Initialize Vertex AI SDK\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>REGION</code> unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#define-utility-functions","title":"Define Utility functions\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#initialize-gemini","title":"Initialize Gemini\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#prompt-1-audio-understanding","title":"Prompt #1. Audio Understanding\u00b6","text":"<p>This task requires the input to be presented in two different modalities: text and audio. The example of the API call is below, however this is non-optimal prompt and we can make it better.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#prompt-2-crafting-an-effective-prompt","title":"Prompt #2. Crafting an effective prompt\u00b6","text":"<p>To get the best results from Gemini for a task, think about both what you tell it and how you tell it.</p> <ul> <li>What: Include all the necessary information to solve the task, like instructions, examples, and background details.</li> <li>How:  Structure this information clearly.<ul> <li>Order: Organize prompt in a logical sequence.</li> <li>Delimiters/Separators: Use headings or keywords to highlight key information. XML tags or Markdown headers are a good way to format.</li> </ul> </li> </ul> <p>A well-structured prompt is easier for the model to understand and process, leading to more accurate and relevant responses.</p> <p>Let's rewrite the prompt and add a persona (or role), give clear goals, use XML tags as prompt separators.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#prompt-3-using-system-instruction","title":"Prompt #3. Using system instruction\u00b6","text":"<p>System Instruction (SI) is an effective way to steer Gemini's behavior and shape how the model responds to your prompt. SI can be used to describe model behavior such as persona, goal, tasks to perform, output format / tone / style, any constraints etc.</p> <p>SI behaves more \"sticky\" (or consistent) during multi-turn behavior. For example, if you want to achieve a behavior that the model will consistently follow, then system instruction is the best way to put this instruction.</p> <p>In this example, we will move the task rules to system instruction.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#prompt-4-audio-understanding-get-structured-outputs","title":"Prompt #4. Audio Understanding: Get structured outputs\u00b6","text":"<p>Gemini models can generate structured outputs such as JSON, providing a blueprint for the model's output. This feature is also referred to as controlled generation.</p> <p>In this example, we demonstrate Gemini to return structured output (JSON) from a audio analysis. One of the ways to achieve better understanding of audio (or any multimodal) content is to prompt the model to explain its \"reasoning\" about the response. This has proven to be very effective method, however it can increase the latency.</p> <p>Vertex AI Gemini API makes it easy to return JSON output by configuring response MIME type as <code>application/json</code>. Optionally, you can also configure <code>response_schema</code> with the JSON schema for the model to generate output as per the schema.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_audio/#conclusion","title":"Conclusion\u00b6","text":"<p>This demonstrated various examples of working with Gemini using audio files. Following are general prompting strategies when working with Gemini on multimodal prompts, that can help achieve better performance from Gemini:</p> <ol> <li>Craft clear and concise instructions.</li> <li>Add your video or any media first for single-media prompts.</li> <li>Add few-shot examples to the prompt to show the model how you want the task done and the expected output.</li> <li>Break down the task step-by-step.</li> <li>Specify the output format.</li> <li>Ask Gemini to include reasoning in its response along with decision or scores</li> <li>Use context caching for repeated queries.</li> </ol> <p>Specifically, when working with audio following may help:</p> <ol> <li>Ask Gemini to avoid summarizing for transcription.</li> <li>Add examples for effective speaker diarization.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/","title":"Multimodal Prompting with Gemini: Working with Images","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Run in Colab   Open in Colab Enterprise       Open in Vertex AI Workbench   View on GitHub  Author(s) Michael Chertushkin Reviewer(s) Rajesh Thallam, Skander Hannachi Last updated 2024-09-16 In\u00a0[\u00a0]: Copied! <pre>! pip install google-cloud-aiplatform --upgrade --quiet --user\n</pre> ! pip install google-cloud-aiplatform --upgrade --quiet --user In\u00a0[\u00a0]: Copied! <pre># Restart kernel after installs so that your environment can access the new packages\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> # Restart kernel after installs so that your environment can access the new packages import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") In\u00a0[\u00a0]: Copied! <pre>import vertexai\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"  # @param {type:\"string\"}\n\nvertexai.init(project=PROJECT_ID, location=REGION)\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n</pre> import vertexai  PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} REGION = \"us-central1\"  # @param {type:\"string\"}  vertexai.init(project=PROJECT_ID, location=REGION) print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") In\u00a0[\u00a0]: Copied! <pre>from vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,\n                                        HarmCategory, Image, Part)\n</pre> from vertexai.generative_models import (GenerativeModel, HarmBlockThreshold,                                         HarmCategory, Image, Part) In\u00a0[\u00a0]: Copied! <pre>import http.client\nimport textwrap\nimport typing\nimport urllib.request\n\nfrom google.cloud import storage\nfrom IPython import display\nfrom IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\ndef wrap(string, max_width=80):\n    return textwrap.fill(string, max_width)\n\n\ndef get_bytes_from_url(url: str) -&gt; bytes:\n    with urllib.request.urlopen(url) as response:\n        response = typing.cast(http.client.HTTPResponse, response)\n        bytes = response.read()\n    return bytes\n\n\ndef get_bytes_from_gcs(gcs_path: str):\n    bucket_name = gcs_path.split(\"/\")[2]\n    object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(object_prefix)\n    return blob.download_as_bytes()\n\n\ndef display_image(image_url: str, width: int = 300, height: int = 200):\n    if image_url.startswith(\"gs://\"):\n        image_bytes = get_bytes_from_gcs(image_url)\n    else:\n        image_bytes = get_bytes_from_url(image_url)\n    display.display(display.Image(data=image_bytes, width=width, height=height))\n\n\ndef display_video(video_url: str, width: int = 300, height: int = 200):\n    if video_url.startswith(\"gs://\"):\n        video_bytes = get_bytes_from_gcs(video_url)\n    else:\n        video_bytes = get_bytes_from_url(video_url)\n    display.display(\n        display.Video(\n            data=video_bytes,\n            width=width,\n            height=height,\n            embed=True,\n            mimetype=\"video/mp4\",\n        )\n    )\n\ndef display_audio(audio_url: str, width: int = 300, height: int = 200):\n    if audio_url.startswith(\"gs://\"):\n        audio_bytes = get_bytes_from_gcs(audio_url)\n    else:\n        audio_bytes = get_bytes_from_url(audio_url)\n    display.display(display.Audio(data=audio_bytes, embed=True))\n\n\ndef print_prompt(contents: list[str | Part]):\n    for content in contents:\n        if isinstance(content, Part):\n            if content.mime_type.startswith(\"image\"):\n                display_image(image_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"video\"):\n                display_video(video_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"audio\"):\n                display_audio(audio_url=content.file_data.file_uri)\n            else:\n                print(content)\n        else:\n            print(content)\n</pre> import http.client import textwrap import typing import urllib.request  from google.cloud import storage from IPython import display from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"   def wrap(string, max_width=80):     return textwrap.fill(string, max_width)   def get_bytes_from_url(url: str) -&gt; bytes:     with urllib.request.urlopen(url) as response:         response = typing.cast(http.client.HTTPResponse, response)         bytes = response.read()     return bytes   def get_bytes_from_gcs(gcs_path: str):     bucket_name = gcs_path.split(\"/\")[2]     object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])     storage_client = storage.Client()     bucket = storage_client.bucket(bucket_name)     blob = bucket.get_blob(object_prefix)     return blob.download_as_bytes()   def display_image(image_url: str, width: int = 300, height: int = 200):     if image_url.startswith(\"gs://\"):         image_bytes = get_bytes_from_gcs(image_url)     else:         image_bytes = get_bytes_from_url(image_url)     display.display(display.Image(data=image_bytes, width=width, height=height))   def display_video(video_url: str, width: int = 300, height: int = 200):     if video_url.startswith(\"gs://\"):         video_bytes = get_bytes_from_gcs(video_url)     else:         video_bytes = get_bytes_from_url(video_url)     display.display(         display.Video(             data=video_bytes,             width=width,             height=height,             embed=True,             mimetype=\"video/mp4\",         )     )  def display_audio(audio_url: str, width: int = 300, height: int = 200):     if audio_url.startswith(\"gs://\"):         audio_bytes = get_bytes_from_gcs(audio_url)     else:         audio_bytes = get_bytes_from_url(audio_url)     display.display(display.Audio(data=audio_bytes, embed=True))   def print_prompt(contents: list[str | Part]):     for content in contents:         if isinstance(content, Part):             if content.mime_type.startswith(\"image\"):                 display_image(image_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"video\"):                 display_video(video_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"audio\"):                 display_audio(audio_url=content.file_data.file_uri)             else:                 print(content)         else:             print(content) In\u00a0[\u00a0]: Copied! <pre># Gemini Config\nGENERATION_CONFIG = {\n    \"max_output_tokens\": 8192,\n    \"temperature\": 0.1,\n    \"top_p\": 0.95,\n}\n\nSAFETY_CONFIG = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n}\n\ngemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\")\nimage_path_prefix = (\n    \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/images\"\n)\n\n\ndef generate(\n    model,\n    contents,\n    safety_settings=SAFETY_CONFIG,\n    generation_config=GENERATION_CONFIG,\n    as_markdown=False,\n):\n    responses = model.generate_content(\n        contents=contents,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        stream=False,\n    )\n    if isinstance(responses, list):\n        for response in responses:\n            if as_markdown:\n                display.display(display.Markdown(response.text))\n            else:\n                print(wrap(response.text), end=\"\")\n    else:\n        if as_markdown:\n            display.display(display.Markdown(responses.text))\n        else:\n            print(wrap(responses.text), end=\"\")\n</pre> # Gemini Config GENERATION_CONFIG = {     \"max_output_tokens\": 8192,     \"temperature\": 0.1,     \"top_p\": 0.95, }  SAFETY_CONFIG = {     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH, }  gemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\") image_path_prefix = (     \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/images\" )   def generate(     model,     contents,     safety_settings=SAFETY_CONFIG,     generation_config=GENERATION_CONFIG,     as_markdown=False, ):     responses = model.generate_content(         contents=contents,         generation_config=generation_config,         safety_settings=safety_settings,         stream=False,     )     if isinstance(responses, list):         for response in responses:             if as_markdown:                 display.display(display.Markdown(response.text))             else:                 print(wrap(response.text), end=\"\")     else:         if as_markdown:             display.display(display.Markdown(responses.text))         else:             print(wrap(responses.text), end=\"\") In\u00a0[7]: Copied! <pre>image_path = f\"{image_path_prefix}/example_1.jpg\"\nimage_content = Part.from_uri(uri=image_path, mime_type=\"image/jpeg\")\ndisplay_image(image_path)\n</pre> image_path = f\"{image_path_prefix}/example_1.jpg\" image_content = Part.from_uri(uri=image_path, mime_type=\"image/jpeg\") display_image(image_path) In\u00a0[15]: Copied! <pre>prompt = \"Describe what is depicted on the image\"\ncontents = [image_content, prompt]\ngenerate(gemini, contents)\n</pre> prompt = \"Describe what is depicted on the image\" contents = [image_content, prompt] generate(gemini, contents) <pre>The image shows a group of men in suits standing in a hallway with a green and\nwhite checkered floor. The hallway appears to be in a school or institutional\nsetting, as there are lockers visible in the background.  In the foreground, a\nman in a gray suit is standing on a scale, adjusting the height measurement bar.\nNext to him, former President Barack Obama is standing with a slight smile,\nlooking towards the man on the scale. Several other men in suits are standing\nbehind them, some smiling and looking in the same direction.  There are mirrors\non the walls, reflecting the scene and adding depth to the image. The overall\natmosphere seems lighthearted and jovial.</pre> <p>As we see the model was not able to pick the dynamics of the situation (the humor with which president Obama is joking).</p> <p>Let's change the prompt asking Gemini to add more details and see what happens.</p> In\u00a0[17]: Copied! <pre>prompt = \"\"\"You are good at looking at pictures and uncovering the full story within a visual scene.\nYour task is to provide a rich and insightful description of the image.\n\nKey Points:\n- Decipher the visual puzzle.\n- Uncover hidden meanings.\n- Navigate complex dynamics.\n- Spotlight the heart of the matter.\n- Craft a captivating narrative.\n\nRemember:\n- The most compelling descriptions not only capture what's visible but also hint at what lies beneath the surface.\n- Try to recover hidden meaning from the scene, for example some hidden humor.\n\"\"\"\n\n# updated description with prompt changes\ncontents = [image_content, prompt]\ngenerate(gemini, contents)\n</pre> prompt = \"\"\"You are good at looking at pictures and uncovering the full story within a visual scene. Your task is to provide a rich and insightful description of the image.  Key Points: - Decipher the visual puzzle. - Uncover hidden meanings. - Navigate complex dynamics. - Spotlight the heart of the matter. - Craft a captivating narrative.  Remember: - The most compelling descriptions not only capture what's visible but also hint at what lies beneath the surface. - Try to recover hidden meaning from the scene, for example some hidden humor. \"\"\"  # updated description with prompt changes contents = [image_content, prompt] generate(gemini, contents) <pre>In a brightly lit hallway with a distinctive green and white checkered floor, a\ngroup of men in suits are gathered, creating a scene of both formality and\namusement. The focal point is a man in a gray suit standing on a scale,\nseemingly having his height measured by another man in a similar suit who is\nadjusting the measuring device.  Former President Barack Obama, dressed in a\ndark suit, stands nearby with a playful expression, his body language suggesting\nhe's about to step onto the scale as well. His stance is casual, with one leg\nextended, adding a touch of levity to the otherwise serious attire of the group.\nThe hallway is lined with lockers and mirrors, which reflect the scene and\nmultiply the number of people visible, creating a sense of depth and activity.\nThe mirrors also offer different perspectives of the individuals, adding to the\nvisual complexity of the image.  The overall atmosphere is lighthearted, with\nsmiles and laughter evident on the faces of some of the men. The presence of\nObama and the informal poses suggest a relaxed and jovial environment, perhaps a\nmoment of camaraderie amidst official duties. The scene captures a blend of\nprofessionalism and playfulness, hinting at the human side of these individuals\nin their formal attire.</pre> <p>After changing the prompt, the Gemini was able to capture humor and playful interaction.</p> <p>We followed a few tips when rewriting the prompt:</p> <ul> <li>Give a persona or a role to adopt (you are good at looking at pictures)</li> <li>Specify a mission or goal (your task is to provide rich description)</li> <li>Be specific about the instructions and structure them such as bullet points, prompt separators (markdown headers or XML tags)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>system_prompt = \"\"\"You are good at looking at pictures and uncovering the full story within a visual scene.\nYour task is to provide a rich and insightful description of the image.\n\nKey Points:\n- Decipher the visual puzzle.\n- Uncover hidden meanings.\n- Navigate complex dynamics.\n- Spotlight the heart of the matter.\n- Craft a captivating narrative.\n\nRemember:\n- The most compelling descriptions not only capture what's visible but also hint at what lies beneath the surface.\n- Try to recover hidden meaning from the scene, for example some hidden humor.\n\"\"\"\n</pre> system_prompt = \"\"\"You are good at looking at pictures and uncovering the full story within a visual scene. Your task is to provide a rich and insightful description of the image.  Key Points: - Decipher the visual puzzle. - Uncover hidden meanings. - Navigate complex dynamics. - Spotlight the heart of the matter. - Craft a captivating narrative.  Remember: - The most compelling descriptions not only capture what's visible but also hint at what lies beneath the surface. - Try to recover hidden meaning from the scene, for example some hidden humor. \"\"\" In\u00a0[20]: Copied! <pre>gemini_si = GenerativeModel(\n    model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt\n)\nsimple_prompt = \"Describe what is depicted on the image\"\n\ncontents = [image_content, simple_prompt]\ngenerate(gemini_si, contents)\n</pre> gemini_si = GenerativeModel(     model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt ) simple_prompt = \"Describe what is depicted on the image\"  contents = [image_content, simple_prompt] generate(gemini_si, contents) <pre>In a brightly lit hallway with a retro green and white checkered floor, a group\nof men in suits are gathered, seemingly in good spirits. The setting appears to\nbe a locker room or a similar institutional space, given the presence of lockers\nand mirrors along the walls.  The focal point of the image is a man in a gray\nsuit standing on a scale, adjusting the measuring bar. He holds a black folder,\nsuggesting he might be a staff member or someone involved in an official\ncapacity.  Former President Barack Obama is prominently featured, walking with a\nslight swagger and a smile, looking towards the man on the scale. His presence\ndraws attention and suggests that this is an event of some significance or\nperhaps a lighthearted moment during a formal occasion.  The other men in the\nhallway are also dressed in suits and ties, some smiling and engaged in the\nscene, while others are partially visible, adding depth to the composition. The\nmirrors on either side of the hallway create reflections that multiply the\nnumber of people and add to the sense of activity and camaraderie.  The overall\natmosphere is one of levity and good humor, with the men appearing relaxed and\nenjoying the moment. The presence of the scale and the act of measuring height\nadd a touch of the unexpected to the formal attire and setting, creating a\nmemorable and engaging image.</pre> <p>In this example we achieved the same level of description as Prompt #1, but with using system instruction (or system prompt):</p> <ul> <li>Add the persona, instructions, and mission into system instruction</li> <li>Used the simple prompt as before in Prompt #1</li> </ul> In\u00a0[8]: Copied! <pre>image_path = f\"{image_path_prefix}/city_street.png\"\nimage_content = Part.from_uri(uri=image_path, mime_type=\"image/png\")\ndisplay_image(image_path)\n</pre> image_path = f\"{image_path_prefix}/city_street.png\" image_content = Part.from_uri(uri=image_path, mime_type=\"image/png\") display_image(image_path) <p>Let's run with image first and then text in the prompt.</p> In\u00a0[9]: Copied! <pre>prompt_3 = (\n    \"Analyze the image and list the physical objects you can detect from the image.\"\n)\n\ncontents = [image_content, prompt_3]\ngenerate(gemini, contents)\n</pre> prompt_3 = (     \"Analyze the image and list the physical objects you can detect from the image.\" )  contents = [image_content, prompt_3] generate(gemini, contents) <pre>Here are the bounding box detections: ```json [   {\"box_2d\": [434, 606, 478,\n695], \"label\": \"sign\"},   {\"box_2d\": [286, 766, 330, 823], \"label\": \"sign\"},\n{\"box_2d\": [389, 893, 411, 921], \"label\": \"sign\"},   {\"box_2d\": [434, 873, 468,\n923], \"label\": \"sign\"},   {\"box_2d\": [565, 424, 730, 592], \"label\": \"car\"},\n{\"box_2d\": [580, 187, 674, 290], \"label\": \"car\"},   {\"box_2d\": [588, 278, 665,\n337], \"label\": \"car\"},   {\"box_2d\": [588, 660, 652, 712], \"label\": \"car\"},\n{\"box_2d\": [594, 698, 665, 742], \"label\": \"car\"},   {\"box_2d\": [548, 724, 708,\n923], \"label\": \"taxi\"},   {\"box_2d\": [594, 622, 643, 666], \"label\": \"car\"},\n{\"box_2d\": [604, 0, 771, 57], \"label\": \"car\"},   {\"box_2d\": [592, 387, 665,\n427], \"label\": \"motorcycle\"},   {\"box_2d\": [594, 893, 907, 1000], \"label\":\n\"car\"} ] ```</pre> <p>Let's run with text first and then image in the prompt.</p> In\u00a0[10]: Copied! <pre>contents = [prompt_3, image_content]\ngenerate(gemini, contents)\n</pre> contents = [prompt_3, image_content] generate(gemini, contents) <pre>Here's a breakdown of the physical objects I can detect in the image:  *\n**Vehicles:**     *   Cars (multiple, including a blue sedan, a yellow taxi, and\nseveral others)     *   Motorcycle *   **Buildings:**     *   Various buildings\n(different heights, styles, and materials like brick and glass) *   **Street\nInfrastructure:**     *   Traffic lights     *   Street signs (including \"Bus\nLane,\" \"One Way,\" and street name signs like \"E 83 St\")     *   Street\nlamps/light poles     *   Crosswalk markings     *   Road markings (lane\ndividers, arrows) *   **Vegetation:**     *   Trees *   **People:**     *\nPeople (visible on the sidewalks, some near outdoor seating) *   **Outdoor\nSeating:**     *   Tables and chairs (indicating outdoor dining areas) *\n**Sidewalks:**     *   Sidewalks on either side of the street. *   **Taxi\nsign:**     *   Taxi sign on top of the yellow taxi.</pre> <p>From this particular example, we see better response with image-first-then-text compared to text-first-then-image. Your mileage may vary depending on the use case.</p> In\u00a0[25]: Copied! <pre># Transformer architecture\n# Image source: https://aiml.com/compare-the-different-sequence-models-rnn-lstm-gru-and-transformers/\ndisplay_image(f\"{image_path_prefix}/example_5.png\")\n</pre> # Transformer architecture # Image source: https://aiml.com/compare-the-different-sequence-models-rnn-lstm-gru-and-transformers/ display_image(f\"{image_path_prefix}/example_5.png\") In\u00a0[26]: Copied! <pre>display_image(f\"{image_path_prefix}/example_2.png\")\n</pre> display_image(f\"{image_path_prefix}/example_2.png\") <p>To construct an effective prompt with examples, enumerate images such as <code>EXAMPLE# 1</code> in the below prompt.</p> In\u00a0[27]: Copied! <pre>prompt_4 = \"Analyze the model architecture in the image and count the number of blocks. Use following examples as reference when analyzing the image and returning the response.\"\nimage_content = Part.from_uri(\n    uri=f\"{image_path_prefix}/example_5.png\", mime_type=\"image/png\"\n)\n\ncontents = [\n    prompt_4,\n    \"EXAMPLE# 1\",\n    Part.from_uri(uri=f\"{image_path_prefix}/example_2.png\", mime_type=\"image/png\"),\n    '\"response\": {\"name\": \"RNN\", \"number_of_blocks\": 1}',\n    \"EXAMPLE# 2\",\n    Part.from_uri(uri=f\"{image_path_prefix}/example_3.png\", mime_type=\"image/png\"),\n    '\"response\": {\"name\": \"GRU\", \"number_of_blocks\": 3}',\n    \"EXAMPLE# 3\",\n    Part.from_uri(uri=f\"{image_path_prefix}/example_4.png\", mime_type=\"image/png\"),\n    '\"response\": {\"name\": \"LSTM\", \"number_of_blocks\": 5}',\n    \"ARCHITECTURE:\",\n    image_content,\n    '\"response\":',\n]\n\nprint_prompt(contents)\n</pre> prompt_4 = \"Analyze the model architecture in the image and count the number of blocks. Use following examples as reference when analyzing the image and returning the response.\" image_content = Part.from_uri(     uri=f\"{image_path_prefix}/example_5.png\", mime_type=\"image/png\" )  contents = [     prompt_4,     \"EXAMPLE# 1\",     Part.from_uri(uri=f\"{image_path_prefix}/example_2.png\", mime_type=\"image/png\"),     '\"response\": {\"name\": \"RNN\", \"number_of_blocks\": 1}',     \"EXAMPLE# 2\",     Part.from_uri(uri=f\"{image_path_prefix}/example_3.png\", mime_type=\"image/png\"),     '\"response\": {\"name\": \"GRU\", \"number_of_blocks\": 3}',     \"EXAMPLE# 3\",     Part.from_uri(uri=f\"{image_path_prefix}/example_4.png\", mime_type=\"image/png\"),     '\"response\": {\"name\": \"LSTM\", \"number_of_blocks\": 5}',     \"ARCHITECTURE:\",     image_content,     '\"response\":', ]  print_prompt(contents) <pre>Analyze the model architecture in the image and count the number of blocks. Use following examples as reference when analyzing the image and returning the response.\nEXAMPLE# 1\n</pre> <pre>\"response\": {\"name\": \"RNN\", \"number_of_blocks\": 1}\nEXAMPLE# 2\n</pre> <pre>\"response\": {\"name\": \"GRU\", \"number_of_blocks\": 3}\nEXAMPLE# 3\n</pre> <pre>\"response\": {\"name\": \"LSTM\", \"number_of_blocks\": 5}\nARCHITECTURE:\n</pre> <pre>\"response\":\n</pre> In\u00a0[28]: Copied! <pre>generate(\n    gemini,\n    contents,\n    generation_config=dict(**GENERATION_CONFIG, response_mime_type=\"application/json\"),\n)\n</pre> generate(     gemini,     contents,     generation_config=dict(**GENERATION_CONFIG, response_mime_type=\"application/json\"), ) <pre>{ \"response\": { \"name\": \"Transformers\", \"number_of_blocks\": 10 } }</pre> In\u00a0[29]: Copied! <pre>image_path = f\"{image_path_prefix}/order_1.png\"\nimage_content = Part.from_uri(uri=image_path, mime_type=\"image/png\")\ndisplay_image(image_path)\n</pre> image_path = f\"{image_path_prefix}/order_1.png\" image_content = Part.from_uri(uri=image_path, mime_type=\"image/png\") display_image(image_path) In\u00a0[30]: Copied! <pre>prompt_5 = \"Describe the image\"\n\ncontents = [image_content, prompt_5]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt_5 = \"Describe the image\"  contents = [image_content, prompt_5] generate(gemini, contents, as_markdown=True) <p>Here's a description of the image:</p> <p>Overall:</p> <p>The image is a purchase order document. It's formatted with clear sections for different types of information.</p> <p>Sections:</p> <ul> <li>Header: The title \"PURCHASE ORDER\" is prominently displayed at the top.</li> <li>Company Information:<ul> <li>Buyer: Includes the buyer's company name (ACME, INC), address, phone, fax, and email.</li> <li>Vendor: Includes the vendor's company name (LLM, INC), address, phone, fax, and email.</li> </ul> </li> <li>Shipping Information:<ul> <li>\"Ship To\" section with attention to Bert Simpson, company name (ACME, INC), address, phone, fax, and email.</li> </ul> </li> <li>Order Details:<ul> <li>Date and PO Number.</li> <li>Department and Requester.</li> <li>Payment Terms and Delivery Date.</li> </ul> </li> <li>Item List:<ul> <li>A table with columns for Item #, Description, Quantity (Qty), Unit Price, and Total.</li> <li>Two items are listed: \"15\" LED Monitor\" and \"Vertical Mounting Stand\".</li> </ul> </li> <li>Totals:<ul> <li>Tax Rate, Taxes, Shipping &amp; Handling, and Total Due are listed at the bottom of the item list.</li> </ul> </li> </ul> <p>Key Details:</p> <ul> <li>The purchase order is from ACME, INC to LLM, INC.</li> <li>The order includes a 15\" LED Monitor and Vertical Mounting Stands.</li> <li>The total due is $440.00.</li> <li>The delivery date is 9/25/2023.</li> <li>The date of the purchase order is 9/1/2023.</li> <li>The PO number is PO-2023-A123.</li> </ul> <p>As we see, the model successfully extracted main information, but it did not pick up all values from the table. Let's fix that with the same approach we used for task 1.</p> In\u00a0[31]: Copied! <pre>system_prompt_5 = \"\"\"You are an expert at document understanding and highly \ncapable of extracting all relevant information from bills, receipts, and \nvarious documents.\n\nYour task is to process the given document and identify all pertinent details \nsuch as the vendor/merchant name, date, transaction details (items, quantities, \nprices, etc.), total amount, payment method, and any other noteworthy information.\n\n# INSTRUCTIONS\n- Analyze Document Structure\n- Identify Key Sections\n- Extract Data:\n  - Vendor/Merchant Name\n  - Date\n  - Transaction Details:\n    - Items\n    - Quantities\n    - Prices\n    - Subtotals\n    - Total Amount\n    - Payment Method\n   - Other Information\n- Present the extracted information in a clear and structured format, using appropriate headings and labels.\n\n# CONSTRAINTS:\n- Handle Variations\n- Prioritize Accuracy\n- Handle Ambiguity\n- Maintain Confidentiality\"\"\"\n</pre> system_prompt_5 = \"\"\"You are an expert at document understanding and highly  capable of extracting all relevant information from bills, receipts, and  various documents.  Your task is to process the given document and identify all pertinent details  such as the vendor/merchant name, date, transaction details (items, quantities,  prices, etc.), total amount, payment method, and any other noteworthy information.  # INSTRUCTIONS - Analyze Document Structure - Identify Key Sections - Extract Data:   - Vendor/Merchant Name   - Date   - Transaction Details:     - Items     - Quantities     - Prices     - Subtotals     - Total Amount     - Payment Method    - Other Information - Present the extracted information in a clear and structured format, using appropriate headings and labels.  # CONSTRAINTS: - Handle Variations - Prioritize Accuracy - Handle Ambiguity - Maintain Confidentiality\"\"\" In\u00a0[33]: Copied! <pre>gemini_si = GenerativeModel(\n    model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt_5\n)\ncontents = [image_content, \"DOCUMENT:\"]\ngenerate(gemini_si, contents, as_markdown=True)\n</pre> gemini_si = GenerativeModel(     model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt_5 ) contents = [image_content, \"DOCUMENT:\"] generate(gemini_si, contents, as_markdown=True) <p>Here's the extracted information from the purchase order:</p> <p>Vendor Information:</p> <ul> <li>Vendor Name: LLM, INC</li> <li>Address: 123 Bison Street, Gecko City, ST 12345</li> <li>Phone: (123) 456-7890</li> <li>Fax: (123) 456 - 7800</li> <li>Email: langchain@llminc.com</li> </ul> <p>Customer Information:</p> <ul> <li>Company Name: ACME, INC</li> <li>Address: 456 Model Garden, Codey City, BY, 67890</li> <li>Phone: (222) - 345 - 6666</li> <li>Fax: (222) - 345 - 6000</li> <li>Email: buyer1@acmeinc.com</li> </ul> <p>Purchase Order Details:</p> <ul> <li>PO Number: PO-2023-A123</li> <li>Date: 9/1/2023</li> <li>Delivery Date: 9/25/2023</li> <li>Department: Engineering</li> <li>Requested By: Bert Simpson</li> <li>Payment Terms: Net 15 Days</li> <li>Ship To: Attn: BERT SIMPSON, ACME, INC, 456 Model Garden St, Codey City, BY, 67890</li> </ul> <p>Transaction Details:</p> Item # Description Qty Unit Price Total A233 15\" LED Monitor 1 $200.00 $200.00 B124 Vertical Mounting Stand 2 $100.00 $200.00 <p>Summary:</p> <ul> <li>Tax Rate: 10%</li> <li>Taxes: $40</li> <li>Shipping &amp; Handling: $0</li> <li>Total Due: $440.00</li> </ul> <p>As we see with the modification of the prompt and adding task in the system instruction, the model was able to extract the entities from the table in the way we wanted to do it.</p> In\u00a0[34]: Copied! <pre>image_path = f\"{image_path_prefix}/math_1.png\"\nimage_content = Part.from_uri(uri=image_path, mime_type=\"image/png\")\ndisplay_image(image_path)\n</pre> image_path = f\"{image_path_prefix}/math_1.png\" image_content = Part.from_uri(uri=image_path, mime_type=\"image/png\") display_image(image_path) In\u00a0[36]: Copied! <pre>prompt_6 = \"Solve the mathematical problem\"\n\ncontents = [image_content, prompt_6]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt_6 = \"Solve the mathematical problem\"  contents = [image_content, prompt_6] generate(gemini, contents, as_markdown=True) <p>To find the solutions of the equation $x^2 + 7x + 12 = 0$, we need to factor the quadratic expression. We are looking for two numbers that multiply to 12 and add up to 7. These numbers are 3 and 4. So, we can factor the quadratic as $(x + 3)(x + 4) = 0$. Now, we set each factor equal to zero and solve for x: $x + 3 = 0 \\Rightarrow x = -3$ $x + 4 = 0 \\Rightarrow x = -4$ The solutions are $x = -3$ and $x = -4$.</p> <p>Comparing our solutions to the given options, we see that option A matches our solutions.</p> <p>Final Answer: The final answer is $\\boxed{x = -3; x = -4}$</p> <p>Let's now switch to a different problem and update the prompt with better instructions.</p> In\u00a0[37]: Copied! <pre>image_path = f\"{image_path_prefix}/math_2.png\"\nimage_content = Part.from_uri(uri=image_path, mime_type=\"image/png\")\ndisplay_image(image_path)\n</pre> image_path = f\"{image_path_prefix}/math_2.png\" image_content = Part.from_uri(uri=image_path, mime_type=\"image/png\") display_image(image_path) In\u00a0[38]: Copied! <pre>prompt_6 = \"\"\"Please provide a detailed, step-by-step solution, clearly \noutlining the reasoning behind each step. Show all intermediate results and \ncalculations, ensuring a comprehensive and easy-to-follow explanation.\n\nIf the equation involves any specific mathematical concepts or techniques, \nplease identify and explain them as part of the solution.\n\nIf there are multiple solutions or special cases, please address them comprehensively.\n\nFinally, present the final answer or answers in a clear and concise manner. \"\"\"\n\ncontents = [image_content, prompt_6]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt_6 = \"\"\"Please provide a detailed, step-by-step solution, clearly  outlining the reasoning behind each step. Show all intermediate results and  calculations, ensuring a comprehensive and easy-to-follow explanation.  If the equation involves any specific mathematical concepts or techniques,  please identify and explain them as part of the solution.  If there are multiple solutions or special cases, please address them comprehensively.  Finally, present the final answer or answers in a clear and concise manner. \"\"\"  contents = [image_content, prompt_6] generate(gemini, contents, as_markdown=True) <p>Here's a step-by-step solution to the equation \u03c0^(x+1) = e:</p> <p>1. Take the natural logarithm of both sides:</p> <p>To solve for x, we need to get rid of the exponent. Taking the natural logarithm (ln) of both sides of the equation will help us do this.  The natural logarithm is the logarithm to the base e.</p> <p>ln(\u03c0^(x+1)) = ln(e)</p> <p>2. Apply the power rule of logarithms:</p> <p>The power rule of logarithms states that ln(a^b) = b * ln(a). Applying this rule to the left side of the equation, we get:</p> <p>(x + 1) * ln(\u03c0) = ln(e)</p> <p>3. Simplify ln(e):</p> <p>The natural logarithm of e is always 1, because e raised to the power of 1 is e.</p> <p>(x + 1) * ln(\u03c0) = 1</p> <p>4. Isolate (x + 1):</p> <p>Divide both sides of the equation by ln(\u03c0):</p> <p>x + 1 = 1 / ln(\u03c0)</p> <p>5. Solve for x:</p> <p>Subtract 1 from both sides of the equation:</p> <p>x = (1 / ln(\u03c0)) - 1</p> <p>6. Simplify (optional):</p> <p>We can combine the terms on the right side into a single fraction:</p> <p>x = (1 - ln(\u03c0)) / ln(\u03c0)</p> <p>Final Answer:</p> <p>The solution to the equation \u03c0^(x+1) = e is:</p> <p>x = (1 / ln(\u03c0)) - 1  or  x = (1 - ln(\u03c0)) / ln(\u03c0)</p> <p>Here we ask Gemini to use step-by-step reasoning and ask it to output intermediate steps also. This allows us to be more confident in the output answer. Asking the model to return reasoning and intermediate steps helps LLM to arrive at the answer better.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#multimodal-prompting-with-gemini-working-with-images","title":"Multimodal Prompting with Gemini: Working with Images\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#overview","title":"Overview\u00b6","text":"<p>Gemini 2.0 models supports adding image, audio, video, and PDF files in text or chat prompts for a text, image or code response. Gemini 2.0 Flash supports up to 1 Million input tokens with up to 3600 images per prompt. You can add images to Gemini requests to perform image understanding tasks such as image captioning, visual question and answering, comparing images, object or text detection and more.</p> <p>In this notebook we cover prompting recipes and strategies for working with Gemini on image files and show examples on the way. This notebook is organized as follows:</p> <ul> <li>Image Understanding</li> <li>Using system instruction</li> <li>Structuring prompt with images</li> <li>Adding few-shot examples the image prompt</li> <li>Document understanding</li> <li>Math understanding</li> </ul>  This notebook does not cover image generation task. Imagen on Vertex AI lets you quickly generate high-quality images from simple text descriptions. Refer to this notebook for image generation."},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#getting-started","title":"Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.</li> <li>Make sure that billing is enabled for your project.</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API.</li> <li>Enable the Cloud Storage API.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, including the optional section, you will need to have the Owner role for your project.</p> <p>If you want to skip the optional section, you need at least the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#install-vertex-ai-sdk-for-python-and-other-dependencies-if-needed","title":"Install Vertex AI SDK for Python and other dependencies (If Needed)\u00b6","text":"<p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.## Install Vertex AI SDK for Python and other dependencies (If Needed)</p> <p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and Initialize Vertex AI SDK\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>REGION</code> unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#define-utility-functions","title":"Define Utility functions\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#initialize-gemini","title":"Initialize Gemini\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-1-image-understanding","title":"Prompt #1. Image Understanding\u00b6","text":"<p>This task requires the input to be presented in two different modalities: text and image. The example of the API call is below, however this is non-optimal prompt and we can make it better.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-2-image-understanding-using-system-instruction","title":"Prompt #2. Image Understanding: Using System instruction\u00b6","text":"<p>System Instruction (SI) is an effective way to steer Gemini's behavior and shape how the model responds to your prompt. SI can be used to describe model behavior such as persona, goal, tasks to perform, output format / tone / style, any constraints etc.</p> <p>SI behaves more \"sticky\" (or consistent) during multi-turn behavior. For example, if you want to achieve a behavior that the model will consistently follow, then system instruction is the best way to put this instruction.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-3-image-understanding-structuring-and-order-of-images-and-texts","title":"Prompt #3. Image Understanding: Structuring and order of images and texts\u00b6","text":"<p>Gemini works well with images and text in any order.  For single-image prompts, starting with the image and then text may improve performance. If your prompt needs images and text mixed together, use the order that feels most natural.</p> <p>That being said, this isn't a hard and fast rule, and your results may vary.  To illustrate, we've included examples of both image-first and text-first prompts below, and in this case there's no significant difference between the two.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-4-image-understanding-adding-few-shot-examples","title":"Prompt #4. Image Understanding: Adding few-shot examples\u00b6","text":"<p>You can add multiple images in the prompt that Gemini can use as examples to understand the output you want. Adding these few-shot examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. Let's examine how to use few-shot examples for the image understanding task.</p> <p>This prompt uses Gemini to count number of blocks in a image of Transformer architecture. To help the model, we add 3 images of different architectures - RNN, GRU and LSTM.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-5-document-understanding","title":"Prompt #5. Document understanding\u00b6","text":"<p>Let's examine the task of document understanding using Gemini.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#prompt-6-math-understanding","title":"Prompt #6. Math Understanding\u00b6","text":"<p>In this prompt, let's examine Gemini's capabilities of math understanding by uploading a screenshot of a math problem and solve with Gemini.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_image/#conclusion","title":"Conclusion\u00b6","text":"<p>This demonstrated various examples of working with Gemini using images. Following are general prompting strategies when working with Gemini on multimodal prompts, that can help achieve better performance from Gemini:</p> <ol> <li>Craft clear and concise instructions.</li> <li>Add your image first for single-image prompts.</li> <li>Add few-shot examples to the prompt to show the model how you want the task done and the expected output.</li> <li>Break down the task step-by-step.</li> <li>Specify the output format.</li> <li>Ask Gemini to include reasoning in its response along with decision or scores</li> <li>Use context caching for repeated queries.</li> </ol> <p>Specifically, when working with images following may help:</p> <ol> <li>Enumerate when prompt has multiple images.</li> <li>Use a single image for optimal text detection.</li> <li>You can detect objects in images with bounding boxes.</li> <li>Guiding models\u2019 attention by adding hints.</li> <li>Ask for detailed analysis for optimizing output.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/","title":"Multimodal Prompting with Gemini: Working with Videos","text":"In\u00a0[1]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Run in Colab   Open in Colab Enterprise       Open in Vertex AI Workbench   View on GitHub  Author(s) Michael Chertushkin Reviewer(s) Rajesh Thallam, Skander Hannachi Last updated 2024-09-16 In\u00a0[\u00a0]: Copied! <pre>! pip install google-cloud-aiplatform --upgrade --quiet --user\n</pre> ! pip install google-cloud-aiplatform --upgrade --quiet --user In\u00a0[\u00a0]: Copied! <pre># Restart kernel after installs so that your environment can access the new packages\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> # Restart kernel after installs so that your environment can access the new packages import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) In\u00a0[1]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") In\u00a0[\u00a0]: Copied! <pre>import vertexai\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"  # @param {type:\"string\"}\n\nvertexai.init(project=PROJECT_ID, location=REGION)\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n</pre> import vertexai  PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} REGION = \"us-central1\"  # @param {type:\"string\"}  vertexai.init(project=PROJECT_ID, location=REGION) print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") In\u00a0[\u00a0]: Copied! <pre>from vertexai.generative_models import (GenerationConfig, GenerativeModel,\n                                        HarmBlockThreshold, HarmCategory, Part)\n</pre> from vertexai.generative_models import (GenerationConfig, GenerativeModel,                                         HarmBlockThreshold, HarmCategory, Part) In\u00a0[\u00a0]: Copied! <pre>import http.client\nimport textwrap\nimport typing\nimport urllib.request\n\nfrom google.cloud import storage\nfrom IPython import display\nfrom IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\ndef wrap(string, max_width=80):\n    return textwrap.fill(string, max_width)\n\n\ndef get_bytes_from_url(url: str) -&gt; bytes:\n    with urllib.request.urlopen(url) as response:\n        response = typing.cast(http.client.HTTPResponse, response)\n        bytes = response.read()\n    return bytes\n\n\ndef get_bytes_from_gcs(gcs_path: str):\n    bucket_name = gcs_path.split(\"/\")[2]\n    object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.get_blob(object_prefix)\n    return blob.download_as_bytes()\n\n\ndef display_image(image_url: str, width: int = 300, height: int = 200):\n    if image_url.startswith(\"gs://\"):\n        image_bytes = get_bytes_from_gcs(image_url)\n    else:\n        image_bytes = get_bytes_from_url(image_url)\n    display.display(display.Image(data=image_bytes, width=width, height=height))\n\n\ndef display_video(video_url: str, width: int = 300, height: int = 200):\n    if video_url.startswith(\"gs://\"):\n        video_bytes = get_bytes_from_gcs(video_url)\n    else:\n        video_bytes = get_bytes_from_url(video_url)\n    display.display(\n        display.Video(\n            data=video_bytes,\n            width=width,\n            height=height,\n            embed=True,\n            mimetype=\"video/mp4\",\n        )\n    )\n\ndef display_audio(audio_url: str, width: int = 300, height: int = 200):\n    if audio_url.startswith(\"gs://\"):\n        audio_bytes = get_bytes_from_gcs(audio_url)\n    else:\n        audio_bytes = get_bytes_from_url(audio_url)\n    display.display(display.Audio(data=audio_bytes, embed=True))\n\n\ndef print_prompt(contents: list[str | Part]):\n    for content in contents:\n        if isinstance(content, Part):\n            if content.mime_type.startswith(\"image\"):\n                display_image(image_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"video\"):\n                display_video(video_url=content.file_data.file_uri)\n            elif content.mime_type.startswith(\"audio\"):\n                display_audio(audio_url=content.file_data.file_uri)\n            else:\n                print(content)\n        else:\n            print(content)\n</pre> import http.client import textwrap import typing import urllib.request  from google.cloud import storage from IPython import display from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"   def wrap(string, max_width=80):     return textwrap.fill(string, max_width)   def get_bytes_from_url(url: str) -&gt; bytes:     with urllib.request.urlopen(url) as response:         response = typing.cast(http.client.HTTPResponse, response)         bytes = response.read()     return bytes   def get_bytes_from_gcs(gcs_path: str):     bucket_name = gcs_path.split(\"/\")[2]     object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])     storage_client = storage.Client()     bucket = storage_client.bucket(bucket_name)     blob = bucket.get_blob(object_prefix)     return blob.download_as_bytes()   def display_image(image_url: str, width: int = 300, height: int = 200):     if image_url.startswith(\"gs://\"):         image_bytes = get_bytes_from_gcs(image_url)     else:         image_bytes = get_bytes_from_url(image_url)     display.display(display.Image(data=image_bytes, width=width, height=height))   def display_video(video_url: str, width: int = 300, height: int = 200):     if video_url.startswith(\"gs://\"):         video_bytes = get_bytes_from_gcs(video_url)     else:         video_bytes = get_bytes_from_url(video_url)     display.display(         display.Video(             data=video_bytes,             width=width,             height=height,             embed=True,             mimetype=\"video/mp4\",         )     )  def display_audio(audio_url: str, width: int = 300, height: int = 200):     if audio_url.startswith(\"gs://\"):         audio_bytes = get_bytes_from_gcs(audio_url)     else:         audio_bytes = get_bytes_from_url(audio_url)     display.display(display.Audio(data=audio_bytes, embed=True))   def print_prompt(contents: list[str | Part]):     for content in contents:         if isinstance(content, Part):             if content.mime_type.startswith(\"image\"):                 display_image(image_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"video\"):                 display_video(video_url=content.file_data.file_uri)             elif content.mime_type.startswith(\"audio\"):                 display_audio(audio_url=content.file_data.file_uri)             else:                 print(content)         else:             print(content) In\u00a0[\u00a0]: Copied! <pre># Gemini Config\nGENERATION_CONFIG = {\n    \"max_output_tokens\": 8192,\n    \"temperature\": 0.1,\n    \"top_p\": 0.95,\n}\n\nSAFETY_CONFIG = {\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n}\n\ngemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\")\nvideos_path_prefix = (\n    \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos\"\n)\n\n\ndef generate(\n    model,\n    contents,\n    safety_settings=SAFETY_CONFIG,\n    generation_config=GENERATION_CONFIG,\n    as_markdown=False,\n):\n    responses = model.generate_content(\n        contents=contents,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        stream=False,\n    )\n    if isinstance(responses, list):\n        for response in responses:\n            if as_markdown:\n                display.display(display.Markdown(response.text))\n            else:\n                print(wrap(response.text), end=\"\")\n    else:\n        if as_markdown:\n            display.display(display.Markdown(responses.text))\n        else:\n            print(wrap(responses.text), end=\"\")\n</pre> # Gemini Config GENERATION_CONFIG = {     \"max_output_tokens\": 8192,     \"temperature\": 0.1,     \"top_p\": 0.95, }  SAFETY_CONFIG = {     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH, }  gemini = GenerativeModel(model_name=\"gemini-2.0-flash-001\") videos_path_prefix = (     \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos\" )   def generate(     model,     contents,     safety_settings=SAFETY_CONFIG,     generation_config=GENERATION_CONFIG,     as_markdown=False, ):     responses = model.generate_content(         contents=contents,         generation_config=generation_config,         safety_settings=safety_settings,         stream=False,     )     if isinstance(responses, list):         for response in responses:             if as_markdown:                 display.display(display.Markdown(response.text))             else:                 print(wrap(response.text), end=\"\")     else:         if as_markdown:             display.display(display.Markdown(responses.text))         else:             print(wrap(responses.text), end=\"\") In\u00a0[\u00a0]: Copied! <pre>display_video(\n    video_url=\"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos/video_1.mp4\"\n)\n</pre> display_video(     video_url=\"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos/video_1.mp4\" ) In\u00a0[5]: Copied! <pre>video_path = f\"{videos_path_prefix}/video_1.mp4\"\nvideo_content = Part.from_uri(uri=video_path, mime_type=\"video/mp4\")\nprompt = \"\"\"Provide a description of the video. The description should also \ncontain anything important which people say in the video.\"\"\"\n\ncontents = [video_content, prompt]\n# print_prompt(contents)\n</pre> video_path = f\"{videos_path_prefix}/video_1.mp4\" video_content = Part.from_uri(uri=video_path, mime_type=\"video/mp4\") prompt = \"\"\"Provide a description of the video. The description should also  contain anything important which people say in the video.\"\"\"  contents = [video_content, prompt] # print_prompt(contents) In\u00a0[6]: Copied! <pre>generate(gemini, contents)\n</pre> generate(gemini, contents) <pre>Here is a description of the video:  The video shows a person tossing a pink\ncollapsible cup in the air and catching it. The background is a white curtain.\nThe person's arm and hand are visible. The cup is the main focus of the video.\nThe video is shot in a bright, minimalist style. There is no audio in the video.</pre> <p>As we see the model correctly picked what happens there, but it did not provide much details. Let's modify the prompt.</p> In\u00a0[7]: Copied! <pre>prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \nand produce the detailed description about what happens on the video.\n\nKey Points:\n- Use timestamps (in MM:SS format) to output key events from the video.\n- Add information about what happens at each timestamp.\n- Add information about entities in the video and capture the relationship between them.\n- Highlight the central theme or focus of the video.\n\nRemember:\n- Try to recover hidden meaning from the scene. For example, some hidden humor \n  or some hidden context.\n\"\"\"\n\ncontents = [video_content, prompt]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video  and produce the detailed description about what happens on the video.  Key Points: - Use timestamps (in MM:SS format) to output key events from the video. - Add information about what happens at each timestamp. - Add information about entities in the video and capture the relationship between them. - Highlight the central theme or focus of the video.  Remember: - Try to recover hidden meaning from the scene. For example, some hidden humor    or some hidden context. \"\"\"  contents = [video_content, prompt] generate(gemini, contents, as_markdown=True) <p>Here's a detailed analysis of the video:</p> <ul> <li>00:00 A hand throws a pink collapsible cup into the air against a white curtain backdrop.</li> <li>00:01 The hand catches the cup.</li> <li>00:02 The hand throws the cup into the air again, this time in its collapsed form.</li> <li>00:03 The hand catches the cup in its expanded form.</li> <li>00:04 The hand shakes the cup.</li> <li>00:05 The hand holds the cup still.</li> <li>00:06 The hand moves the cup around.</li> <li>00:07 The hand throws the cup into the air again.</li> <li>00:08 The hand holds the cup still.</li> <li>00:09 The hand shakes the cup.</li> <li>00:10 The hand holds the cup still.</li> </ul> <p>The central theme of the video is showcasing a pink collapsible cup. The hand interacts with the cup by throwing it in the air, catching it, shaking it, and holding it still. The white curtain backdrop provides a clean and simple background, drawing attention to the cup and the hand's interaction with it.</p> <p>The response with the updated prompt captures much more details. Although this prompt is rather generic and can be used for other videos, let's add specifics to the prompt. For example, if we want to capture at which time certain event happened.</p> In\u00a0[8]: Copied! <pre>prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \nand produce the detailed description about what happens on the video.\n\nKey Points:\n- Use timestamps (in MM:SS format) to output key events from the video.\n- Add information about what happens at each timestamp.\n- Add information about entities in the video and capture the relationship between them.\n- Highlight the central theme or focus of the video.\n\nRemember:\n- Try to recover hidden meaning from the scene. For example, some hidden humor \n  or some hidden context.\n\nAt which moment the cup was thrown for the second time?\n\"\"\"\n\ncontents = [video_content, prompt]\ngenerate(gemini, contents, as_markdown=True)\n</pre> prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video  and produce the detailed description about what happens on the video.  Key Points: - Use timestamps (in MM:SS format) to output key events from the video. - Add information about what happens at each timestamp. - Add information about entities in the video and capture the relationship between them. - Highlight the central theme or focus of the video.  Remember: - Try to recover hidden meaning from the scene. For example, some hidden humor    or some hidden context.  At which moment the cup was thrown for the second time? \"\"\"  contents = [video_content, prompt] generate(gemini, contents, as_markdown=True) <p>Here's a detailed analysis of the video:</p> <ul> <li>00:00 A hand throws a pink collapsible cup into the air against a white curtain backdrop.</li> <li>00:01 The hand catches the cup.</li> <li>00:02 The hand throws the cup again.</li> <li>00:03 The hand catches the cup again.</li> <li>00:04 The hand shakes the cup.</li> <li>00:07 The hand throws the cup again.</li> <li>00:08 The hand catches the cup again.</li> <li>00:09 The hand shakes the cup.</li> </ul> <p>The central theme of the video is a person playing with a pink collapsible cup, repeatedly throwing it into the air and catching it.</p> In\u00a0[9]: Copied! <pre>system_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \nand produce the detailed description about what happens on the video.\n\nKey Points:\n- Use timestamps (in MM:SS format) to output key events from the video.\n- Add information about what happens at each timestamp.\n- Add information about entities in the video and capture the relationship between them.\n- Highlight the central theme or focus of the video.\n\nRemember:\n- Try to recover hidden meaning from the scene. For example, some hidden humor \n  or some hidden context.\n\"\"\"\n\nprompt = \"At which moment the cup was thrown for the second time?\"\n</pre> system_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video  and produce the detailed description about what happens on the video.  Key Points: - Use timestamps (in MM:SS format) to output key events from the video. - Add information about what happens at each timestamp. - Add information about entities in the video and capture the relationship between them. - Highlight the central theme or focus of the video.  Remember: - Try to recover hidden meaning from the scene. For example, some hidden humor    or some hidden context. \"\"\"  prompt = \"At which moment the cup was thrown for the second time?\" In\u00a0[10]: Copied! <pre>gemini_si = GenerativeModel(\n    model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt\n)\n\ncontents = [video_content, prompt]\ngenerate(gemini_si, contents, as_markdown=True)\n</pre> gemini_si = GenerativeModel(     model_name=\"gemini-2.0-flash-001\", system_instruction=system_prompt )  contents = [video_content, prompt] generate(gemini_si, contents, as_markdown=True) <p>[00:07] The cup was thrown for the second time.</p> In\u00a0[11]: Copied! <pre>step_by_step_prompt = \"\"\"Describe the video. Analyze the video step-by-step. \nOutput all times when the cup is thrown with timestamps. \nAfter that output the timestamp, when the cup is thrown for the second time.\n\"\"\"\n\ncontents = [video_content, step_by_step_prompt]\ngenerate(gemini_si, contents, as_markdown=True)\n</pre> step_by_step_prompt = \"\"\"Describe the video. Analyze the video step-by-step.  Output all times when the cup is thrown with timestamps.  After that output the timestamp, when the cup is thrown for the second time. \"\"\"  contents = [video_content, step_by_step_prompt] generate(gemini_si, contents, as_markdown=True) <p>Here's a breakdown of the video:</p> <p>The central theme of the video revolves around a person playfully tossing and catching a pink, collapsible cup. The background is a simple white curtain, keeping the focus entirely on the cup and the hand interacting with it.</p> <p>Here's a step-by-step analysis with timestamps:</p> <ul> <li>00:00 The person throws the pink cup into the air.</li> <li>00:01 The person catches the pink cup.</li> <li>00:02 The person throws the pink cup into the air again.</li> <li>00:03 The person catches the pink cup.</li> <li>00:07 The person throws the pink cup into the air for the third time.</li> </ul> <p>The cup is thrown for the second time at 00:02.</p> In\u00a0[13]: Copied! <pre>response_schema = {\n    \"type\": \"ARRAY\",\n    \"items\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"harmfulness_reasoning\": {\n                \"type\": \"STRING\",\n                \"description\": \"Step-by-step detailed reasoning about how harmful is the video\",\n            },\n            \"harmfulness_score\": {\n                \"type\": \"INTEGER\",\n                \"description\": \"Number between 0 and 5 indicating how harmful is the video\",\n            },\n        },\n        \"required\": [\"harmfulness_reasoning\", \"harmfulness_score\"],\n    },\n}\n</pre> response_schema = {     \"type\": \"ARRAY\",     \"items\": {         \"type\": \"OBJECT\",         \"properties\": {             \"harmfulness_reasoning\": {                 \"type\": \"STRING\",                 \"description\": \"Step-by-step detailed reasoning about how harmful is the video\",             },             \"harmfulness_score\": {                 \"type\": \"INTEGER\",                 \"description\": \"Number between 0 and 5 indicating how harmful is the video\",             },         },         \"required\": [\"harmfulness_reasoning\", \"harmfulness_score\"],     }, } In\u00a0[14]: Copied! <pre>structured_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \nand produce a harmfulness score - how harmful this video can be for kids.\"\"\"\n\ncontents = [video_content, structured_prompt]\n\ngenerate(\n    gemini,\n    contents,\n    generation_config=GenerationConfig(\n        response_mime_type=\"application/json\", response_schema=response_schema\n    ),\n)\n</pre> structured_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video  and produce a harmfulness score - how harmful this video can be for kids.\"\"\"  contents = [video_content, structured_prompt]  generate(     gemini,     contents,     generation_config=GenerationConfig(         response_mime_type=\"application/json\", response_schema=response_schema     ), ) <pre>[   {     \"harmfulness_reasoning\": \"The video features a hand tossing and\ncatching a pink cup. There is no indication of any harmful or dangerous content,\nnor does it contain any themes or visuals that would be considered inappropriate\nfor children. The scene is simple and does not present any risk of promoting\nnegative behaviors.\",     \"harmfulness_score\": 0   } ]</pre> <p>The model returned the correct score for the video by asking the model to output \"reasoning\" along with the score. Adding \"reasoning\" field before the \"score\" gives a consistent and correct score. The intuition is  that LLM can generate \"reasoning\" first and rely on the thoughts to properly produce the score.</p> In\u00a0[15]: Copied! <pre>long_video_path = f\"{videos_path_prefix}/long_video_1.mp4\"\nlong_video_content = Part.from_uri(uri=long_video_path, mime_type=\"video/mp4\")\n\nprompt = \"\"\"Describe what happens in the beginning, in the middle and in the \nend of the video. Also, list the name of the main character and any problems \nthey face.\"\"\"\n\ncontents = [long_video_content, prompt]\n# print_prompt(contents)\n</pre> long_video_path = f\"{videos_path_prefix}/long_video_1.mp4\" long_video_content = Part.from_uri(uri=long_video_path, mime_type=\"video/mp4\")  prompt = \"\"\"Describe what happens in the beginning, in the middle and in the  end of the video. Also, list the name of the main character and any problems  they face.\"\"\"  contents = [long_video_content, prompt] # print_prompt(contents) In\u00a0[16]: Copied! <pre># Time the call without context caching\nfrom timeit import default_timer as timer\n\nstart = timer()\ngenerate(gemini, contents)\nend = timer()\n\nprint(f\"\\nTime elapsed: {end - start} seconds\")\n</pre> # Time the call without context caching from timeit import default_timer as timer  start = timer() generate(gemini, contents) end = timer()  print(f\"\\nTime elapsed: {end - start} seconds\") <pre>Here's a breakdown of the video:  **Beginning (0:00 - 1:25):**  *   The video\nopens with the title card for \"Sherlock Jr.\" starring Buster Keaton, presented\nby Joseph M. Schenck. *   Credits for the story, photography, art direction, and\nelectrician are shown. *   Copyright information for 1924 is displayed. *   A\nproverb appears: \"Don't try to do two things at once and expect to do justice to\nboth.\" *   The opening narration sets the scene: a boy working as a moving\npicture operator in a small-town theater is also studying to be a detective.\n**Middle (1:26 - 38:59):**  *   The video shows the main character, a young man\nwith a mustache, reading a book titled \"How-To-Be-A-Detective\" in an empty\ntheater. *   His boss tells him to clean the theater instead of reading\ndetective books. *   The young man is shown sweeping the theater and then\nwalking to a confectionery store. *   He sees a girl in the store and wants to\nbuy her chocolates, but he doesn't have enough money. *   The girl's father\nhires a man to help him. *   The girl is seen with a dog, and a man steals her\nring. *   The girl's father is seen with the man he hired, and he discovers that\nhis watch has been stolen. *   The young man is called to investigate the theft.\n*   The young man searches everyone, but the watch is not found. *   The young\nman is told to leave the house and never come back. *   The young man returns to\nthe theater and starts the movie. *   The movie is \"Hearts and Pearls\" and the\nyoung man falls asleep. *   The young man dreams that he is in the movie. *\nThe young man is seen in the movie, and he is trying to steal the pearls. *\nThe young man is seen in the movie, and he is trying to escape. *   The young\nman is seen in the movie, and he is being chased by the police. *   The young\nman is seen in the movie, and he is trying to get away on a motorcycle. *   The\nyoung man is seen in the movie, and he is trying to get away in a car. *   The\nyoung man is seen in the movie, and he is trying to get away in a boat. *   The\nyoung man is seen in the movie, and he is swimming away.  **End (39:00 -\n44:06):**  *   The young man wakes up and is back in the projection booth. *\nThe girl comes to the projection booth and tells him that her father made a\nmistake. *   The girl shows the young man the pawn ticket for the watch. *   The\nyoung man is seen in the movie, and he is now a detective. *   The detective is\nseen with his assistant, Gillette. *   The detective is seen in the movie, and\nhe is trying to solve the case. *   The detective is seen in the movie, and he\nis trying to catch the thief. *   The detective is seen in the movie, and he is\ntrying to save the girl. *   The detective is seen in the movie, and he is\nreunited with the girl. *   The movie ends.  **Main Character and Problems:**  *\n**Main Character:** The young man working as a movie projectionist (played by\nBuster Keaton). He is also studying to be a detective. *   **Problems:**     *\nHe is trying to balance his job with his dream of becoming a detective.     *\nHe is not taken seriously as a detective.     *   He is accused of stealing a\nwatch.     *   He is kicked out of the girl's house.     *   He is trying to\nsave the girl from the thief.\nTime elapsed: 40.79677771499996 seconds\n</pre> In\u00a0[17]: Copied! <pre>import datetime\n\nfrom vertexai.preview import caching\nfrom vertexai.preview.generative_models import GenerativeModel\n\ncached_content = caching.CachedContent.create(\n    model_name=\"gemini-2.0-flash-001\",\n    contents=[long_video_content],\n    ttl=datetime.timedelta(hours=1),\n    display_name=\"long video cache\",\n)\n\nmodel_cached = GenerativeModel.from_cached_content(cached_content=cached_content)\n</pre> import datetime  from vertexai.preview import caching from vertexai.preview.generative_models import GenerativeModel  cached_content = caching.CachedContent.create(     model_name=\"gemini-2.0-flash-001\",     contents=[long_video_content],     ttl=datetime.timedelta(hours=1),     display_name=\"long video cache\", )  model_cached = GenerativeModel.from_cached_content(cached_content=cached_content) In\u00a0[18]: Copied! <pre># Call with context caching\nstart = timer()\nresponses = model_cached.generate_content(\n    prompt,\n    generation_config=GENERATION_CONFIG,\n    safety_settings=SAFETY_CONFIG,\n    stream=False,\n)\nend = timer()\n\nprint(wrap(responses.text), end=\"\")\n\nprint(f\"\\nTime elapsed: {end - start} seconds\")\n</pre> # Call with context caching start = timer() responses = model_cached.generate_content(     prompt,     generation_config=GENERATION_CONFIG,     safety_settings=SAFETY_CONFIG,     stream=False, ) end = timer()  print(wrap(responses.text), end=\"\")  print(f\"\\nTime elapsed: {end - start} seconds\") <pre>Here's a breakdown of the video:  **Beginning (0:00-1:25):**  *   The video\nstarts with the title card for the film \"Sherlock Jr.\" starring Buster Keaton,\npresented by Joseph M. Schenck. *   Credits are shown, including director,\nwriters, photography, art director, and electrician. *   Copyright information\nis displayed, indicating the film was copyrighted in 1924. *   A proverb is\npresented: \"Don't try to do two things at once and expect to do justice to\nboth.\" *   The introduction explains that the story is about a boy who tried to\ndo two things at once: work as a moving picture operator and study to be a\ndetective.  **Middle (1:26-38:59):**  *   The scene shifts to a movie theater\nwhere a young man (Buster Keaton) is reading a book titled \"How-To-Be-A-\nDetective.\" *   His boss tells him to clean the theater instead of reading. *\nThe young man is shown working at the theater, but he is distracted by his\ndetective studies. *   He tries to buy a box of chocolates for a girl he likes,\nbut he doesn't have enough money. *   The girl is seen with another man, who\nbuys her a more expensive box of chocolates. *   The young man is called to a\nhouse where a crime has been committed. *   He tries to investigate, but he is\nclumsy and makes mistakes. *   He is accused of stealing a watch and is kicked\nout of the house. *   He returns to the theater and falls asleep in the\nprojection booth. *   While asleep, he dreams that he enters the movie screen\nand becomes the detective in the film. *   He interacts with the characters and\ntries to solve the crime, but he is clumsy and makes mistakes. *   He is chased\nby the villains and ends up in a series of dangerous situations. *   He is\nchased by the police and ends up falling into a river.  **End (38:59-44:06):**\n*   The young man wakes up from his dream and realizes that he is late for work.\n*   He rushes to the projection booth and starts the movie. *   He sees the girl\nhe likes in the audience and realizes that she is in danger. *   He enters the\nmovie screen and saves her from the villains. *   The film ends with the young\nman and the girl together.  **Main Character and Problems:**  *   **Main\nCharacter:** The young man, played by Buster Keaton, who works as a movie\nprojectionist and aspires to be a detective. *   **Problems:**     *   Balancing\nhis job with his detective studies.     *   Not having enough money to impress\nthe girl he likes.     *   Being accused of stealing a watch.     *   Being\nclumsy and making mistakes as a detective.     *   Being unable to save the girl\nhe likes in the real world.\nTime elapsed: 29.962379157999976 seconds\n</pre> <p>As we see the result with context caching was relatively faster than without context caching. Not only that, the cost of the request is lower as we did not need to send the video again during the prompt for analysis.</p> <p>Context caching therefore is ideal for the repeated questions against the same long file: video, document, audio.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#multimodal-prompting-with-gemini-working-with-videos","title":"Multimodal Prompting with Gemini: Working with Videos\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#overview","title":"Overview\u00b6","text":"<p>Gemini models supports adding image, audio, video, and PDF files in text or chat prompts for a text or code response. Gemini 2.0 Flash supports up to 1 Million input tokens with up to 1 hours length of video per prompt. Gemini can analyze the audio embedded within a video as well. You can add videos to Gemini requests to perform video analysis tasks such as video summarization, video chapterization (or localization), key event detection, scene analysis, captioning and transcription and more.</p> <p>In this notebook we cover prompting recipes and strategies for working with Gemini on videos and show some examples on the way. This notebook is organized as follows:</p> <ul> <li>Video Understanding</li> <li>Key event detection</li> <li>Using System instruction</li> <li>Analyzing videos with step-by-step reasoning</li> <li>Generating structured output</li> <li>Using context caching for repeated queries</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#getting-started","title":"Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.</li> <li>Make sure that billing is enabled for your project.</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API.</li> <li>Enable the Cloud Storage API.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, including the optional section, you will need to have the Owner role for your project.</p> <p>If you want to skip the optional section, you need at least the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#install-vertex-ai-sdk-for-python-and-other-dependencies-if-needed","title":"Install Vertex AI SDK for Python and other dependencies (If Needed)\u00b6","text":"<p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.## Install Vertex AI SDK for Python and other dependencies (If Needed)</p> <p>The list <code>packages</code> contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and Initialize Vertex AI SDK\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>REGION</code> unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#define-utility-functions","title":"Define Utility functions\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#initialize-gemini","title":"Initialize Gemini\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-1-video-understanding","title":"Prompt #1. Video Understanding\u00b6","text":"<p>This task requires the input to be presented in two different modalities: text and video. The example of the API call is below, however this is non-optimal prompt and we can make it better.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#video-understanding-advanced-prompt","title":"Video Understanding. Advanced Prompt\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-2-video-understanding-key-events-detection","title":"Prompt #2. Video Understanding: Key events detection\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-3-video-understanding-using-system-instruction","title":"Prompt #3. Video Understanding: Using System instruction\u00b6","text":"<p>System Instruction (SI) is an effective way to steer Gemini's behavior and shape how the model responds to your prompt. SI can be used to describe model behavior such as persona, goal, tasks to perform, output format / tone / style, any constraints etc.</p> <p>SI behaves more \"sticky\" (or consistent) during multi-turn behavior. For example, if you want to achieve a behavior that the model will consistently follow, then system instruction is the best way to put this instruction.</p> <p>In this example, we will move the task rules to system instruction and the question on a specific event in the user prompt.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-4-video-understanding-step-by-step-reasoning","title":"Prompt #4. Video Understanding: Step-by-step reasoning\u00b6","text":"<p>We see that actually a mistake happened in analyzing the video. The model does not show all the timestamps where the cup is thrown. Let's fix it with \"step-by-step reasoning\".</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-5-video-understanding-get-structured-outputs","title":"Prompt #5. Video Understanding: Get structured outputs\u00b6","text":"<p>Gemini models can generate structured outputs such as JSON, providing a blueprint for the model's output. This feature is also referred to as controlled generation.</p> <p>In this example, we demonstrate Gemini to return structured output (JSON) from a video analysis. One of the ways to achieve better understanding of video (or any multimodal) content is to prompt the model to explain its \"reasoning\" about the response. This has proven to be very effective method, however it can increase the latency.</p> <p>Vertex AI Gemini API makes it easy to return JSON output by configuring response MIME type as <code>application/json</code>. Optionally, you can also configure <code>response_schema</code> with the JSON schema for the model to generate output as per the schema.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#prompt-6-video-understanding-context-caching","title":"Prompt #6. Video Understanding: Context Caching\u00b6","text":"<p>Context caching is a method to reduce the cost of requests that contain repeated content with high input token count. It can potentially reduce the latency at the cost of storing the objects in the cache. The user can specify cache expiration time for which the object is saved in cache.</p> <p>Context caching helps a lot when we want:</p> <ul> <li>to repeatedly ask questions about the long video</li> <li>to reduce costs and save latency</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video/#conclusion","title":"Conclusion\u00b6","text":"<p>This demonstrated various examples of working with Gemini using videos. Following are general prompting strategies when working with Gemini on multimodal prompts, that can help achieve better performance from Gemini:</p> <ol> <li>Craft clear and concise instructions.</li> <li>Add your video or any media first for single-media prompts.</li> <li>Add few-shot examples to the prompt to show the model how you want the task done and the expected output.</li> <li>Break down the task step-by-step.</li> <li>Specify the output format.</li> <li>Ask Gemini to include reasoning in its response along with decision or scores</li> <li>Use context caching for repeated queries.</li> </ol> <p>Specifically, when working with videos following may help:</p> <ol> <li>Specify timestamp format when localizing videos.</li> <li>Ask Gemini to focus on visual content for well-known video clips.</li> <li>Process long videos in segments for dense outputs.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/1_extract_pdf_pages_with_gemini/","title":"1 extract pdf pages with gemini","text":"In\u00a0[10]: Copied! <pre># Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2023 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[\u00a0]: Copied! <pre># Install python packages\n! pip install -U pypdf\n! pip install -U google-cloud-aiplatform\n! pip install -U pdf2image\n</pre> # Install python packages ! pip install -U pypdf ! pip install -U google-cloud-aiplatform ! pip install -U pdf2image In\u00a0[1]: Copied! <pre># Import all the required python packages\nimport io\nimport json\nimport pypdf\nimport vertexai\n\nfrom pdf2image import convert_from_bytes\nfrom IPython.display import display\nfrom typing import Iterable\n\nfrom vertexai.preview.generative_models import (\n    GenerationResponse,\n    GenerativeModel,\n    HarmBlockThreshold,\n    HarmCategory,\n    Part\n)\n</pre> # Import all the required python packages import io import json import pypdf import vertexai  from pdf2image import convert_from_bytes from IPython.display import display from typing import Iterable  from vertexai.preview.generative_models import (     GenerationResponse,     GenerativeModel,     HarmBlockThreshold,     HarmCategory,     Part ) <p>Include information about your project in the next cell.</p> In\u00a0[2]: Copied! <pre>PROJECT_ID = \"[your-project-id]\"  # Replace with your project ID\nLOCATION = \"us-central1\"  # Replace with your location\nMODEL_NAME = \"gemini-2.0-flash-001\"\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)\nmodel = GenerativeModel(MODEL_NAME)\nBLOCK_LEVEL = HarmBlockThreshold.BLOCK_ONLY_HIGH\n</pre> PROJECT_ID = \"[your-project-id]\"  # Replace with your project ID LOCATION = \"us-central1\"  # Replace with your location MODEL_NAME = \"gemini-2.0-flash-001\"  vertexai.init(project=PROJECT_ID, location=LOCATION) model = GenerativeModel(MODEL_NAME) BLOCK_LEVEL = HarmBlockThreshold.BLOCK_ONLY_HIGH <p>The following is the prompt used to extract the pages related to the question.</p> In\u00a0[3]: Copied! <pre>PROMPT_PAGES = \"\"\"\nReturn the numbers of all pages in the document above that contain information related to the question below.\n&lt;Instructions&gt;\n - Use the document above as your only source of information to determine which pages are related to the question below.\n - Return the page numbers of the document above that are related to the question. When in doubt, return the page anyway.\n - The response should be a JSON list, as shown in the example below.\n&lt;/Instructions&gt;\n&lt;Suggestions&gt;\n - The document above is a financial report with various tables, charts, infographics, lists, and additional text information.\n - Pay CLOSE ATTENTION to the chart legends and chart COLORS to determine the pages. Colors may indicate which information is important for determining the pages.\n - The color of the chart legends represents the color of the bars in the chart.\n - Use ONLY this document as context to determine the pages.\n - In most cases, the page number can be found in the footer.\n&lt;/Suggestions&gt;\n&lt;Question&gt;\n{question}\n&lt;/Question&gt;\n&lt;Example JSON Output&gt;\n{{\n  \"pages\": [1, 2, 3, 4, 5]\n}}\n&lt;/Example JSON Output&gt;\njson:\"\"\"\n</pre> PROMPT_PAGES = \"\"\" Return the numbers of all pages in the document above that contain information related to the question below.   - Use the document above as your only source of information to determine which pages are related to the question below.  - Return the page numbers of the document above that are related to the question. When in doubt, return the page anyway.  - The response should be a JSON list, as shown in the example below.    - The document above is a financial report with various tables, charts, infographics, lists, and additional text information.  - Pay CLOSE ATTENTION to the chart legends and chart COLORS to determine the pages. Colors may indicate which information is important for determining the pages.  - The color of the chart legends represents the color of the bars in the chart.  - Use ONLY this document as context to determine the pages.  - In most cases, the page number can be found in the footer.   {question}   {{   \"pages\": [1, 2, 3, 4, 5] }}  json:\"\"\" In\u00a0[4]: Copied! <pre>def pdf_cut(pdf_bytes: bytes, pages: list[int]) -&gt; bytes:\n    \"\"\"Using the pdf bytes and a list of page numbers,\n    return the pdf bytes of a new pdf with only those pages\n    Args:\n        pdf_bytes:\n            Bytes of a pdf file\n        pages:\n            List of page numbers to extract from the pdf bytes\n    Returns:\n        Bytes of a new pdf with only the extracted pages\n    \"\"\"\n    pdf_reader = pypdf.PdfReader(io.BytesIO(pdf_bytes))\n    pdf_writer = pypdf.PdfWriter()\n    for page in pages:\n        try:\n            pdf_writer.add_page(pdf_reader.pages[page - 1])\n        except Exception as e:\n            pass\n    output = io.BytesIO()\n    pdf_writer.write(output)\n    return output.getvalue()\n</pre> def pdf_cut(pdf_bytes: bytes, pages: list[int]) -&gt; bytes:     \"\"\"Using the pdf bytes and a list of page numbers,     return the pdf bytes of a new pdf with only those pages     Args:         pdf_bytes:             Bytes of a pdf file         pages:             List of page numbers to extract from the pdf bytes     Returns:         Bytes of a new pdf with only the extracted pages     \"\"\"     pdf_reader = pypdf.PdfReader(io.BytesIO(pdf_bytes))     pdf_writer = pypdf.PdfWriter()     for page in pages:         try:             pdf_writer.add_page(pdf_reader.pages[page - 1])         except Exception as e:             pass     output = io.BytesIO()     pdf_writer.write(output)     return output.getvalue() In\u00a0[5]: Copied! <pre>def generate(\n    prompt: list,\n    max_output_tokens: int = 2048,\n    temperature: int = 2,\n    top_p: float = 0.4,\n    stream: bool = False,\n) -&gt; GenerationResponse | Iterable[GenerationResponse]:\n    \"\"\"\n    Function to generate response using Gemini 2.0\n\n    Args:\n        prompt:\n            List of prompt parts\n        max_output_tokens:\n            Max Output tokens\n        temperature:\n            Temperature for the model\n        top_p:\n            Top-p for the model\n        stream:\n            Strem results?\n\n    Returns:\n        Model response\n\n    \"\"\"\n    responses = model.generate_content(\n        prompt,\n        generation_config={\n            \"max_output_tokens\": max_output_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        },\n        safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: BLOCK_LEVEL,\n        },\n        stream=stream,\n    )\n\n    return responses\n</pre> def generate(     prompt: list,     max_output_tokens: int = 2048,     temperature: int = 2,     top_p: float = 0.4,     stream: bool = False, ) -&gt; GenerationResponse | Iterable[GenerationResponse]:     \"\"\"     Function to generate response using Gemini 2.0      Args:         prompt:             List of prompt parts         max_output_tokens:             Max Output tokens         temperature:             Temperature for the model         top_p:             Top-p for the model         stream:             Strem results?      Returns:         Model response      \"\"\"     responses = model.generate_content(         prompt,         generation_config={             \"max_output_tokens\": max_output_tokens,             \"temperature\": temperature,             \"top_p\": top_p,         },         safety_settings={             HarmCategory.HARM_CATEGORY_HATE_SPEECH: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_HARASSMENT: BLOCK_LEVEL,         },         stream=stream,     )      return responses In\u00a0[6]: Copied! <pre>def pdf_pages(\n    question: str, \n    pdf_bytes: bytes, \n    instructions_prompt: str = PROMPT_PAGES\n) -&gt; list[int]:\n    \"\"\"\n    Function to generate a list of page numbers with pdf bytes and a question\n\n    Args:\n        question:\n            Question to ask the model\n        pdf_bytes:\n            PDF bytes\n        instructions_prompt:\n            Prompt for the model\n\n    Returns:\n        List of page numbers\n    \"\"\"\n    pdf_document = Part.from_data(data=pdf_bytes, mime_type=\"application/pdf\")\n    prompt = [\n        \"&lt;Document&gt;\",\n        pdf_document,\n        \"&lt;/Document&gt;\",\n        instructions_prompt.format(question=question),\n    ]\n    responses = generate(prompt=prompt)\n\n    if isinstance(responses, GenerationResponse):\n        output_json = json.loads(responses.text)\n    else:\n        output_json = json.loads(\n            \" \".join([response.text for response in responses])\n        )\n    return output_json[\"pages\"]\n</pre> def pdf_pages(     question: str,      pdf_bytes: bytes,      instructions_prompt: str = PROMPT_PAGES ) -&gt; list[int]:     \"\"\"     Function to generate a list of page numbers with pdf bytes and a question      Args:         question:             Question to ask the model         pdf_bytes:             PDF bytes         instructions_prompt:             Prompt for the model      Returns:         List of page numbers     \"\"\"     pdf_document = Part.from_data(data=pdf_bytes, mime_type=\"application/pdf\")     prompt = [         \"\",         pdf_document,         \"\",         instructions_prompt.format(question=question),     ]     responses = generate(prompt=prompt)      if isinstance(responses, GenerationResponse):         output_json = json.loads(responses.text)     else:         output_json = json.loads(             \" \".join([response.text for response in responses])         )     return output_json[\"pages\"] <p>In the next cell, include information about your question and the pdf_path.</p> <p>(Optional) If you are using Colab to test this notebook, you can try the following code to upload your PDF files.</p> <pre>from google.colab import files\nfiles.upload()\n</pre> <p>You can uncomment the code in the cell to use this method.</p> In\u00a0[7]: Copied! <pre># from google.colab import files\n# files.upload()\n</pre> # from google.colab import files # files.upload() In\u00a0[18]: Copied! <pre># Include your question and the path to your PDF\n# question = \"What are the key trends for financial services industry?\"\nquestion = \"From the Consolidated Balance Sheet, what was the difference between the total assets from 2022 to 2023?\"\npdf_path = \"./Cymbal Bank - Financial Statements.pdf\"\n</pre> # Include your question and the path to your PDF # question = \"What are the key trends for financial services industry?\" question = \"From the Consolidated Balance Sheet, what was the difference between the total assets from 2022 to 2023?\" pdf_path = \"./Cymbal Bank - Financial Statements.pdf\" In\u00a0[19]: Copied! <pre># Open the file, extract the pages using Gemini 2.0 and print them\nwith open(pdf_path, \"rb\") as f:\n    pdf_bytes = f.read()\npages = pdf_pages(question=question, pdf_bytes=pdf_bytes)\nprint(pages)\n</pre> # Open the file, extract the pages using Gemini 2.0 and print them with open(pdf_path, \"rb\") as f:     pdf_bytes = f.read() pages = pdf_pages(question=question, pdf_bytes=pdf_bytes) print(pages) <pre>[9]\n</pre> In\u00a0[12]: Copied! <pre># To ensure we find the answer to the question, it will also retrieve the page immediately after those.\nexpanded_pages = set(pages)\nexpanded_pages.update({i+1 for i in pages})\nnew_pdf = pdf_cut(pdf_bytes=pdf_bytes, pages=list(expanded_pages))\n</pre> # To ensure we find the answer to the question, it will also retrieve the page immediately after those. expanded_pages = set(pages) expanded_pages.update({i+1 for i in pages}) new_pdf = pdf_cut(pdf_bytes=pdf_bytes, pages=list(expanded_pages)) In\u00a0[13]: Copied! <pre># Write the result to a new PDF document\nwith open(\"./sample.pdf\", \"wb\") as fp:\n    fp.write(new_pdf)\n</pre> # Write the result to a new PDF document with open(\"./sample.pdf\", \"wb\") as fp:     fp.write(new_pdf) In\u00a0[\u00a0]: Copied! <pre>images = convert_from_bytes(new_pdf)\nfor i, image in enumerate(images):\n    display(image)\n</pre> images = convert_from_bytes(new_pdf) for i, image in enumerate(images):     display(image)"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/1_extract_pdf_pages_with_gemini/#identifying-relevant-pages-in-a-pdf-using-gemini-20","title":"Identifying Relevant Pages in a PDF using Gemini 2.0\u00b6","text":"<p>The goal of this notebook is to extract specific information from a large PDF by using Gemini to identify relevant pages and create a new, focused PDF.</p> <p>In this notebook, you will:</p> <ul> <li>Use Gemini to identify pages in a large PDF that contain information about a given question.</li> <li>Extract and compile the identified pages into a new PDF.</li> <li>Save the PDF to a file</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/1_extract_pdf_pages_with_gemini/#optional-print-the-pdf-pages","title":"(Optional) Print the PDF pages\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/2_pdf_info_extraction_with_gemini/","title":"Sample questions","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2023 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[1]: Copied! <pre># Import python packages\n\nfrom typing import Iterable\nimport io\nimport time\n\nimport vertexai\nfrom vertexai.preview.generative_models import (\n    GenerationResponse,\n    GenerativeModel,\n    HarmBlockThreshold,\n    HarmCategory,\n    Part\n)\n</pre> # Import python packages  from typing import Iterable import io import time  import vertexai from vertexai.preview.generative_models import (     GenerationResponse,     GenerativeModel,     HarmBlockThreshold,     HarmCategory,     Part ) <p>Include information about your project in the next cell.</p> In\u00a0[2]: Copied! <pre>PROJECT_ID = \"[your-project-id]\"  # Replace with your project ID\nLOCATION = \"us-central1\"  # Replace with your location\nMODEL_NAME = \"gemini-2.0-flash-001\"  # Replace with model name\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)\nmodel = GenerativeModel(MODEL_NAME)\nBLOCK_LEVEL = HarmBlockThreshold.BLOCK_ONLY_HIGH\n</pre> PROJECT_ID = \"[your-project-id]\"  # Replace with your project ID LOCATION = \"us-central1\"  # Replace with your location MODEL_NAME = \"gemini-2.0-flash-001\"  # Replace with model name  vertexai.init(project=PROJECT_ID, location=LOCATION) model = GenerativeModel(MODEL_NAME) BLOCK_LEVEL = HarmBlockThreshold.BLOCK_ONLY_HIGH In\u00a0[3]: Copied! <pre>prompt = \"\"\"\nUse the document above to answer the question below. Follow the Instructions and Suggestions below as a guide to answering the question.\n&lt;Instructions&gt;\n- First, analyze the question below and return which variables need to be analyzed, from what time period (example: second quarter of 2020), and any other details present in the question.\n- Then return an analysis of what is asked in the question.\n- Finally, carefully analyze the document above and answer the question below completely and correctly, using the variables determined in the previous step.\n- Explain how you arrived at this result.\n- Answer ONLY what was asked.\n&lt;Instructions&gt;\n&lt;Suggestions&gt;\n- The document above is a financial report with various tables, graphs, infographics, lists, and additional information in text.\n- PAY VERY CLOSE ATTENTION to the legends of the graphs and the COLORS of the graphs to answer the question below. The colors may indicate which information is important to answer the question.\n- The color of the graph legends represents the color of the graph bars.\n- Use ONLY this document as context to answer the question below.\n&lt;/Suggestions&gt;\n&lt;Question&gt;\n{question}\n&lt;/Question&gt;\nanswer:\"\"\"\n</pre> prompt = \"\"\" Use the document above to answer the question below. Follow the Instructions and Suggestions below as a guide to answering the question.  - First, analyze the question below and return which variables need to be analyzed, from what time period (example: second quarter of 2020), and any other details present in the question. - Then return an analysis of what is asked in the question. - Finally, carefully analyze the document above and answer the question below completely and correctly, using the variables determined in the previous step. - Explain how you arrived at this result. - Answer ONLY what was asked.   - The document above is a financial report with various tables, graphs, infographics, lists, and additional information in text. - PAY VERY CLOSE ATTENTION to the legends of the graphs and the COLORS of the graphs to answer the question below. The colors may indicate which information is important to answer the question. - The color of the graph legends represents the color of the graph bars. - Use ONLY this document as context to answer the question below.   {question}  answer:\"\"\" In\u00a0[4]: Copied! <pre>def generate(\n    prompt: list,\n    max_output_tokens: int = 2048,\n    temperature: int = 2,\n    top_p: float = 0.4,\n    stream: bool = False,\n) -&gt; GenerationResponse | Iterable[GenerationResponse]:\n    \"\"\"\n    Function to generate response using Gemini 2.0\n\n    Args:\n        prompt:\n            List of prompt parts\n        max_output_tokens:\n            Max Output tokens\n        temperature:\n            Temperature for the model\n        top_p:\n            Top-p for the model\n        stream:\n            Strem results?\n\n    Returns:\n        Model response\n\n    \"\"\"\n    responses = model.generate_content(\n        prompt,\n        generation_config={\n            \"max_output_tokens\": max_output_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        },\n        safety_settings={\n            HarmCategory.HARM_CATEGORY_HATE_SPEECH: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_LEVEL,\n            HarmCategory.HARM_CATEGORY_HARASSMENT: BLOCK_LEVEL,\n        },\n        stream=stream,\n    )\n\n    return responses\n\n\ndef retry_generate(pdf_document: Part, prompt: str, question: str):\n    predicted = False\n    while not predicted:\n        try:\n            response = generate(\n                prompt=[pdf_document, prompt.format(question=question)]\n            )\n        except Exception as e:\n            print(\"sleeping for 2 seconds ...\")\n            print(e)\n            time.sleep(2)\n        else:\n            predicted = True\n\n    return response\n</pre> def generate(     prompt: list,     max_output_tokens: int = 2048,     temperature: int = 2,     top_p: float = 0.4,     stream: bool = False, ) -&gt; GenerationResponse | Iterable[GenerationResponse]:     \"\"\"     Function to generate response using Gemini 2.0      Args:         prompt:             List of prompt parts         max_output_tokens:             Max Output tokens         temperature:             Temperature for the model         top_p:             Top-p for the model         stream:             Strem results?      Returns:         Model response      \"\"\"     responses = model.generate_content(         prompt,         generation_config={             \"max_output_tokens\": max_output_tokens,             \"temperature\": temperature,             \"top_p\": top_p,         },         safety_settings={             HarmCategory.HARM_CATEGORY_HATE_SPEECH: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_LEVEL,             HarmCategory.HARM_CATEGORY_HARASSMENT: BLOCK_LEVEL,         },         stream=stream,     )      return responses   def retry_generate(pdf_document: Part, prompt: str, question: str):     predicted = False     while not predicted:         try:             response = generate(                 prompt=[pdf_document, prompt.format(question=question)]             )         except Exception as e:             print(\"sleeping for 2 seconds ...\")             print(e)             time.sleep(2)         else:             predicted = True      return response In\u00a0[5]: Copied! <pre># from google.colab import files\n# files.upload()\n</pre> # from google.colab import files # files.upload() In\u00a0[9]: Copied! <pre>question = \"From the Consolidated Balance Sheet, what was the difference between the total assets from 2022 to 2023?\"\npdf_path = \"./Cymbal Bank - Financial Statements.pdf\"\n</pre> question = \"From the Consolidated Balance Sheet, what was the difference between the total assets from 2022 to 2023?\" pdf_path = \"./Cymbal Bank - Financial Statements.pdf\" In\u00a0[10]: Copied! <pre>with open(pdf_path, \"rb\") as fp:\n    pdf_document = Part.from_data(data=fp.read(), mime_type=\"application/pdf\")\n\nresponse = retry_generate(pdf_document, prompt, question)\nprint(response.text)\n</pre> with open(pdf_path, \"rb\") as fp:     pdf_document = Part.from_data(data=fp.read(), mime_type=\"application/pdf\")  response = retry_generate(pdf_document, prompt, question) print(response.text) <pre>## Analysis of the Question:\n\nThe question asks for the difference in **total assets** between the years **2022** and **2023** from the **Consolidated Balance Sheet**. This requires locating the relevant section within the document and identifying the values associated with each year. \n\n\n## Locating the Information:\n\n1. **Consolidated Balance Sheet:** The document provides a \"Consolidated Balance Sheet\" table which contains financial data for the years 2022 and 2023.\n2. **Total Assets:** We need to identify the row labeled \"Total assets\" within the table. \n3. **Values for 2022 and 2023:**  We will find the corresponding values under the \"12/31/2022\" and \"12/31/2023\" columns.\n\n\n## Calculation:\n\n1. **2023 Total Assets:**  $2,238,274 million \n2. **2022 Total Assets:** $2,281,868 million\n3. **Difference:** $2,281,868 million - $2,238,274 million = $43,594 million\n\n\n## Answer:\n\nThe difference in total assets between 2022 and 2023 according to the Consolidated Balance Sheet is **$43,594 million**. This indicates a decrease in total assets from 2022 to 2023. \n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/2_pdf_info_extraction_with_gemini/#long-pdf-qa-with-gemini-20","title":"Long PDF Q&amp;A with Gemini 2.0\u00b6","text":"<p>The goal of this notebook is to extract specific information from a large PDF by using Gemini 2.0.</p> <p>In this notebook, you will:</p> <ul> <li>Use Gemini to answer a specific question contained in a PDF document.</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/pdf_processing/2_pdf_info_extraction_with_gemini/#sample-questions","title":"Sample questions\u00b6","text":"<p>In the next cell, include information about your question and the pdf_path.</p> <p>(Optional) If you are using Colab to test this notebook, you can try the following code to upload your PDF files.</p> <pre>from google.colab import files\nfiles.upload()\n</pre> <p>You can uncomment the code in the cell to use this method.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/","title":"Gemini 2.0 on Vertex AI: Front-end tooling for multimodal analysis and reasoning use cases","text":"<p>This folder has code samples demonstrating processing Gemini 2.0 responses for analysis and multimodal reasoning use cases:</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/#spatial-reasoning-and-bounding-box-rendering","title":"Spatial reasoning and bounding box rendering","text":"<ul> <li>Spatial reasoning with Gemini 2.0 using Vertex AI SDK</li> <li>Interactive spatial reasoning with Gemini 2.0</li> </ul>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/","title":"2D spatial reasoning with Gemini 2.0","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License 2.0\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License 2.0 # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <p>Share to:</p> <p>This notebook demonstrates object detection and spatial reasoning with Gemini 2.0 in Vertex AI.</p> <p>You'll learn how to use Gemini to perform object detection like this: </p> <p>You will find live examples including object detection with</p> <ul> <li>overlaying information</li> <li>searching within an image</li> <li>translating and understanding things in multiple languages</li> <li>using Gemini reasoning abilities</li> </ul> <p>Please note</p> <p>There's no \"magical prompt\". Feel free to experiment with different ones. You can use the samples included in this notebook or upload your own images and write your own prompts.</p> <p>This notebook is based on a notebook published by the Gemini AI Studio team.</p> <p>Please enter the PROJECT_ID of your Google Cloud Project</p> In\u00a0[17]: Copied! <pre>PROJECT_ID = '[your-project-id]' # @param {type: 'string'}\nLOCATION = 'us-central1' # @param {type: 'string'}\n</pre> PROJECT_ID = '[your-project-id]' # @param {type: 'string'} LOCATION = 'us-central1' # @param {type: 'string'} <p>Install or upgrade Vertex AI SDK and restart the Colab runtime (if necessary).</p> In\u00a0[18]: Copied! <pre>def install_or_upgrade_vertex_ai():\n    package_name = \"google-cloud-aiplatform\"\n    required_version = \"1.73.0\"\n\n    try:\n        import google.cloud.aiplatform as aiplatform\n        installed_version = aiplatform.__version__\n\n        if installed_version &lt; required_version:\n            print(f\"Upgrading {package_name} from version {installed_version} to {required_version}...\")\n            !pip install google-cloud-aiplatform --upgrade --quiet --user\n            print(f\"Successfully upgraded {package_name}.\")\n            restart_runtime()\n        else:\n            print(f\"{package_name} is already installed with version {installed_version}.\")\n    except ImportError:\n        print(f\"{package_name} is not installed. Installing version {required_version}...\")\n        !pip install google-cloud-aiplatform --upgrade --quiet --user\n        print(f\"Successfully installed {package_name}.\")\n        restart_runtime()\n\ndef restart_runtime():\n  import IPython\n\n  print('The kernel is going to restart. In Colab or Colab Enterprise, you might see an error message that says \"Your session crashed for an unknown reason.\" This is expected.')\n  print('Once the runtime is restarted you can continue running individual cells or run the entire notebook with the \"Run all\" command.')\n  IPython.Application.instance().kernel.do_shutdown(True)\n\ninstall_or_upgrade_vertex_ai()\n</pre> def install_or_upgrade_vertex_ai():     package_name = \"google-cloud-aiplatform\"     required_version = \"1.73.0\"      try:         import google.cloud.aiplatform as aiplatform         installed_version = aiplatform.__version__          if installed_version &lt; required_version:             print(f\"Upgrading {package_name} from version {installed_version} to {required_version}...\")             !pip install google-cloud-aiplatform --upgrade --quiet --user             print(f\"Successfully upgraded {package_name}.\")             restart_runtime()         else:             print(f\"{package_name} is already installed with version {installed_version}.\")     except ImportError:         print(f\"{package_name} is not installed. Installing version {required_version}...\")         !pip install google-cloud-aiplatform --upgrade --quiet --user         print(f\"Successfully installed {package_name}.\")         restart_runtime()  def restart_runtime():   import IPython    print('The kernel is going to restart. In Colab or Colab Enterprise, you might see an error message that says \"Your session crashed for an unknown reason.\" This is expected.')   print('Once the runtime is restarted you can continue running individual cells or run the entire notebook with the \"Run all\" command.')   IPython.Application.instance().kernel.do_shutdown(True)  install_or_upgrade_vertex_ai() <pre>google-cloud-aiplatform is already installed with version 1.73.0.\n</pre> In\u00a0[\u00a0]: Copied! <pre>import base64\nimport io\nimport os\nimport requests\nimport sys\nfrom io import BytesIO\nfrom PIL import Image\n\nif 'google.colab' in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\n</pre> import base64 import io import os import requests import sys from io import BytesIO from PIL import Image  if 'google.colab' in sys.modules:     from google.colab import auth     auth.authenticate_user() In\u00a0[\u00a0]: Copied! <pre>import vertexai\nfrom vertexai.generative_models import (GenerativeModel, HarmBlockThreshold, HarmCategory, Part)\n\nmodel_name = 'gemini-2.0-flash-exp' # This specific model is required\nvertexai.init(project=PROJECT_ID, location=LOCATION)\n</pre> import vertexai from vertexai.generative_models import (GenerativeModel, HarmBlockThreshold, HarmCategory, Part)  model_name = 'gemini-2.0-flash-exp' # This specific model is required vertexai.init(project=PROJECT_ID, location=LOCATION) In\u00a0[\u00a0]: Copied! <pre>bounding_box_system_instructions = \"\"\"\nReturn bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.\nIf an object is present multiple times, name them according to their unique characteristics (colors, size, position, unique characteristics, etc.)\n\"\"\"\n\nsafety_settings= {\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n}\n\n# Temperature above 0 is recommended to prevent repeated outputs\ngeneration_config = {\n    'temperature': 0.5,\n    'max_output_tokens': 8192,\n    'candidate_count': 1,\n}\n\nmodel = GenerativeModel(\n    model_name=model_name,\n    system_instruction=bounding_box_system_instructions,\n    generation_config=generation_config,\n    safety_settings=safety_settings,\n)\n</pre> bounding_box_system_instructions = \"\"\" Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects. If an object is present multiple times, name them according to their unique characteristics (colors, size, position, unique characteristics, etc.) \"\"\"  safety_settings= {     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH, }  # Temperature above 0 is recommended to prevent repeated outputs generation_config = {     'temperature': 0.5,     'max_output_tokens': 8192,     'candidate_count': 1, }  model = GenerativeModel(     model_name=model_name,     system_instruction=bounding_box_system_instructions,     generation_config=generation_config,     safety_settings=safety_settings, ) In\u00a0[\u00a0]: Copied! <pre># @title Bounding Box Visualization\n\n# Get Noto JP font to display japanese characters\n!apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n\nimport json\nimport random\nimport io\nfrom PIL import Image, ImageDraw, ImageFont\nfrom PIL import ImageColor\n\nadditional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n\ndef plot_bounding_boxes(im, bounding_boxes):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n\n    Args:\n        img_path: The path to the image file.\n        bounding_boxes: A list of bounding boxes containing the name of the object\n         and their positions in normalized [y1 x1 y2 x2] format.\n    \"\"\"\n\n    img = im.resize((1024,1024))\n    width, height = img.size\n    draw = ImageDraw.Draw(img)\n\n    # Define a list of bounding box border colors\n    colors = [\n    'red',\n    'green',\n    'blue',\n    'yellow',\n    'orange',\n    'pink',\n    'purple',\n    'brown',\n    'gray',\n    'beige',\n    'turquoise',\n    'cyan',\n    'magenta',\n    'lime',\n    'navy',\n    'maroon',\n    'teal',\n    'olive',\n    'coral',\n    'lavender',\n    'violet',\n    'gold',\n    'silver',\n    ] + additional_colors\n\n    # We parse out the markdown fencing\n    bounding_boxes = parse_json(bounding_boxes)\n\n    font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n\n    # Iterate over the bounding boxes\n    for i, bounding_box in enumerate(json.loads(bounding_boxes)):\n      # Select a color from the list\n      color = colors[i % len(colors)]\n\n      # Convert normalized coordinates to absolute coordinates\n      abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)\n      abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)\n      abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)\n      abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)\n\n      if abs_x1 &gt; abs_x2:\n        abs_x1, abs_x2 = abs_x2, abs_x1\n\n      if abs_y1 &gt; abs_y2:\n        abs_y1, abs_y2 = abs_y2, abs_y1\n\n      # Draw the bounding box\n      draw.rectangle(\n          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n      )\n\n      # Draw the text\n      if \"label\" in bounding_box:\n        draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n\n    # Display the image\n    img.show()\n    return img\n</pre> # @title Bounding Box Visualization  # Get Noto JP font to display japanese characters !apt-get install fonts-noto-cjk  # For Noto Sans CJK JP  import json import random import io from PIL import Image, ImageDraw, ImageFont from PIL import ImageColor  additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]  def plot_bounding_boxes(im, bounding_boxes):     \"\"\"     Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.      Args:         img_path: The path to the image file.         bounding_boxes: A list of bounding boxes containing the name of the object          and their positions in normalized [y1 x1 y2 x2] format.     \"\"\"      img = im.resize((1024,1024))     width, height = img.size     draw = ImageDraw.Draw(img)      # Define a list of bounding box border colors     colors = [     'red',     'green',     'blue',     'yellow',     'orange',     'pink',     'purple',     'brown',     'gray',     'beige',     'turquoise',     'cyan',     'magenta',     'lime',     'navy',     'maroon',     'teal',     'olive',     'coral',     'lavender',     'violet',     'gold',     'silver',     ] + additional_colors      # We parse out the markdown fencing     bounding_boxes = parse_json(bounding_boxes)      font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)      # Iterate over the bounding boxes     for i, bounding_box in enumerate(json.loads(bounding_boxes)):       # Select a color from the list       color = colors[i % len(colors)]        # Convert normalized coordinates to absolute coordinates       abs_y1 = int(bounding_box[\"box_2d\"][0]/1000 * height)       abs_x1 = int(bounding_box[\"box_2d\"][1]/1000 * width)       abs_y2 = int(bounding_box[\"box_2d\"][2]/1000 * height)       abs_x2 = int(bounding_box[\"box_2d\"][3]/1000 * width)        if abs_x1 &gt; abs_x2:         abs_x1, abs_x2 = abs_x2, abs_x1        if abs_y1 &gt; abs_y2:         abs_y1, abs_y2 = abs_y2, abs_y1        # Draw the bounding box       draw.rectangle(           ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4       )        # Draw the text       if \"label\" in bounding_box:         draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)      # Display the image     img.show()     return img In\u00a0[\u00a0]: Copied! <pre># @title Image encoder\ndef encode_image(local_file_path: str) -&gt; Part:\n    encoded_image = base64.b64encode(open(local_file_path, 'rb').read()).decode('utf-8')\n    return Part.from_data(data=base64.b64decode(encoded_image), mime_type='image/jpeg')\n</pre> # @title Image encoder def encode_image(local_file_path: str) -&gt; Part:     encoded_image = base64.b64encode(open(local_file_path, 'rb').read()).decode('utf-8')     return Part.from_data(data=base64.b64decode(encoded_image), mime_type='image/jpeg')  In\u00a0[\u00a0]: Copied! <pre># @title Model output parsing\ndef parse_json(json_output):\n    # We parse out the markdown fencing\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n            break  # Exit the loop once \"```json\" is found\n    return json_output\n</pre> # @title Model output parsing def parse_json(json_output):     # We parse out the markdown fencing     lines = json_output.splitlines()     for i, line in enumerate(lines):         if line == \"```json\":             json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"             json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"             break  # Exit the loop once \"```json\" is found     return json_output In\u00a0[\u00a0]: Copied! <pre># Load sample images\n!wget https://storage.googleapis.com/generativeai-downloads/images/socks.jpg -O Socks.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/vegetables.jpg -O Vegetables.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/Japanese_Bento.png -O Japanese_bento.png -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/Cupcakes.jpg -O Cupcakes.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/origamis.jpg -O Origamis.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/fruits.jpg -O Fruits.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/cat.jpg -O Cat.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/pumpkins.jpg -O Pumpkins.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/breakfast.jpg -O Breakfast.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/bookshelf.jpg -O Bookshelf.jpg -q\n!wget https://storage.googleapis.com/generativeai-downloads/images/spill.jpg -O Spill.jpg -q\n</pre> # Load sample images !wget https://storage.googleapis.com/generativeai-downloads/images/socks.jpg -O Socks.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/vegetables.jpg -O Vegetables.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/Japanese_Bento.png -O Japanese_bento.png -q !wget https://storage.googleapis.com/generativeai-downloads/images/Cupcakes.jpg -O Cupcakes.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/origamis.jpg -O Origamis.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/fruits.jpg -O Fruits.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/cat.jpg -O Cat.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/pumpkins.jpg -O Pumpkins.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/breakfast.jpg -O Breakfast.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/bookshelf.jpg -O Bookshelf.jpg -q !wget https://storage.googleapis.com/generativeai-downloads/images/spill.jpg -O Spill.jpg -q In\u00a0[\u00a0]: Copied! <pre>image = \"Cupcakes.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\nImage.open(image).resize((400,400))\n</pre> image = \"Cupcakes.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true} Image.open(image).resize((400,400)) <p>Now we can test a simple prompt to find all the cupcakes in the image.</p> <p>To prevent the model from repeating itself, it is recommended to use a temperature above 0, in this case 0.5. Limiting the number of items (25 in the system instructions) is also a way to prevent the model from looping and to speed up the decoding of the bounding boxes.</p> In\u00a0[\u00a0]: Copied! <pre>prompt = \"Detect the 2d bounding boxes of the cupcakes (with \u201clabel\u201d as topping description\u201d)\"  # @param {type:\"string\"}\n\n# Run inference to find the bounding boxes\nresponse = model.generate_content(\n    contents=[prompt, encode_image(image)]\n)\n\nprint(response.text)\n</pre> prompt = \"Detect the 2d bounding boxes of the cupcakes (with \u201clabel\u201d as topping description\u201d)\"  # @param {type:\"string\"}  # Run inference to find the bounding boxes response = model.generate_content(     contents=[prompt, encode_image(image)] )  print(response.text) <p>As you can see, even without any instructions about the format, Gemini is trained to always use this format with a label and the coordinates of the bounding box in a \"box_2d\" array.</p> <p>Please note that Y coordinates preceed X coordinates which is a bit unusual.</p> In\u00a0[\u00a0]: Copied! <pre>plot_bounding_boxes(Image.open(image), response.text)\n</pre> plot_bounding_boxes(Image.open(image), response.text) In\u00a0[\u00a0]: Copied! <pre>image = \"Socks.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\nprompt = \"Find the sock that matches the one at the top and return the bounding box for that sock\"  # @param [\"Detect all rainbow socks\", \"Show me the positions of the socks with the face\",\"Find the sock that matches the one at the top and return the bounding box for that sock\"] {\"allow-input\":true}\n\n# Run inference to find bounding boxes\nresponse = model.generate_content(\n    contents=[prompt, encode_image(image)]\n)\n\n# Check output\nprint(response.text)\n\n# Generate image with bounding boxes\nplot_bounding_boxes(Image.open(image), response.text)\n</pre> image = \"Socks.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true} prompt = \"Find the sock that matches the one at the top and return the bounding box for that sock\"  # @param [\"Detect all rainbow socks\", \"Show me the positions of the socks with the face\",\"Find the sock that matches the one at the top and return the bounding box for that sock\"] {\"allow-input\":true}  # Run inference to find bounding boxes response = model.generate_content(     contents=[prompt, encode_image(image)] )  # Check output print(response.text)  # Generate image with bounding boxes plot_bounding_boxes(Image.open(image), response.text) <p>Try it with different images and prompts. Different samples are proposed but you can also write your own.</p> In\u00a0[\u00a0]: Copied! <pre>image = \"Japanese_bento.png\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\nprompt = \"Explain what those dishes are with a 5 words description\"  # @param [\"Detect food items, with Japanese characters + english translation in \\\"label\\\".\", \"Show me the vegan dishes\",\"Explain what those dishes are with a 5 words description\",\"Find the dishes with allergens and label them accordingly\"] {\"allow-input\":true}\n\n# Run inference to find bounding boxes\nresponse = model.generate_content(\n    contents=[prompt, encode_image(image)]\n)\n\n# Generate image with bounding boxes\nplot_bounding_boxes(Image.open(image), response.text)\n</pre> image = \"Japanese_bento.png\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true} prompt = \"Explain what those dishes are with a 5 words description\"  # @param [\"Detect food items, with Japanese characters + english translation in \\\"label\\\".\", \"Show me the vegan dishes\",\"Explain what those dishes are with a 5 words description\",\"Find the dishes with allergens and label them accordingly\"] {\"allow-input\":true}  # Run inference to find bounding boxes response = model.generate_content(     contents=[prompt, encode_image(image)] )  # Generate image with bounding boxes plot_bounding_boxes(Image.open(image), response.text) In\u00a0[\u00a0]: Copied! <pre>image = \"Origamis.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true}\nprompt = \"Draw a square around the fox' shadow\"  # @param [\"Find the two origami animals.\", \"Where are the origamis' shadows?\",\"Draw a square around the fox' shadow\"] {\"allow-input\":true}\n\n# Run inference to find bounding boxes\nresponse = model.generate_content(\n    contents=[prompt, encode_image(image)]\n)\n\n# Generate image with bounding boxes\nplot_bounding_boxes(Image.open(image), response.text)\n</pre> image = \"Origamis.jpg\" # @param [\"Socks.jpg\",\"Vegetables.jpg\",\"Japanese_bento.png\",\"Cupcakes.jpg\",\"Origamis.jpg\",\"Fruits.jpg\",\"Cat.jpg\",\"Pumpkins.jpg\",\"Breakfast.jpg\",\"Bookshelf.jpg\", \"Spill.jpg\"] {\"allow-input\":true} prompt = \"Draw a square around the fox' shadow\"  # @param [\"Find the two origami animals.\", \"Where are the origamis' shadows?\",\"Draw a square around the fox' shadow\"] {\"allow-input\":true}  # Run inference to find bounding boxes response = model.generate_content(     contents=[prompt, encode_image(image)] )  # Generate image with bounding boxes plot_bounding_boxes(Image.open(image), response.text) <p>If you check the previous examples, the Japanese food one in particular,multiple other prompt samples are provided to experiment with Gemini reasoning capabilities.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#copyright-2024-google-llc","title":"Copyright 2024 Google LLC.\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#2d-spatial-reasoning-with-gemini-20","title":"2D spatial reasoning with Gemini 2.0\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#setup","title":"Setup\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#authenticate-to-google-cloud","title":"Authenticate to Google Cloud\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project. More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#initialize-vertex-ai-sdk-client","title":"Initialize Vertex AI SDK client\u00b6","text":"<p>Please rerun this cell if you ever get a Session Timeout error:</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#configure-the-model","title":"Configure the model\u00b6","text":"<p>The system instructions are mainly used to make the prompts shorter by not having to reapeat each time the format. They are also telling the model how to deal with similar objects which is a nice way to let it be creative.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#utils","title":"Utils\u00b6","text":"<p>These scripts will be needed to plot the bounding boxes. Of course they are just examples and you are free to use any other libraries.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#download-sample-images","title":"Download sample images\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#object-detection-with-bounding-boxes","title":"Object detection with Bounding Boxes\u00b6","text":"<p>Let's start by loading an image:</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#search-within-an-image","title":"Search within an image\u00b6","text":"<p>A more nuanced example of finding requested objects and displaying bounding boxes with additional information.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#multilingual-capabilities","title":"Multilingual capabilities\u00b6","text":"<p>As Gemini is able to understand multiple languages, you can combine spatial reasoning with multilingual capabilities.</p> <p>You can give it an image like this and prompt it to label each item with Japanese characters and English translation. The model reads the text and recognize the pictures from the image itself and translates them.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#summary","title":"Summary\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#reasoning-capabilities","title":"Reasoning capabilities\u00b6","text":"<p>The model can also reason based on the image. You can ask it about the positions of items, their utility, or, like in this example, to find the shadow of a speficic item.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_SDK_for_gemini2/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrates a few ways to leverage Gemini 2.0's spacial reasoning for various tasks, including object detection, spatial reasoning, and multilingual capabilities.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/","title":"Leveraging Gemini for Bounding Boxes","text":"In\u00a0[\u00a0]: Copied! <pre>#@title Licensed under the Apache License, Version 2.0 (the \"License\")\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> #@title Licensed under the Apache License, Version 2.0 (the \"License\") # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      <p>Share to:</p> Author(s) Emmanuel Awa, Dennis Kashkin Reviewer(s) Skander Hannachi, Rajesh Thallam Last updated 2024 12 11: Gemini 2.0 Flash Experimental Release 2024 12 11: Initial Publication In\u00a0[\u00a0]: Copied! <pre>import sys\nimport importlib.metadata\nimport time\nimport IPython\n\ndef check_package_version(package_name, required_version):\n    try:\n        installed_version = importlib.metadata.version(package_name)\n        if installed_version != required_version:\n            print(f\"Warning: {package_name} {required_version} \"\n                  f\"required, but {installed_version} is installed.\")\n            return False  # Indicate version mismatch\n        return True  # Indicate correct version\n    except importlib.metadata.PackageNotFoundError:\n        print(f\"Warning: {package_name} is not installed.\")\n        return False  # Indicate package not found\n\n# List of packages and their required versions\npackages_to_check = {\n    'google-cloud-aiplatform': '1.74.0',  # Replace with your desired version\n    'gradio': '5.8.0',  # Replace with your desired version\n    # Add more packages and versions as needed\n}\n\n# Check if any required package is missing or has a version mismatch\nrestart_required = False\nfor package_name, required_version in packages_to_check.items():\n    if not check_package_version(package_name, required_version):\n        restart_required = True\n        print(f\"Installing {package_name}=={required_version}\")\n        !pip install {package_name}=={required_version} --quiet --user\n\n# Restart the kernel if necessary\nif restart_required:\n    print(\"Restarting kernel...\")\n    time.sleep(5)  # Add time for the environment to update\n    app = IPython.Application.instance()\n    app.kernel.do_shutdown(True)\n</pre> import sys import importlib.metadata import time import IPython  def check_package_version(package_name, required_version):     try:         installed_version = importlib.metadata.version(package_name)         if installed_version != required_version:             print(f\"Warning: {package_name} {required_version} \"                   f\"required, but {installed_version} is installed.\")             return False  # Indicate version mismatch         return True  # Indicate correct version     except importlib.metadata.PackageNotFoundError:         print(f\"Warning: {package_name} is not installed.\")         return False  # Indicate package not found  # List of packages and their required versions packages_to_check = {     'google-cloud-aiplatform': '1.74.0',  # Replace with your desired version     'gradio': '5.8.0',  # Replace with your desired version     # Add more packages and versions as needed }  # Check if any required package is missing or has a version mismatch restart_required = False for package_name, required_version in packages_to_check.items():     if not check_package_version(package_name, required_version):         restart_required = True         print(f\"Installing {package_name}=={required_version}\")         !pip install {package_name}=={required_version} --quiet --user  # Restart the kernel if necessary if restart_required:     print(\"Restarting kernel...\")     time.sleep(5)  # Add time for the environment to update     app = IPython.Application.instance()     app.kernel.do_shutdown(True) <p>Before running the notebook, you will need to provide the following:</p> <ul> <li>GCP PROJECT_ID: Your Google Cloud Project ID.</li> <li>LOCATION: The region for your Vertex AI resources (e.g., 'us-central1').</li> </ul> <p>Make sure you have a Google Cloud Project with billing enabled before proceeding. You can create a new project or use an existing one.</p> <p>You will be prompted to enter these values in a form below.</p> In\u00a0[\u00a0]: Copied! <pre>PROJECT_ID = '[your-project-id-here]' # @param {type: 'string'}\nLOCATION = 'us-central1' # @param {type: 'string'}\n</pre> PROJECT_ID = '[your-project-id-here]' # @param {type: 'string'} LOCATION = 'us-central1' # @param {type: 'string'} In\u00a0[\u00a0]: Copied! <pre>import sys\nif 'google.colab' in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\n    print('Authenticated')\n</pre> import sys if 'google.colab' in sys.modules:     from google.colab import auth     auth.authenticate_user()     print('Authenticated') In\u00a0[\u00a0]: Copied! <pre>import base64\nimport hashlib\nimport json\nimport os\nimport sys\nimport re\nimport gradio as gr\nfrom google.colab import auth, userdata\nfrom typing import Optional, Union\nfrom PIL import Image as PILImage\nfrom PIL import ImageDraw, ImageColor, ImageFont, UnidentifiedImageError\n\nimport vertexai\nfrom vertexai.generative_models import (GenerativeModel,\n                                        HarmBlockThreshold,\n                                        HarmCategory,\n                                        Part)\n</pre> import base64 import hashlib import json import os import sys import re import gradio as gr from google.colab import auth, userdata from typing import Optional, Union from PIL import Image as PILImage from PIL import ImageDraw, ImageColor, ImageFont, UnidentifiedImageError  import vertexai from vertexai.generative_models import (GenerativeModel,                                         HarmBlockThreshold,                                         HarmCategory,                                         Part) In\u00a0[\u00a0]: Copied! <pre>MODEL_NAME = 'gemini-2.0-flash-exp'\nvertexai.init(project=PROJECT_ID, location=LOCATION)\n</pre> MODEL_NAME = 'gemini-2.0-flash-exp' vertexai.init(project=PROJECT_ID, location=LOCATION) In\u00a0[\u00a0]: Copied! <pre>class BoundingBox():\n    \"\"\"Create a BoundingBox in Gemini format (X and Y are on 0..1000 scale)\"\"\"\n\n    def __init__(self, top: int, left: int, bottom: int, right: int, label: str | None = None):\n\n        if None in [top, left, bottom, right]:\n            raise ValueError(f\"BoundingBox requires all coordinates to be set.\")\n        if top &lt; 0 or top &gt; 1000: raise ValueError(f'ymin must be an integer between 0 and 1000')\n        if left &lt; 0: raise ValueError(f'xmin must be an integer between 0 and 1000')\n        if bottom &lt; 0: raise ValueError(f'ymax must be an integer between 0 and 1000')\n        if right &lt; 0: raise ValueError(f'xmax must be an integer between 0 and 1000')\n        if right &lt;= left: raise ValueError(f'xmax must be greater than xmin (right={right}, left={left})')\n        if bottom &lt;= top: raise ValueError(f'ymax must be greater than ymin (bottom={bottom}, top={top})')\n        self.left = left\n        self.right = right\n        self.top = top\n        self.bottom = bottom\n        self.label = label\n        signature = label or f'{top}-{left}-{bottom}-{right}'\n        int_hash = int(hashlib.sha256(signature.encode('utf-8')).hexdigest(), 16) % (10 ** 8)\n        colors = list(ImageColor.colormap.keys())\n        colors = [color for color in colors if color != 'grey'] # Reserve grey for borders\n        stable_color_index = int_hash % len(colors)\n        self.color = colors[stable_color_index]\n        print(f'Stable color index: {stable_color_index} based on int_hash={int_hash}: {self.color}, {len(colors)} colors')\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the bounding box.\"\"\"\n        return f'TLBR[{self.top}, {self.left}, {self.bottom}, {self.right}]: {self.label or \"\"} #{self.color}'\n\n    @staticmethod\n    def is_numeric(value: str) -&gt; bool:\n        \"\"\"Check if a string is a number.\"\"\"\n        return value.strip().lstrip('-').replace('.', '', 1).isdigit()\n\n    @staticmethod\n    def from_markdown(text: str) -&gt; Union['BoundingBox', None]:\n        \"\"\"Create a bounding box from a markdown string.\"\"\"\n        if not text:\n            return None\n        for line in text.strip().splitlines():\n            line = line.strip().lstrip('-').strip()\n            # Extract the numbers from the line after removing brackets and splitting by comma\n            if '[' in line and ']' in line:\n                numbers = line.split('[')[1].split(']')[0].split(',')\n            else:\n                numbers =  line.split(',')\n            if len(numbers) != 4:\n                print(f'Skipping response line with {len(numbers)} comma separated parts instead of 4: {text}')\n                continue\n            ints = [int(num.strip()) for num in numbers if BoundingBox.is_numeric(num)]\n            if len(ints) != 4:\n                print(f'Skipping response line with only {len(numbers)} comma separated numbers instead of 4: {text}')\n                continue\n            return BoundingBox(ints[0], ints[1], ints[2], ints[3]) # Using the first bounding box (even if the model returns multiple)\n        return None\n\n    @staticmethod\n    def from_list_of_ints(array, label: str | None = None) -&gt; 'BoundingBox':\n        \"\"\"Create a bounding box from a list of integers.\"\"\"\n        if not isinstance(array, list):\n            raise ValueError(f'Model returned unexpected JSON structure for bounding box coordinates: {json.dumps(array)}')\n        for coordinate in array:\n            if not isinstance(coordinate, int):\n                raise ValueError(f'Model returned unrecognized JSON bounding box coordinate: {coordinate}')\n        return BoundingBox(top=array[0], left=array[1], bottom=array[2], right=array[3], label=label)\n</pre> class BoundingBox():     \"\"\"Create a BoundingBox in Gemini format (X and Y are on 0..1000 scale)\"\"\"      def __init__(self, top: int, left: int, bottom: int, right: int, label: str | None = None):          if None in [top, left, bottom, right]:             raise ValueError(f\"BoundingBox requires all coordinates to be set.\")         if top &lt; 0 or top &gt; 1000: raise ValueError(f'ymin must be an integer between 0 and 1000')         if left &lt; 0: raise ValueError(f'xmin must be an integer between 0 and 1000')         if bottom &lt; 0: raise ValueError(f'ymax must be an integer between 0 and 1000')         if right &lt; 0: raise ValueError(f'xmax must be an integer between 0 and 1000')         if right &lt;= left: raise ValueError(f'xmax must be greater than xmin (right={right}, left={left})')         if bottom &lt;= top: raise ValueError(f'ymax must be greater than ymin (bottom={bottom}, top={top})')         self.left = left         self.right = right         self.top = top         self.bottom = bottom         self.label = label         signature = label or f'{top}-{left}-{bottom}-{right}'         int_hash = int(hashlib.sha256(signature.encode('utf-8')).hexdigest(), 16) % (10 ** 8)         colors = list(ImageColor.colormap.keys())         colors = [color for color in colors if color != 'grey'] # Reserve grey for borders         stable_color_index = int_hash % len(colors)         self.color = colors[stable_color_index]         print(f'Stable color index: {stable_color_index} based on int_hash={int_hash}: {self.color}, {len(colors)} colors')      def __repr__(self):         \"\"\"Return a string representation of the bounding box.\"\"\"         return f'TLBR[{self.top}, {self.left}, {self.bottom}, {self.right}]: {self.label or \"\"} #{self.color}'      @staticmethod     def is_numeric(value: str) -&gt; bool:         \"\"\"Check if a string is a number.\"\"\"         return value.strip().lstrip('-').replace('.', '', 1).isdigit()      @staticmethod     def from_markdown(text: str) -&gt; Union['BoundingBox', None]:         \"\"\"Create a bounding box from a markdown string.\"\"\"         if not text:             return None         for line in text.strip().splitlines():             line = line.strip().lstrip('-').strip()             # Extract the numbers from the line after removing brackets and splitting by comma             if '[' in line and ']' in line:                 numbers = line.split('[')[1].split(']')[0].split(',')             else:                 numbers =  line.split(',')             if len(numbers) != 4:                 print(f'Skipping response line with {len(numbers)} comma separated parts instead of 4: {text}')                 continue             ints = [int(num.strip()) for num in numbers if BoundingBox.is_numeric(num)]             if len(ints) != 4:                 print(f'Skipping response line with only {len(numbers)} comma separated numbers instead of 4: {text}')                 continue             return BoundingBox(ints[0], ints[1], ints[2], ints[3]) # Using the first bounding box (even if the model returns multiple)         return None      @staticmethod     def from_list_of_ints(array, label: str | None = None) -&gt; 'BoundingBox':         \"\"\"Create a bounding box from a list of integers.\"\"\"         if not isinstance(array, list):             raise ValueError(f'Model returned unexpected JSON structure for bounding box coordinates: {json.dumps(array)}')         for coordinate in array:             if not isinstance(coordinate, int):                 raise ValueError(f'Model returned unrecognized JSON bounding box coordinate: {coordinate}')         return BoundingBox(top=array[0], left=array[1], bottom=array[2], right=array[3], label=label)   In\u00a0[\u00a0]: Copied! <pre>def read_image(local_path_to_image_file: str) -&gt; PILImage.Image:\n    \"\"\"\n    Reads an image from a local file path.\n\n    Args:\n        local_path_to_image_file: Path to the local image file.\n\n    Returns:\n        The image as a PIL Image object.\n    \"\"\"\n    return PILImage.open(local_path_to_image_file)\n\ndef encode_image_for_gemini(local_path_to_image_file: str):\n    \"\"\"\n    Encodes an image for Gemini.\n\n    Args:\n        local_path_to_image_file: Path to the local image file.\n\n    Returns:\n        The encoded image as a Part object.\n    \"\"\"\n    encoded_image = base64.b64encode(open(local_path_to_image_file, 'rb').read()).decode('utf-8')\n    return Part.from_data(data=base64.b64decode(encoded_image), mime_type='image/jpeg')\n\ndef strip_json_code_block(text: str) -&gt; str:\n    \"\"\"Strips the ```json code block markers from a string and returns the JSON.\n\n    Args:\n        text: The input string containing the JSON code block.\n\n    Returns:\n        The extracted JSON string.\n    \"\"\"\n    pattern = r\"```json\\s*(.*?)\\s*```\"  # Matches ```json ... ``` with optional whitespace\n    match = re.search(pattern, text, re.DOTALL)\n    if match:\n        return match.group(1).strip()\n    else:\n        return text  # Return original text if no code block is found\n\ndef download_image_from_gcs(gcs_uri: str, local_path: str) -&gt; None:\n    \"\"\"Downloads an image from Google Cloud Storage (GCS) to a local file.\n\n    Args:\n        gcs_uri: The GCS URI of the image to download (e.g., 'gs://bucket-name/image.jpg').\n        local_path: The local path where the downloaded image will be saved (e.g., './image.jpg').\n    \"\"\"\n    if not os.path.exists(local_path):\n        print(f'Local path to image file does not exist: {local_path}')\n        print(f'Downloading sample image from GCS...')\n        !gsutil cp \"{gcs_uri}\" \"{local_path}\"\n    else:\n        print(f'Image already exists at: {local_path}')\n</pre> def read_image(local_path_to_image_file: str) -&gt; PILImage.Image:     \"\"\"     Reads an image from a local file path.      Args:         local_path_to_image_file: Path to the local image file.      Returns:         The image as a PIL Image object.     \"\"\"     return PILImage.open(local_path_to_image_file)  def encode_image_for_gemini(local_path_to_image_file: str):     \"\"\"     Encodes an image for Gemini.      Args:         local_path_to_image_file: Path to the local image file.      Returns:         The encoded image as a Part object.     \"\"\"     encoded_image = base64.b64encode(open(local_path_to_image_file, 'rb').read()).decode('utf-8')     return Part.from_data(data=base64.b64decode(encoded_image), mime_type='image/jpeg')  def strip_json_code_block(text: str) -&gt; str:     \"\"\"Strips the ```json code block markers from a string and returns the JSON.      Args:         text: The input string containing the JSON code block.      Returns:         The extracted JSON string.     \"\"\"     pattern = r\"```json\\s*(.*?)\\s*```\"  # Matches ```json ... ``` with optional whitespace     match = re.search(pattern, text, re.DOTALL)     if match:         return match.group(1).strip()     else:         return text  # Return original text if no code block is found  def download_image_from_gcs(gcs_uri: str, local_path: str) -&gt; None:     \"\"\"Downloads an image from Google Cloud Storage (GCS) to a local file.      Args:         gcs_uri: The GCS URI of the image to download (e.g., 'gs://bucket-name/image.jpg').         local_path: The local path where the downloaded image will be saved (e.g., './image.jpg').     \"\"\"     if not os.path.exists(local_path):         print(f'Local path to image file does not exist: {local_path}')         print(f'Downloading sample image from GCS...')         !gsutil cp \"{gcs_uri}\" \"{local_path}\"     else:         print(f'Image already exists at: {local_path}')  In\u00a0[\u00a0]: Copied! <pre>def generate_bounding_boxes(\n    model_name: str,\n    system_instrs: str,\n    user_prompt: str,\n    local_path_to_image_file: str,\n    generation_config: Optional[dict] = None,\n    safety_settings: Optional[dict] = None\n) -&gt; list:\n    \"\"\"\n    Sends a message to Gemini and returns the model's response for bounding boxes.\n\n    Args:\n        model_name: The name of the generative model to use.\n        system_in: System-level instructions for the model.\n        user_message: The user's message to send to the model.\n        local_path_to_image_file: Path to the local image file.\n        generation_config: (Optional) Configuration for the model's generation process.\n        safety_settings: (Optional) Safety settings for the model.\n\n    Returns:\n        A list of bounding boxes, where each box is represented as a dictionary\n        with keys like 'x', 'y', 'width', 'height', and 'label'.\n\n    Raises:\n        ValueError: If the image file path is invalid or the model returns an unexpected response.\n    \"\"\"\n    if not os.path.exists(local_path_to_image_file):\n        raise ValueError(f'Local path to image file does not exist: {local_path_to_image_file}')\n\n    generation_config = generation_config or {\n        'temperature': 0.36,\n        'top_p': 1.0,\n        'top_k': 40,\n        'max_output_tokens': 8192,\n        'candidate_count': 1,\n    }\n    safety_settings = safety_settings or {\n        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n    }\n\n    model = GenerativeModel(\n        model_name=model_name,\n        system_instruction=system_instrs,\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n    )\n    response = model.generate_content(contents=[user_prompt, encode_image_for_gemini(local_path_to_image_file)], stream=False)\n    try:\n        bounding_boxes_list = json.loads(strip_json_code_block(response.text))\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Error decoding JSON response from model: {e}\")\n\n    if not isinstance(bounding_boxes_list, list):\n        raise ValueError(f'Model returned unexpected JSON structure instead of an array of objects: {json.dumps(bounding_boxes_list)}')\n\n    return bounding_boxes_list\n</pre> def generate_bounding_boxes(     model_name: str,     system_instrs: str,     user_prompt: str,     local_path_to_image_file: str,     generation_config: Optional[dict] = None,     safety_settings: Optional[dict] = None ) -&gt; list:     \"\"\"     Sends a message to Gemini and returns the model's response for bounding boxes.      Args:         model_name: The name of the generative model to use.         system_in: System-level instructions for the model.         user_message: The user's message to send to the model.         local_path_to_image_file: Path to the local image file.         generation_config: (Optional) Configuration for the model's generation process.         safety_settings: (Optional) Safety settings for the model.      Returns:         A list of bounding boxes, where each box is represented as a dictionary         with keys like 'x', 'y', 'width', 'height', and 'label'.      Raises:         ValueError: If the image file path is invalid or the model returns an unexpected response.     \"\"\"     if not os.path.exists(local_path_to_image_file):         raise ValueError(f'Local path to image file does not exist: {local_path_to_image_file}')      generation_config = generation_config or {         'temperature': 0.36,         'top_p': 1.0,         'top_k': 40,         'max_output_tokens': 8192,         'candidate_count': 1,     }     safety_settings = safety_settings or {         HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,         HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,         HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,         HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,     }      model = GenerativeModel(         model_name=model_name,         system_instruction=system_instrs,         generation_config=generation_config,         safety_settings=safety_settings,     )     response = model.generate_content(contents=[user_prompt, encode_image_for_gemini(local_path_to_image_file)], stream=False)     try:         bounding_boxes_list = json.loads(strip_json_code_block(response.text))     except json.JSONDecodeError as e:         raise ValueError(f\"Error decoding JSON response from model: {e}\")      if not isinstance(bounding_boxes_list, list):         raise ValueError(f'Model returned unexpected JSON structure instead of an array of objects: {json.dumps(bounding_boxes_list)}')      return bounding_boxes_list In\u00a0[\u00a0]: Copied! <pre>def plot_bounding_boxes(\n        image: PILImage.Image, bounding_boxes: list\n    ) -&gt; PILImage.Image:\n        \"\"\"\n        Overlays bounding boxes on an image with random colors and optional labels.\n        BoundingBoxes in Gemini format (X and Y are on 0..1000 scale) are auto scaled to the actual image size.\n\n        Args:\n            image: source image\n            bounding_boxes: a list with one or more bounding boxes\n\n        Returns: a new PIL Image object with bounding boxes\n        \"\"\"\n        if not image:\n            raise ValueError(\"image is required\")\n        if not bounding_boxes:\n            return image\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default(size=15)\n        for bb in bounding_boxes:\n            # Convert Gemini coordinates from Gemini scale (0..1000) to absolute pixels\n            left = int(bb.left / 1000 * image.width)\n            top = int(bb.top / 1000 * image.height)\n            right = int(bb.right / 1000 * image.width)\n            bottom = int(bb.bottom / 1000 * image.height)\n            if right &lt; left + 8:\n                right = left + 8 # we cannot fit the border\n            if bottom &lt; top + 8:\n                bottom = top + 8\n            # Border line style: 1 pixel grey + 3 pixels box specific color + 1 pixel grey\n            draw.rectangle(((left, top), (right, bottom)), outline=ImageColor.colormap['grey'], width=1)\n            draw.rectangle(((left+1, top+1), (right-1, bottom-1)), outline=bb.color, width=3)\n            draw.rectangle(((left+4, top+4), (right-4, bottom-4)), outline=ImageColor.colormap['grey'], width=1)\n            if bb.label:\n                # Check if the label fits inside of the bounding box\n                label_left, label_top, label_right, label_bottom = font.getbbox(bb.label)\n                print(f'label coordinates: label_left={label_left}, label_top={label_top}, label_right={label_right}, label_bottom={label_bottom}')\n                is_box_wide_enough = (label_right - label_left) + 8 &lt; (right - left)\n                is_box_tall_enough = (label_bottom - label_top) + 6 &lt; (bottom - top)\n                print(f'{bb} is_box_wide_enough={is_box_wide_enough}, is_box_tall_enough={is_box_tall_enough}')\n                if is_box_wide_enough and is_box_tall_enough:\n                    label_offset_x = 7\n                    label_offset_y = 3\n                else: # Print the label below the bounding box\n                    label_offset_x = 0\n                    label_offset_y = bottom - top\n                    print(f'label_offset_y={label_offset_y}')\n                draw.text((left + label_offset_x, top + label_offset_y), bb.label, fill=ImageColor.colormap['red'], font=font)\n        return image\n\n\ndef render_predicted_bounding_boxes(predictions: list, source_image_path: str, result_image_path: str) -&gt; None:\n    \"\"\"Renders predicted bounding boxes onto an image and saves the result.\n\n    This function takes a list of predictions from a model,\n    reads the source image, extracts bounding box information\n    from the predictions, overlays the boxes onto the image,\n    and saves the resulting image to the specified path.\n\n    Args:\n        predictions: A list of predictions from the model, expected to\n                     contain bounding box data and optional labels.\n        source_image_path: The path to the source image file.\n        result_image_path: The path where the resulting image\n                           with bounding boxes will be saved.\n\n    Raises:\n        ValueError: If the predictions are not in the expected format,\n                    or if required attributes are missing.\n\n    Returns:\n        None. The function saves the resulting image to the\n        specified path.\n    \"\"\"\n    image = read_image(source_image_path)\n    boxes: list[BoundingBox] = []\n    if not isinstance(predictions, list):\n        raise ValueError(f'Model returned unexpected JSON structure instead of an array of objects: {json.dumps(predictions)}')\n    for item in predictions:\n        if not isinstance(item, dict):\n            raise ValueError(f'Model returned unexpected array item: {json.dumps(item)}')\n        if 'box_2d' not in item:\n            raise ValueError(f'Model returned bounding box with missing attribute \"box_2d\": {json.dumps(item)}')\n        label = item['label'] if 'label' in item else None\n        box_2d = item['box_2d']\n        if not isinstance(box_2d, list):\n            raise ValueError(f'Model returned unexpected box_2d value instead of an array: {json.dumps(box_2d)}')\n        boxes.append(BoundingBox.from_list_of_ints(box_2d, label=label))\n    class_labels = {box.label for box in boxes}\n    if len(class_labels) == 1: # no need to display identical labels\n        for box in boxes:\n            box.label = None\n    result = plot_bounding_boxes(image, boxes)\n    result.save(result_image_path)\n</pre> def plot_bounding_boxes(         image: PILImage.Image, bounding_boxes: list     ) -&gt; PILImage.Image:         \"\"\"         Overlays bounding boxes on an image with random colors and optional labels.         BoundingBoxes in Gemini format (X and Y are on 0..1000 scale) are auto scaled to the actual image size.          Args:             image: source image             bounding_boxes: a list with one or more bounding boxes          Returns: a new PIL Image object with bounding boxes         \"\"\"         if not image:             raise ValueError(\"image is required\")         if not bounding_boxes:             return image         draw = ImageDraw.Draw(image)         font = ImageFont.load_default(size=15)         for bb in bounding_boxes:             # Convert Gemini coordinates from Gemini scale (0..1000) to absolute pixels             left = int(bb.left / 1000 * image.width)             top = int(bb.top / 1000 * image.height)             right = int(bb.right / 1000 * image.width)             bottom = int(bb.bottom / 1000 * image.height)             if right &lt; left + 8:                 right = left + 8 # we cannot fit the border             if bottom &lt; top + 8:                 bottom = top + 8             # Border line style: 1 pixel grey + 3 pixels box specific color + 1 pixel grey             draw.rectangle(((left, top), (right, bottom)), outline=ImageColor.colormap['grey'], width=1)             draw.rectangle(((left+1, top+1), (right-1, bottom-1)), outline=bb.color, width=3)             draw.rectangle(((left+4, top+4), (right-4, bottom-4)), outline=ImageColor.colormap['grey'], width=1)             if bb.label:                 # Check if the label fits inside of the bounding box                 label_left, label_top, label_right, label_bottom = font.getbbox(bb.label)                 print(f'label coordinates: label_left={label_left}, label_top={label_top}, label_right={label_right}, label_bottom={label_bottom}')                 is_box_wide_enough = (label_right - label_left) + 8 &lt; (right - left)                 is_box_tall_enough = (label_bottom - label_top) + 6 &lt; (bottom - top)                 print(f'{bb} is_box_wide_enough={is_box_wide_enough}, is_box_tall_enough={is_box_tall_enough}')                 if is_box_wide_enough and is_box_tall_enough:                     label_offset_x = 7                     label_offset_y = 3                 else: # Print the label below the bounding box                     label_offset_x = 0                     label_offset_y = bottom - top                     print(f'label_offset_y={label_offset_y}')                 draw.text((left + label_offset_x, top + label_offset_y), bb.label, fill=ImageColor.colormap['red'], font=font)         return image   def render_predicted_bounding_boxes(predictions: list, source_image_path: str, result_image_path: str) -&gt; None:     \"\"\"Renders predicted bounding boxes onto an image and saves the result.      This function takes a list of predictions from a model,     reads the source image, extracts bounding box information     from the predictions, overlays the boxes onto the image,     and saves the resulting image to the specified path.      Args:         predictions: A list of predictions from the model, expected to                      contain bounding box data and optional labels.         source_image_path: The path to the source image file.         result_image_path: The path where the resulting image                            with bounding boxes will be saved.      Raises:         ValueError: If the predictions are not in the expected format,                     or if required attributes are missing.      Returns:         None. The function saves the resulting image to the         specified path.     \"\"\"     image = read_image(source_image_path)     boxes: list[BoundingBox] = []     if not isinstance(predictions, list):         raise ValueError(f'Model returned unexpected JSON structure instead of an array of objects: {json.dumps(predictions)}')     for item in predictions:         if not isinstance(item, dict):             raise ValueError(f'Model returned unexpected array item: {json.dumps(item)}')         if 'box_2d' not in item:             raise ValueError(f'Model returned bounding box with missing attribute \"box_2d\": {json.dumps(item)}')         label = item['label'] if 'label' in item else None         box_2d = item['box_2d']         if not isinstance(box_2d, list):             raise ValueError(f'Model returned unexpected box_2d value instead of an array: {json.dumps(box_2d)}')         boxes.append(BoundingBox.from_list_of_ints(box_2d, label=label))     class_labels = {box.label for box in boxes}     if len(class_labels) == 1: # no need to display identical labels         for box in boxes:             box.label = None     result = plot_bounding_boxes(image, boxes)     result.save(result_image_path)  In\u00a0[\u00a0]: Copied! <pre>SYSTEM_INSTRUCTIONS = '''\nReturn bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects.\nIf an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..).\n'''\n\nPROMPT_SINGLE_OBJECT = 'Could you display the bounding boxes around the Ferris wheel.'\n\nPROMPT_SINGLE_CLASS = 'Give me the bounding boxes for all the kites in the park.'\n\nPROMPT_MULTIPLE_CLASSES = 'What are the regions defined by the bounding boxes for two types of animals: cats and dogs.'\n</pre> SYSTEM_INSTRUCTIONS = ''' Return bounding boxes as a JSON array with labels. Never return masks or code fencing. Limit to 25 objects. If an object is present multiple times, name them according to their unique characteristic (colors, size, position, unique characteristics, etc..). '''  PROMPT_SINGLE_OBJECT = 'Could you display the bounding boxes around the Ferris wheel.'  PROMPT_SINGLE_CLASS = 'Give me the bounding boxes for all the kites in the park.'  PROMPT_MULTIPLE_CLASSES = 'What are the regions defined by the bounding boxes for two types of animals: cats and dogs.' In\u00a0[\u00a0]: Copied! <pre>model_name = MODEL_NAME\nGCS_IMAGE_URI = 'gs://public-aaie-genai-samples/gemini_2_0/spatial_understanding/park.jpg'\nlocal_image_path = './park.jpg'\ndownload_image_from_gcs(GCS_IMAGE_URI, local_image_path)\nresults = generate_bounding_boxes(model_name,\n                                  SYSTEM_INSTRUCTIONS,\n                                  PROMPT_SINGLE_CLASS,\n                                  local_path_to_image_file=local_image_path)\nprint(results)\n</pre> model_name = MODEL_NAME GCS_IMAGE_URI = 'gs://public-aaie-genai-samples/gemini_2_0/spatial_understanding/park.jpg' local_image_path = './park.jpg' download_image_from_gcs(GCS_IMAGE_URI, local_image_path) results = generate_bounding_boxes(model_name,                                   SYSTEM_INSTRUCTIONS,                                   PROMPT_SINGLE_CLASS,                                   local_path_to_image_file=local_image_path) print(results) In\u00a0[\u00a0]: Copied! <pre># @title Download Defaut Image for app\nGCS_IMAGE_URI = 'gs://public-aaie-genai-samples/gemini_2_0/spatial_understanding/park.jpg'\nDEFAULT_IMAGE_PATH = './park.jpg'\ndownload_image_from_gcs(GCS_IMAGE_URI, DEFAULT_IMAGE_PATH)\n</pre> # @title Download Defaut Image for app GCS_IMAGE_URI = 'gs://public-aaie-genai-samples/gemini_2_0/spatial_understanding/park.jpg' DEFAULT_IMAGE_PATH = './park.jpg' download_image_from_gcs(GCS_IMAGE_URI, DEFAULT_IMAGE_PATH) In\u00a0[\u00a0]: Copied! <pre># @title Image Processor\ndef process_image(file_name: str, user_prompt: str = PROMPT_SINGLE_CLASS):\n    \"\"\"\n    Takes an input image uploaded from local disk, uploads it to colab and processes it\n    \"\"\"\n    try:\n        current_dir = os.getcwd()\n        base_name = os.path.basename(file_name)\n        save_path = os.path.join(current_dir, base_name)\n        image = PILImage.open(file_name)\n        image.save(save_path)\n        message = f'Image saved as {save_path} in the current directory.'\n        print(message)\n        try:\n            results = generate_bounding_boxes(MODEL_NAME, SYSTEM_INSTRUCTIONS, user_prompt=user_prompt, local_path_to_image_file=save_path)\n        except Exception as e:\n            error_message = f\"Error generating bounding boxes: {e}\"\n            raise gr.Error(error_message)\n        bb_save_path = os.path.join(current_dir, f'{base_name}_bb.jpg')\n        render_predicted_bounding_boxes(results, save_path, bb_save_path)\n        return PILImage.open(bb_save_path)\n    except FileNotFoundError:\n        raise gr.Error(f\"Error: Image file not found at {file_name}\")\n    except UnidentifiedImageError:\n        raise gr.Error(f\"Error: Could not open or read image file {file_name}\")\n    except Exception as e:  # Catch any other unexpected errors\n        raise gr.Error(f\"An unexpected error occurred during processing: {e}\")\n</pre> # @title Image Processor def process_image(file_name: str, user_prompt: str = PROMPT_SINGLE_CLASS):     \"\"\"     Takes an input image uploaded from local disk, uploads it to colab and processes it     \"\"\"     try:         current_dir = os.getcwd()         base_name = os.path.basename(file_name)         save_path = os.path.join(current_dir, base_name)         image = PILImage.open(file_name)         image.save(save_path)         message = f'Image saved as {save_path} in the current directory.'         print(message)         try:             results = generate_bounding_boxes(MODEL_NAME, SYSTEM_INSTRUCTIONS, user_prompt=user_prompt, local_path_to_image_file=save_path)         except Exception as e:             error_message = f\"Error generating bounding boxes: {e}\"             raise gr.Error(error_message)         bb_save_path = os.path.join(current_dir, f'{base_name}_bb.jpg')         render_predicted_bounding_boxes(results, save_path, bb_save_path)         return PILImage.open(bb_save_path)     except FileNotFoundError:         raise gr.Error(f\"Error: Image file not found at {file_name}\")     except UnidentifiedImageError:         raise gr.Error(f\"Error: Could not open or read image file {file_name}\")     except Exception as e:  # Catch any other unexpected errors         raise gr.Error(f\"An unexpected error occurred during processing: {e}\")  In\u00a0[\u00a0]: Copied! <pre># @title Main Gradio application\nwith gr.Blocks(title=\"BoxIt With Gemini 2.0\") as demo:\n    gr.Markdown('# **BoxIt**')\n\n    with gr.Row():\n        image_display = gr.Image(type='filepath', label='Image', value=DEFAULT_IMAGE_PATH)\n\n        with gr.Column():\n            prompt_type = gr.Radio(\n                choices=['Predefined Prompts', 'Custom Prompt'],\n                label='Select Prompt Style',\n            )\n            predefined_prompts = [PROMPT_SINGLE_OBJECT, PROMPT_SINGLE_CLASS, PROMPT_MULTIPLE_CLASSES]\n            prompt_dropdown = gr.Dropdown(\n                choices=predefined_prompts,\n                label='Choose a predefined prompt:',\n                visible=False,\n            )\n            custom_prompt = gr.Textbox(\n                lines=2,\n                label='Enter your prompt',\n                visible=False,\n            )\n            submit_btn = gr.Button('Find Bounding Boxes')\n            # bounding_box_output = gr.Textbox(label=\"Bounding Boxes\", visible=False)\n\n    original_image = gr.State(DEFAULT_IMAGE_PATH)  # Store the *original* uploaded image\n\n    def toggle_prompt_input(prompt_type, original_img):\n        if original_img is not None:\n            if prompt_type == 'Predefined Prompts':\n                return gr.update(visible=True), gr.update(visible=False), original_img\n            elif prompt_type == 'Custom Prompt':\n                return gr.update(visible=False), gr.update(visible=True), original_img\n            else:  # \"Select Prompt Style\"\n                return gr.update(visible=False), gr.update(visible=False), original_img\n        else:\n            return gr.update(visible=False), gr.update(visible=False), gr.update()\n\n    prompt_type.change(\n        fn=toggle_prompt_input,\n        inputs=[prompt_type, original_image],\n        outputs=[prompt_dropdown, custom_prompt, image_display],\n    )\n\n    def process_and_display(img, prompt_type, selected_prompt, custom_prompt):\n        if not img:\n            return gr.update()\n\n        if prompt_type == 'Predefined Prompts':\n            prompt = selected_prompt\n        elif prompt_type == 'Custom Prompt':\n            prompt = custom_prompt\n        else:  # \"Select Prompt Style\" - Do nothing\n            return img\n        print(f'Prompt: {prompt}')\n        print(f'Processing image: {img}')\n        try:\n            processed_image = process_image(file_name=img, user_prompt=prompt)\n            return processed_image\n        except Exception as e:\n            print(f'Error processing image: {e}')\n            return img  # Return the original image in case of an error\n\n    image_display.upload(lambda x: x, inputs=image_display, outputs=original_image)\n\n    submit_btn.click(\n        fn=process_and_display,\n        inputs=[original_image, prompt_type, prompt_dropdown, custom_prompt],\n        outputs=image_display\n    )\n\n    demo.queue()\n    demo.launch(quiet=True)\n</pre> # @title Main Gradio application with gr.Blocks(title=\"BoxIt With Gemini 2.0\") as demo:     gr.Markdown('# **BoxIt**')      with gr.Row():         image_display = gr.Image(type='filepath', label='Image', value=DEFAULT_IMAGE_PATH)          with gr.Column():             prompt_type = gr.Radio(                 choices=['Predefined Prompts', 'Custom Prompt'],                 label='Select Prompt Style',             )             predefined_prompts = [PROMPT_SINGLE_OBJECT, PROMPT_SINGLE_CLASS, PROMPT_MULTIPLE_CLASSES]             prompt_dropdown = gr.Dropdown(                 choices=predefined_prompts,                 label='Choose a predefined prompt:',                 visible=False,             )             custom_prompt = gr.Textbox(                 lines=2,                 label='Enter your prompt',                 visible=False,             )             submit_btn = gr.Button('Find Bounding Boxes')             # bounding_box_output = gr.Textbox(label=\"Bounding Boxes\", visible=False)      original_image = gr.State(DEFAULT_IMAGE_PATH)  # Store the *original* uploaded image      def toggle_prompt_input(prompt_type, original_img):         if original_img is not None:             if prompt_type == 'Predefined Prompts':                 return gr.update(visible=True), gr.update(visible=False), original_img             elif prompt_type == 'Custom Prompt':                 return gr.update(visible=False), gr.update(visible=True), original_img             else:  # \"Select Prompt Style\"                 return gr.update(visible=False), gr.update(visible=False), original_img         else:             return gr.update(visible=False), gr.update(visible=False), gr.update()      prompt_type.change(         fn=toggle_prompt_input,         inputs=[prompt_type, original_image],         outputs=[prompt_dropdown, custom_prompt, image_display],     )      def process_and_display(img, prompt_type, selected_prompt, custom_prompt):         if not img:             return gr.update()          if prompt_type == 'Predefined Prompts':             prompt = selected_prompt         elif prompt_type == 'Custom Prompt':             prompt = custom_prompt         else:  # \"Select Prompt Style\" - Do nothing             return img         print(f'Prompt: {prompt}')         print(f'Processing image: {img}')         try:             processed_image = process_image(file_name=img, user_prompt=prompt)             return processed_image         except Exception as e:             print(f'Error processing image: {e}')             return img  # Return the original image in case of an error      image_display.upload(lambda x: x, inputs=image_display, outputs=original_image)      submit_btn.click(         fn=process_and_display,         inputs=[original_image, prompt_type, prompt_dropdown, custom_prompt],         outputs=image_display     )      demo.queue()     demo.launch(quiet=True)"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#copyright-2024-google-llc","title":"Copyright 2024 Google LLC.\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#explore-object-detection-with-gemini-20-in-vertex-ai","title":"Explore Object Detection with Gemini 2.0 in Vertex AI\u00b6","text":"<p>This notebook showcases the power of Gemini 2.0 for object detection and spatial understanding using Vertex AI.</p> <p>Using the embedded app, you'll discover how to leverage Gemini to accurately identify and locate objects in images, similar to the example shown below.</p> <p></p> <p>Feel free to explore different prompt styles to achieve the desired results. You can start with the pre-defined prompts and image provided, or personalize your experience by uploading your own images and switching to 'Custom Prompt' to craft your own.</p> <p>IMPORTANT NOTICE: This notebook only showcases functionality available in model name <code>gemini-2.0-flash-exp</code></p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#environment-setup-gcp-and-libraries","title":"Environment Setup: GCP and Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#install-packages-and-restart-runtime-if-needed","title":"Install Packages and Restart Runtime (If needed)\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel. It confirms if the right packages are already installed and restarts the runtime if needed.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#authentication","title":"Authentication\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#leveraging-gemini-for-bounding-boxes","title":"Leveraging Gemini for Bounding Boxes\u00b6","text":"<p>This section demonstrates how to utilize the power of Google's Gemini model to identify and extract bounding boxes of objects within images. We'll explore prompt engineering techniques to guide Gemini in accurately detecting desired elements, and then visualize the results by overlaying the bounding boxes onto the original image. This showcases Gemini's capabilities for object detection and its potential applications in various computer vision tasks.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#bounding-box-data-class","title":"Bounding Box Data Class\u00b6","text":"<p>Define a object that represents a bounding box with 4 coordinates in Gemini format where X and Y are on the 0 to 1000 scale</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#utilities-preprocessing-and-postprocessing","title":"Utilities - Preprocessing and Postprocessing\u00b6","text":"<p>Some helper functions for preprocessing and postprocessing</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#generating-bounding-boxes-with-gemini","title":"Generating Bounding Boxes with Gemini\u00b6","text":"<p>This section focuses on generating bounding boxes using the Gemini model. It involves the following steps:</p> <ol> <li>Defining the <code>generate_bounding_boxes</code> function: This function handles the interaction with the Gemini API, sending the image and prompt and receiving the predicted bounding boxes.</li> <li>Setting Generation Parameters: We'll define parameters like temperature and top_p to control the model's creativity and diversity.</li> <li>Generating Results: We'll use the defined function and parameters to obtain bounding box predictions from Gemini for a given image.</li> </ol>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#utilities-visualizing-bounding-boxes-with-plotting-and-rendering","title":"Utilities - Visualizing Bounding Boxes with Plotting and Rendering\u00b6","text":"<p>This section introduces a set of utility functions designed to visualize the bounding boxes identified by Gemini. These functions handle tasks such as reading images, plotting bounding boxes with distinct colors and labels, and rendering the final output with overlaid boxes onto the source image. These utilities streamline the process of visualizing object detection results and provide a clear representation of Gemini's capabilities.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#prompt-templates","title":"Prompt Templates\u00b6","text":""},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#testing-the-bounding-box-generation","title":"Testing the Bounding Box Generation\u00b6","text":"<p>Now that we've defined the <code>generate_bounding_boxes</code> function and set the generation parameters, let's test it with a sample image and prompt. This will help us verify that the model is correctly identifying and returning bounding boxes.</p> <p>NOTE: The test on the next cell assumes you have uploaded a sample image to the Colab filesystem and updated <code>sample_image_path</code> with the correct file name below. For the purposes of a seamless experiment, we've uploaded a sample image to GCS.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#interactive-visualization-with-gradio","title":"Interactive Visualization with Gradio\u00b6","text":"<p>This section integrates bounding box generation into a Gradio application, enabling you to interactively visualize object detection results on uploaded images.</p> <p>Predefined Prompts:</p> <p>Start by exploring object detection with our predefined prompts using the provided sample image.</p> <p>Custom Prompts:</p> <p>Switch to \"Custom Prompt\" to unlock the full potential of Gemini.  Experiment with your own prompts, such as precisely locating specific objects within an image and retrieving their bounding box information. For example, you can try prompts like \"Find the red car\" or \"Where are the bicycles?\". Feel free to upload your own images and tailor your prompts for personalized exploration.</p>"},{"location":"genai-on-vertex-ai/gemini/prompting_recipes/spatial_reasoning/spatial_reasoning_app_for_gemini2/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrates how to use Google's Gemini model to identify and extract bounding boxes of elements within an image. It covers the following key aspects:</p> <ol> <li>Environment Setup: Setting up the Google Cloud Project, installing necessary libraries, and authenticating with Google Cloud.</li> <li>Leveraging Gemini for Bounding Boxes: Utilizing the Gemini model to generate bounding boxes for specific objects or classes within an image.</li> <li>Prompt Engineering: Defining prompt templates to guide the Gemini model in accurately detecting the desired elements.</li> <li>Bounding Box Data Class: Defining a class to represent bounding boxes in the Gemini format, along with methods for parsing and processing them.</li> <li>Utilities: Helper functions for reading, encoding, and plotting bounding boxes onto images.</li> <li>Visualizing Bounding Boxes: Displaying the predicted bounding boxes overlaid on the source image for clear visualization.</li> <li>Interactive Interface: Building a Gradio interface to allow users to upload images, generate bounding boxes, and visualize the results interactively.</li> </ol> <p>The notebook showcases the power of Gemini for object detection tasks and provides a practical example of its potential applications in computer vision. It offers a comprehensive guide to using Gemini, from setup to interactive visualization.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/","title":"Retrieval Augmented Generation (RAG)","text":"<p>This folder contains code examples and notebooks for building RAG applications. </p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/#notebooks","title":"Notebooks","text":"<ul> <li>Build your own Grounded RAG application using Vertex AI APIs for RAG and Langchain - Learn how to use Vertex AI Builder APIs for RAG to build a custom grounded RAG application on your own documents.</li> </ul>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/","title":"Build your own Grounded RAG application using Vertex AI APIs for RAG and Langchain","text":"Author(s) Abhishek Bhagwat, Rajesh Thallam Reviewers(s) Alan Blount, Holt Skinner, Skander Hannachi Last updated 2024-06-18 In\u00a0[\u00a0]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. In\u00a0[\u00a0]: Copied! <pre>! pip install google-cloud-aiplatform --upgrade --quiet\n! pip install google-cloud-discoveryengine --upgrade --quiet\n! pip install google-cloud-documentai google-cloud-documentai-toolbox --upgrade --quiet\n! pip install google-cloud-storage --upgrade --quiet\n\n! pip install langchain-google-community --upgrade --quiet\n! pip install langchain-google-vertexai --upgrade --quiet\n! pip install langchain-google-community[vertexaisearch] --upgrade --quiet\n! pip install langchain-google-community[docai] --upgrade --quiet\n\n! pip install rich --upgrade --quiet\n</pre> ! pip install google-cloud-aiplatform --upgrade --quiet ! pip install google-cloud-discoveryengine --upgrade --quiet ! pip install google-cloud-documentai google-cloud-documentai-toolbox --upgrade --quiet ! pip install google-cloud-storage --upgrade --quiet  ! pip install langchain-google-community --upgrade --quiet ! pip install langchain-google-vertexai --upgrade --quiet ! pip install langchain-google-community[vertexaisearch] --upgrade --quiet ! pip install langchain-google-community[docai] --upgrade --quiet  ! pip install rich --upgrade --quiet <pre>   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.5/2.5 MB 22.1 MB/s eta 0:00:0000:0100:01\n  Preparing metadata (setup.py) ... done\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 319.0/319.0 kB 6.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43.1/43.1 kB 2.3 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 467.5/467.5 kB 21.4 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.4/2.4 MB 55.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.3/38.3 MB 14.6 MB/s eta 0:00:00\n  Building wheel for intervaltree (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.6.1 requires pyarrow&lt;16.2.0a0,&gt;=16.1.0, but you have pyarrow 15.0.2 which is incompatible.\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 130.5/130.5 kB 2.4 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.4/50.4 kB 3.3 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.0/77.0 kB 4.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.4/2.4 MB 34.4 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 404.4/404.4 kB 23.5 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 33.8 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 295.8/295.8 kB 10.3 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.4/76.4 kB 3.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.0/78.0 kB 3.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 49.3/49.3 kB 2.7 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 141.9/141.9 kB 7.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.5/54.5 kB 3.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 kB 3.7 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 88.6/88.6 kB 2.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.2/2.2 MB 20.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.2/77.2 kB 3.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 157.3/157.3 kB 6.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 272.5/272.5 kB 3.3 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 kB 2.4 MB/s eta 0:00:00\n  Building wheel for gapic-google-longrunning (setup.py) ... done\n  Building wheel for google-gax (setup.py) ... done\n  Building wheel for ply (setup.py) ... done\n  Building wheel for oauth2client (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npydrive 1.3.1 requires oauth2client&gt;=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\npydrive2 1.20.0 requires oauth2client&gt;=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Restart kernel after installs so that your environment can access the new packages\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> # Restart kernel after installs so that your environment can access the new packages import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) Out[\u00a0]: <pre>{'status': 'ok', 'restart': True}</pre> \u26a0\ufe0f The kernel is going to restart. Please wait until it is finished before continuing to the next step. \u26a0\ufe0f In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") <pre>Authenticated\n</pre> In\u00a0[\u00a0]: Copied! <pre>import vertexai\nfrom google.cloud import documentai\nfrom google.cloud import discoveryengine\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"  # @param {type:\"string\"}\n\nvertexai.init(project=PROJECT_ID, location=REGION)\nprint(f\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\nprint(f\"Document AI API version = {documentai.__version__}\")\nprint(f\"Discovery Engine API version = {discoveryengine.__version__}\")\n</pre> import vertexai from google.cloud import documentai from google.cloud import discoveryengine  PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} REGION = \"us-central1\"  # @param {type:\"string\"}  vertexai.init(project=PROJECT_ID, location=REGION) print(f\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") print(f\"Document AI API version = {documentai.__version__}\") print(f\"Discovery Engine API version = {discoveryengine.__version__}\") <pre>Vertex AI SDK initialized.\nVertex AI SDK version = 1.70.0\nDocument AI API version = 2.33.0\nDiscovery Engine API version = 0.11.14\n</pre> In\u00a0[\u00a0]: Copied! <pre># Cloud storage buckets\nGCS_BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\nGCS_OUTPUT_PATH = f\"{GCS_BUCKET_URI}\"  # DocAI Layout Parser Output Path\nGCS_BUCKET_NAME = GCS_BUCKET_URI.replace(\"gs://\", \"\")\n\n# Vertex AI Vector Search\n# parameter description here\n# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index\nVS_INDEX_NAME = \"[your-index-name]\"  # @param {type:\"string\"}\nVS_INDEX_ENDPOINT_NAME = \"[your-index-endpoint-name]\"  # @param {type:\"string\"}\nVS_CONTENTS_DELTA_URI = f\"{GCS_BUCKET_URI}/index/embeddings\"\nVS_DIMENSIONS = 768\nVS_APPROX_NEIGHBORS = 150\nVS_INDEX_UPDATE_METHOD = \"STREAM_UPDATE\"\nVS_INDEX_SHARD_SIZE = \"SHARD_SIZE_SMALL\"\nVS_LEAF_NODE_EMB_COUNT = 500\nVS_LEAF_SEARCH_PERCENT = 80\nVS_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\"\nVS_MACHINE_TYPE = \"e2-standard-16\"\nVS_MIN_REPLICAS = 1\nVS_MAX_REPLICAS = 1\nVS_DESCRIPTION = \"Index for DIY RAG with Vertex AI APIs\"  # @param {type:\"string\"}\n\n# Models\nEMBEDDINGS_MODEL_NAME = \"text-embedding-004\"\nLLM_MODEL_NAME = \"gemini-2.0-flash-001\"\n\n# DocumentAI Processor\nDOCAI_LOCATION = \"us\"  # @param [\"us\", \"eu\"]\nDOCAI_PROCESSOR_NAME = \"[your-docai-processor-name]\"  # @param {type:\"string\"}\n\n# Enable/disable flags\n# flag to create Google Cloud resources configured above\n# refer to the notes before this cell\nCREATE_RESOURCES = False  # @param {type:\"boolean\"}\n# flag to run data ingestion\nRUN_INGESTION = True  # @param {type:\"boolean\"}\n</pre> # Cloud storage buckets GCS_BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"} GCS_OUTPUT_PATH = f\"{GCS_BUCKET_URI}\"  # DocAI Layout Parser Output Path GCS_BUCKET_NAME = GCS_BUCKET_URI.replace(\"gs://\", \"\")  # Vertex AI Vector Search # parameter description here # https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index VS_INDEX_NAME = \"[your-index-name]\"  # @param {type:\"string\"} VS_INDEX_ENDPOINT_NAME = \"[your-index-endpoint-name]\"  # @param {type:\"string\"} VS_CONTENTS_DELTA_URI = f\"{GCS_BUCKET_URI}/index/embeddings\" VS_DIMENSIONS = 768 VS_APPROX_NEIGHBORS = 150 VS_INDEX_UPDATE_METHOD = \"STREAM_UPDATE\" VS_INDEX_SHARD_SIZE = \"SHARD_SIZE_SMALL\" VS_LEAF_NODE_EMB_COUNT = 500 VS_LEAF_SEARCH_PERCENT = 80 VS_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\" VS_MACHINE_TYPE = \"e2-standard-16\" VS_MIN_REPLICAS = 1 VS_MAX_REPLICAS = 1 VS_DESCRIPTION = \"Index for DIY RAG with Vertex AI APIs\"  # @param {type:\"string\"}  # Models EMBEDDINGS_MODEL_NAME = \"text-embedding-004\" LLM_MODEL_NAME = \"gemini-2.0-flash-001\"  # DocumentAI Processor DOCAI_LOCATION = \"us\"  # @param [\"us\", \"eu\"] DOCAI_PROCESSOR_NAME = \"[your-docai-processor-name]\"  # @param {type:\"string\"}  # Enable/disable flags # flag to create Google Cloud resources configured above # refer to the notes before this cell CREATE_RESOURCES = False  # @param {type:\"boolean\"} # flag to run data ingestion RUN_INGESTION = True  # @param {type:\"boolean\"} In\u00a0[\u00a0]: Copied! <pre># @title Document AI LangChain Integration\n\"\"\"Module contains a PDF parser based on Document AI from Google Cloud.\n\nYou need to install two libraries to use this parser:\npip install google-cloud-documentai\npip install google-cloud-documentai-toolbox\n\"\"\"\n\nimport logging\nimport re\nimport time\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Iterator, List, Optional, Sequence\n\nfrom langchain_core.document_loaders import BaseBlobParser\nfrom langchain_core.document_loaders.blob_loaders import Blob\nfrom langchain_core.documents import Document\nfrom langchain_core.utils.iter import batch_iterate\n\nfrom langchain_google_community._utils import get_client_info\n\nif TYPE_CHECKING:\n    from google.api_core.operation import Operation  # type: ignore[import]\n    from google.cloud.documentai import (  # type: ignore[import]\n        DocumentProcessorServiceClient,\n    )\n\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass DocAIParsingResults:\n    \"\"\"A dataclass to store Document AI parsing results.\"\"\"\n\n    source_path: str\n    parsed_path: str\n\n\nclass DocAIParser(BaseBlobParser):\n    \"\"\"`Google Cloud Document AI` parser.\n\n    For a detailed explanation of Document AI, refer to the product documentation.\n    https://cloud.google.com/document-ai/docs/overview\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        client: Optional[\"DocumentProcessorServiceClient\"] = None,\n        project_id: Optional[str] = None,\n        location: Optional[str] = None,\n        gcs_output_path: Optional[str] = None,\n        processor_name: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the parser.\n\n        Args:\n            client: a DocumentProcessorServiceClient to use\n            location: a Google Cloud location where a Document AI processor is located\n            gcs_output_path: a path on Google Cloud Storage to store parsing results\n            processor_name: full resource name of a Document AI processor or processor\n                version\n\n        You should provide either a client or location (and then a client\n            would be instantiated).\n        \"\"\"\n\n        if bool(client) == bool(location):\n            raise ValueError(\n                \"You must specify either a client or a location to instantiate \"\n                \"a client.\"\n            )\n\n        pattern = r\"projects\\/[0-9]+\\/locations\\/[a-z\\-0-9]+\\/processors\\/[a-z0-9]+\"\n        if processor_name and not re.fullmatch(pattern, processor_name):\n            raise ValueError(\n                f\"Processor name {processor_name} has the wrong format. If your \"\n                \"prediction endpoint looks like https://us-documentai.googleapis.com\"\n                \"/v1/projects/PROJECT_ID/locations/us/processors/PROCESSOR_ID:process,\"\n                \" use only projects/PROJECT_ID/locations/us/processors/PROCESSOR_ID \"\n                \"part.\"\n            )\n\n        self._gcs_output_path = gcs_output_path\n        self._processor_name = processor_name\n        if client:\n            self._client = client\n        else:\n            try:\n                from google.api_core.client_options import ClientOptions\n                from google.cloud.documentai import DocumentProcessorServiceClient\n            except ImportError as exc:\n                raise ImportError(\n                    \"Could not import google-cloud-documentai python package. \"\n                    \"Please, install docai dependency group: \"\n                    \"`pip install langchain-google-community[docai]`\"\n                ) from exc\n            options = ClientOptions(\n                quota_project_id=project_id,\n                api_endpoint=f\"{location}-documentai.googleapis.com\",\n            )\n            self._client = DocumentProcessorServiceClient(\n                client_options=options,\n                client_info=get_client_info(module=\"document-ai\"),\n            )\n            # get processor type\n            self._processor_type = self._client.get_processor(name=processor_name).type\n            if self._processor_type == \"LAYOUT_PARSER_PROCESSOR\":\n                self._use_layout_parser = True\n            else:\n                self._use_layout_parser = False\n\n    def lazy_parse(self, blob: Blob) -&gt; Iterator[Document]:\n        \"\"\"Parses a blob lazily.\n\n        Args:\n            blobs: a Blob to parse\n\n        This is a long-running operation. A recommended way is to batch\n            documents together and use the `batch_parse()` method.\n        \"\"\"\n        yield from self.batch_parse([blob], gcs_output_path=self._gcs_output_path)\n\n    def online_process(\n        self,\n        blob: Blob,\n        enable_native_pdf_parsing: bool = True,\n        field_mask: Optional[str] = None,\n        page_range: Optional[List[int]] = None,\n        chunk_size: int = 500,\n        include_ancestor_headings: bool = True,\n    ) -&gt; Iterator[Document]:\n        \"\"\"Parses a blob lazily using online processing.\n\n        Args:\n            blob: a blob to parse.\n            enable_native_pdf_parsing: enable pdf embedded text extraction\n            field_mask: a comma-separated list of which fields to include in the\n                Document AI response.\n                suggested: \"text,pages.pageNumber,pages.layout\"\n            page_range: list of page numbers to parse. If `None`,\n                entire document will be parsed.\n            chunk_size: the maximum number of characters per chunk\n            include_ancestor_headings: whether to include ancestor headings in the chunks\n                https://cloud.google.com/document-ai/docs/reference/rpc/google.cloud.documentai.v1beta3#chunkingconfig\n        \"\"\"\n        try:\n            from google.cloud import documentai\n            from google.cloud.documentai_v1.types import (  # type: ignore[import, attr-defined]\n                OcrConfig,\n                ProcessOptions,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"Could not import google-cloud-documentai python package. \"\n                \"Please, install docai dependency group: \"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n        try:\n            from google.cloud.documentai_toolbox.wrappers.page import (  # type: ignore[import]\n                _text_from_layout,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"documentai_toolbox package not found, please install it with \"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n\n        if self._use_layout_parser:\n            layout_config = ProcessOptions.LayoutConfig(\n                chunking_config=ProcessOptions.LayoutConfig.ChunkingConfig(\n                    chunk_size=chunk_size,\n                    include_ancestor_headings=include_ancestor_headings,\n                )\n            )\n            individual_page_selector = (\n                ProcessOptions.IndividualPageSelector(pages=page_range)\n                if page_range\n                else None\n            )\n            process_options = ProcessOptions(\n                layout_config=layout_config,\n                individual_page_selector=individual_page_selector,\n            )\n        else:\n            ocr_config = (\n                OcrConfig(enable_native_pdf_parsing=enable_native_pdf_parsing)\n                if enable_native_pdf_parsing\n                else None\n            )\n            individual_page_selector = (\n                ProcessOptions.IndividualPageSelector(pages=page_range)\n                if page_range\n                else None\n            )\n            process_options = ProcessOptions(\n                ocr_config=ocr_config, individual_page_selector=individual_page_selector\n            )\n\n        response = self._client.process_document(\n            documentai.ProcessRequest(\n                name=self._processor_name,\n                gcs_document=documentai.GcsDocument(\n                    gcs_uri=blob.path,\n                    mime_type=blob.mimetype or \"application/pdf\",\n                ),\n                process_options=process_options,\n                skip_human_review=True,\n                field_mask=field_mask,\n            )\n        )\n\n        if self._use_layout_parser:\n            yield from (\n                Document(\n                    page_content=chunk.content,\n                    metadata={\n                        \"chunk_id\": chunk.chunk_id,\n                        \"source\": blob.path,\n                    },\n                )\n                for chunk in response.document.chunked_document.chunks\n            )\n        else:\n            yield from (\n                Document(\n                    page_content=_text_from_layout(page.layout, response.document.text),\n                    metadata={\n                        \"page\": page.page_number,\n                        \"source\": blob.path,\n                    },\n                )\n                for page in response.document.pages\n            )\n\n    def batch_parse(\n        self,\n        blobs: Sequence[Blob],\n        gcs_output_path: Optional[str] = None,\n        timeout_sec: int = 3600,\n        check_in_interval_sec: int = 60,\n        chunk_size: int = 500,\n        include_ancestor_headings: bool = True,\n    ) -&gt; Iterator[Document]:\n        \"\"\"Parses a list of blobs lazily.\n\n        Args:\n            blobs: a list of blobs to parse.\n            gcs_output_path: a path on Google Cloud Storage to store parsing results.\n            timeout_sec: a timeout to wait for Document AI to complete, in seconds.\n            check_in_interval_sec: an interval to wait until next check\n                whether parsing operations have been completed, in seconds\n        This is a long-running operation. A recommended way is to decouple\n            parsing from creating LangChain Documents:\n            &gt;&gt;&gt; operations = parser.docai_parse(blobs, gcs_path)\n            &gt;&gt;&gt; parser.is_running(operations)\n            You can get operations names and save them:\n            &gt;&gt;&gt; names = [op.operation.name for op in operations]\n            And when all operations are finished, you can use their results:\n            &gt;&gt;&gt; operations = parser.operations_from_names(operation_names)\n            &gt;&gt;&gt; results = parser.get_results(operations)\n            &gt;&gt;&gt; docs = parser.parse_from_results(results)\n        \"\"\"\n        output_path = gcs_output_path or self._gcs_output_path\n        if not output_path:\n            raise ValueError(\n                \"An output path on Google Cloud Storage should be provided.\"\n            )\n        operations = self.docai_parse(\n            blobs,\n            gcs_output_path=output_path,\n            chunk_size=chunk_size,\n            include_ancestor_headings=include_ancestor_headings,\n        )\n        operation_names = [op.operation.name for op in operations]\n        logger.debug(\n            \"Started parsing with Document AI, submitted operations %s\", operation_names\n        )\n        time_elapsed = 0\n        while self.is_running(operations):\n            time.sleep(check_in_interval_sec)\n            time_elapsed += check_in_interval_sec\n            if time_elapsed &gt; timeout_sec:\n                raise TimeoutError(\n                    \"Timeout exceeded! Check operations \" f\"{operation_names} later!\"\n                )\n            logger.debug(\".\")\n\n        results = self.get_results(operations=operations)\n        yield from self.parse_from_results(results)\n\n    def parse_from_results(\n        self, results: List[DocAIParsingResults]\n    ) -&gt; Iterator[Document]:\n        try:\n            from google.cloud.documentai_toolbox.utilities.gcs_utilities import (  # type: ignore[import]\n                split_gcs_uri,\n            )\n            from google.cloud.documentai_toolbox.wrappers.document import (  # type: ignore[import]\n                _get_shards,\n            )\n            from google.cloud.documentai_toolbox.wrappers.page import _text_from_layout\n        except ImportError as exc:\n            raise ImportError(\n                \"documentai_toolbox package not found, please install it with \"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n        for result in results:\n            print(f\"processing: {result.parsed_path}\")\n            gcs_bucket_name, gcs_prefix = split_gcs_uri(result.parsed_path)\n            shards = _get_shards(gcs_bucket_name, gcs_prefix + \"/\")\n            if self._use_layout_parser:\n                yield from (\n                    Document(\n                        page_content=chunk.content,\n                        metadata={\n                            \"chunk_id\": chunk.chunk_id,\n                            \"source\": result.source_path,\n                        },\n                    )\n                    for shard in shards\n                    for chunk in shard.chunked_document.chunks\n                )\n            else:\n                yield from (\n                    Document(\n                        page_content=_text_from_layout(page.layout, shard.text),\n                        metadata={\n                            \"page\": page.page_number,\n                            \"source\": result.source_path,\n                        },\n                    )\n                    for shard in shards\n                    for page in shard.pages\n                )\n\n    def operations_from_names(self, operation_names: List[str]) -&gt; List[\"Operation\"]:\n        \"\"\"Initializes Long-Running Operations from their names.\"\"\"\n        try:\n            from google.longrunning.operations_pb2 import (  # type: ignore[import]\n                GetOperationRequest,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"long running operations package not found, please install it with\"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n\n        return [\n            self._client.get_operation(request=GetOperationRequest(name=name))\n            for name in operation_names\n        ]\n\n    def is_running(self, operations: List[\"Operation\"]) -&gt; bool:\n        return any(not op.done() for op in operations)\n\n    def docai_parse(\n        self,\n        blobs: Sequence[Blob],\n        *,\n        gcs_output_path: Optional[str] = None,\n        processor_name: Optional[str] = None,\n        batch_size: int = 1000,\n        enable_native_pdf_parsing: bool = True,\n        field_mask: Optional[str] = None,\n        chunk_size: Optional[int] = 500,\n        include_ancestor_headings: Optional[bool] = True,\n    ) -&gt; List[\"Operation\"]:\n        \"\"\"Runs Google Document AI PDF Batch Processing on a list of blobs.\n\n        Args:\n            blobs: a list of blobs to be parsed\n            gcs_output_path: a path (folder) on GCS to store results\n            processor_name: name of a Document AI processor.\n            batch_size: amount of documents per batch\n            enable_native_pdf_parsing: a config option for the parser\n            field_mask: a comma-separated list of which fields to include in the\n                Document AI response.\n                suggested: \"text,pages.pageNumber,pages.layout\"\n            chunking_config: Serving config for chunking when using layout\n                parser processor. Specify config parameters as dictionary elements.\n                https://cloud.google.com/document-ai/docs/reference/rpc/google.cloud.documentai.v1beta3#chunkingconfig\n\n        Document AI has a 1000 file limit per batch, so batches larger than that need\n        to be split into multiple requests.\n        Batch processing is an async long-running operation\n        and results are stored in a output GCS bucket.\n        \"\"\"\n        try:\n            from google.cloud import documentai\n            from google.cloud.documentai_v1.types import OcrConfig, ProcessOptions\n        except ImportError as exc:\n            raise ImportError(\n                \"documentai package not found, please install it with \"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n\n        output_path = gcs_output_path or self._gcs_output_path\n        if output_path is None:\n            raise ValueError(\n                \"An output path on Google Cloud Storage should be provided.\"\n            )\n        processor_name = processor_name or self._processor_name\n        if processor_name is None:\n            raise ValueError(\"A Document AI processor name should be provided.\")\n\n        operations = []\n        for batch in batch_iterate(size=batch_size, iterable=blobs):\n            input_config = documentai.BatchDocumentsInputConfig(\n                gcs_documents=documentai.GcsDocuments(\n                    documents=[\n                        documentai.GcsDocument(\n                            gcs_uri=blob.path,\n                            mime_type=blob.mimetype or \"application/pdf\",\n                        )\n                        for blob in batch\n                    ]\n                )\n            )\n\n            output_config = documentai.DocumentOutputConfig(\n                gcs_output_config=documentai.DocumentOutputConfig.GcsOutputConfig(\n                    gcs_uri=output_path, field_mask=field_mask\n                )\n            )\n\n            if self._use_layout_parser:\n                layout_config = ProcessOptions.LayoutConfig(\n                    chunking_config=ProcessOptions.LayoutConfig.ChunkingConfig(\n                        chunk_size=chunk_size,\n                        include_ancestor_headings=include_ancestor_headings,\n                    )\n                )\n                process_options = ProcessOptions(layout_config=layout_config)\n            else:\n                process_options = (\n                    ProcessOptions(\n                        ocr_config=OcrConfig(\n                            enable_native_pdf_parsing=enable_native_pdf_parsing\n                        )\n                    )\n                    if enable_native_pdf_parsing\n                    else None\n                )\n            operations.append(\n                self._client.batch_process_documents(\n                    documentai.BatchProcessRequest(\n                        name=processor_name,\n                        input_documents=input_config,\n                        document_output_config=output_config,\n                        process_options=process_options,\n                        skip_human_review=True,\n                    )\n                )\n            )\n        return operations\n\n    def get_results(self, operations: List[\"Operation\"]) -&gt; List[DocAIParsingResults]:\n        try:\n            from google.cloud.documentai_v1 import (  # type: ignore[import]\n                BatchProcessMetadata,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"documentai package not found, please install it with \"\n                \"`pip install langchain-google-community[docai]`\"\n            ) from exc\n\n        return [\n            DocAIParsingResults(\n                source_path=status.input_gcs_source,\n                parsed_path=status.output_gcs_destination,\n            )\n            for op in operations\n            for status in (\n                op.metadata.individual_process_statuses\n                if isinstance(op.metadata, BatchProcessMetadata)\n                else BatchProcessMetadata.deserialize(\n                    op.metadata.value\n                ).individual_process_statuses\n            )\n        ]\n</pre> # @title Document AI LangChain Integration \"\"\"Module contains a PDF parser based on Document AI from Google Cloud.  You need to install two libraries to use this parser: pip install google-cloud-documentai pip install google-cloud-documentai-toolbox \"\"\"  import logging import re import time from dataclasses import dataclass from typing import TYPE_CHECKING, Iterator, List, Optional, Sequence  from langchain_core.document_loaders import BaseBlobParser from langchain_core.document_loaders.blob_loaders import Blob from langchain_core.documents import Document from langchain_core.utils.iter import batch_iterate  from langchain_google_community._utils import get_client_info  if TYPE_CHECKING:     from google.api_core.operation import Operation  # type: ignore[import]     from google.cloud.documentai import (  # type: ignore[import]         DocumentProcessorServiceClient,     )   logger = logging.getLogger(__name__)   @dataclass class DocAIParsingResults:     \"\"\"A dataclass to store Document AI parsing results.\"\"\"      source_path: str     parsed_path: str   class DocAIParser(BaseBlobParser):     \"\"\"`Google Cloud Document AI` parser.      For a detailed explanation of Document AI, refer to the product documentation.     https://cloud.google.com/document-ai/docs/overview     \"\"\"      def __init__(         self,         *,         client: Optional[\"DocumentProcessorServiceClient\"] = None,         project_id: Optional[str] = None,         location: Optional[str] = None,         gcs_output_path: Optional[str] = None,         processor_name: Optional[str] = None,     ) -&gt; None:         \"\"\"Initializes the parser.          Args:             client: a DocumentProcessorServiceClient to use             location: a Google Cloud location where a Document AI processor is located             gcs_output_path: a path on Google Cloud Storage to store parsing results             processor_name: full resource name of a Document AI processor or processor                 version          You should provide either a client or location (and then a client             would be instantiated).         \"\"\"          if bool(client) == bool(location):             raise ValueError(                 \"You must specify either a client or a location to instantiate \"                 \"a client.\"             )          pattern = r\"projects\\/[0-9]+\\/locations\\/[a-z\\-0-9]+\\/processors\\/[a-z0-9]+\"         if processor_name and not re.fullmatch(pattern, processor_name):             raise ValueError(                 f\"Processor name {processor_name} has the wrong format. If your \"                 \"prediction endpoint looks like https://us-documentai.googleapis.com\"                 \"/v1/projects/PROJECT_ID/locations/us/processors/PROCESSOR_ID:process,\"                 \" use only projects/PROJECT_ID/locations/us/processors/PROCESSOR_ID \"                 \"part.\"             )          self._gcs_output_path = gcs_output_path         self._processor_name = processor_name         if client:             self._client = client         else:             try:                 from google.api_core.client_options import ClientOptions                 from google.cloud.documentai import DocumentProcessorServiceClient             except ImportError as exc:                 raise ImportError(                     \"Could not import google-cloud-documentai python package. \"                     \"Please, install docai dependency group: \"                     \"`pip install langchain-google-community[docai]`\"                 ) from exc             options = ClientOptions(                 quota_project_id=project_id,                 api_endpoint=f\"{location}-documentai.googleapis.com\",             )             self._client = DocumentProcessorServiceClient(                 client_options=options,                 client_info=get_client_info(module=\"document-ai\"),             )             # get processor type             self._processor_type = self._client.get_processor(name=processor_name).type             if self._processor_type == \"LAYOUT_PARSER_PROCESSOR\":                 self._use_layout_parser = True             else:                 self._use_layout_parser = False      def lazy_parse(self, blob: Blob) -&gt; Iterator[Document]:         \"\"\"Parses a blob lazily.          Args:             blobs: a Blob to parse          This is a long-running operation. A recommended way is to batch             documents together and use the `batch_parse()` method.         \"\"\"         yield from self.batch_parse([blob], gcs_output_path=self._gcs_output_path)      def online_process(         self,         blob: Blob,         enable_native_pdf_parsing: bool = True,         field_mask: Optional[str] = None,         page_range: Optional[List[int]] = None,         chunk_size: int = 500,         include_ancestor_headings: bool = True,     ) -&gt; Iterator[Document]:         \"\"\"Parses a blob lazily using online processing.          Args:             blob: a blob to parse.             enable_native_pdf_parsing: enable pdf embedded text extraction             field_mask: a comma-separated list of which fields to include in the                 Document AI response.                 suggested: \"text,pages.pageNumber,pages.layout\"             page_range: list of page numbers to parse. If `None`,                 entire document will be parsed.             chunk_size: the maximum number of characters per chunk             include_ancestor_headings: whether to include ancestor headings in the chunks                 https://cloud.google.com/document-ai/docs/reference/rpc/google.cloud.documentai.v1beta3#chunkingconfig         \"\"\"         try:             from google.cloud import documentai             from google.cloud.documentai_v1.types import (  # type: ignore[import, attr-defined]                 OcrConfig,                 ProcessOptions,             )         except ImportError as exc:             raise ImportError(                 \"Could not import google-cloud-documentai python package. \"                 \"Please, install docai dependency group: \"                 \"`pip install langchain-google-community[docai]`\"             ) from exc         try:             from google.cloud.documentai_toolbox.wrappers.page import (  # type: ignore[import]                 _text_from_layout,             )         except ImportError as exc:             raise ImportError(                 \"documentai_toolbox package not found, please install it with \"                 \"`pip install langchain-google-community[docai]`\"             ) from exc          if self._use_layout_parser:             layout_config = ProcessOptions.LayoutConfig(                 chunking_config=ProcessOptions.LayoutConfig.ChunkingConfig(                     chunk_size=chunk_size,                     include_ancestor_headings=include_ancestor_headings,                 )             )             individual_page_selector = (                 ProcessOptions.IndividualPageSelector(pages=page_range)                 if page_range                 else None             )             process_options = ProcessOptions(                 layout_config=layout_config,                 individual_page_selector=individual_page_selector,             )         else:             ocr_config = (                 OcrConfig(enable_native_pdf_parsing=enable_native_pdf_parsing)                 if enable_native_pdf_parsing                 else None             )             individual_page_selector = (                 ProcessOptions.IndividualPageSelector(pages=page_range)                 if page_range                 else None             )             process_options = ProcessOptions(                 ocr_config=ocr_config, individual_page_selector=individual_page_selector             )          response = self._client.process_document(             documentai.ProcessRequest(                 name=self._processor_name,                 gcs_document=documentai.GcsDocument(                     gcs_uri=blob.path,                     mime_type=blob.mimetype or \"application/pdf\",                 ),                 process_options=process_options,                 skip_human_review=True,                 field_mask=field_mask,             )         )          if self._use_layout_parser:             yield from (                 Document(                     page_content=chunk.content,                     metadata={                         \"chunk_id\": chunk.chunk_id,                         \"source\": blob.path,                     },                 )                 for chunk in response.document.chunked_document.chunks             )         else:             yield from (                 Document(                     page_content=_text_from_layout(page.layout, response.document.text),                     metadata={                         \"page\": page.page_number,                         \"source\": blob.path,                     },                 )                 for page in response.document.pages             )      def batch_parse(         self,         blobs: Sequence[Blob],         gcs_output_path: Optional[str] = None,         timeout_sec: int = 3600,         check_in_interval_sec: int = 60,         chunk_size: int = 500,         include_ancestor_headings: bool = True,     ) -&gt; Iterator[Document]:         \"\"\"Parses a list of blobs lazily.          Args:             blobs: a list of blobs to parse.             gcs_output_path: a path on Google Cloud Storage to store parsing results.             timeout_sec: a timeout to wait for Document AI to complete, in seconds.             check_in_interval_sec: an interval to wait until next check                 whether parsing operations have been completed, in seconds         This is a long-running operation. A recommended way is to decouple             parsing from creating LangChain Documents:             &gt;&gt;&gt; operations = parser.docai_parse(blobs, gcs_path)             &gt;&gt;&gt; parser.is_running(operations)             You can get operations names and save them:             &gt;&gt;&gt; names = [op.operation.name for op in operations]             And when all operations are finished, you can use their results:             &gt;&gt;&gt; operations = parser.operations_from_names(operation_names)             &gt;&gt;&gt; results = parser.get_results(operations)             &gt;&gt;&gt; docs = parser.parse_from_results(results)         \"\"\"         output_path = gcs_output_path or self._gcs_output_path         if not output_path:             raise ValueError(                 \"An output path on Google Cloud Storage should be provided.\"             )         operations = self.docai_parse(             blobs,             gcs_output_path=output_path,             chunk_size=chunk_size,             include_ancestor_headings=include_ancestor_headings,         )         operation_names = [op.operation.name for op in operations]         logger.debug(             \"Started parsing with Document AI, submitted operations %s\", operation_names         )         time_elapsed = 0         while self.is_running(operations):             time.sleep(check_in_interval_sec)             time_elapsed += check_in_interval_sec             if time_elapsed &gt; timeout_sec:                 raise TimeoutError(                     \"Timeout exceeded! Check operations \" f\"{operation_names} later!\"                 )             logger.debug(\".\")          results = self.get_results(operations=operations)         yield from self.parse_from_results(results)      def parse_from_results(         self, results: List[DocAIParsingResults]     ) -&gt; Iterator[Document]:         try:             from google.cloud.documentai_toolbox.utilities.gcs_utilities import (  # type: ignore[import]                 split_gcs_uri,             )             from google.cloud.documentai_toolbox.wrappers.document import (  # type: ignore[import]                 _get_shards,             )             from google.cloud.documentai_toolbox.wrappers.page import _text_from_layout         except ImportError as exc:             raise ImportError(                 \"documentai_toolbox package not found, please install it with \"                 \"`pip install langchain-google-community[docai]`\"             ) from exc         for result in results:             print(f\"processing: {result.parsed_path}\")             gcs_bucket_name, gcs_prefix = split_gcs_uri(result.parsed_path)             shards = _get_shards(gcs_bucket_name, gcs_prefix + \"/\")             if self._use_layout_parser:                 yield from (                     Document(                         page_content=chunk.content,                         metadata={                             \"chunk_id\": chunk.chunk_id,                             \"source\": result.source_path,                         },                     )                     for shard in shards                     for chunk in shard.chunked_document.chunks                 )             else:                 yield from (                     Document(                         page_content=_text_from_layout(page.layout, shard.text),                         metadata={                             \"page\": page.page_number,                             \"source\": result.source_path,                         },                     )                     for shard in shards                     for page in shard.pages                 )      def operations_from_names(self, operation_names: List[str]) -&gt; List[\"Operation\"]:         \"\"\"Initializes Long-Running Operations from their names.\"\"\"         try:             from google.longrunning.operations_pb2 import (  # type: ignore[import]                 GetOperationRequest,             )         except ImportError as exc:             raise ImportError(                 \"long running operations package not found, please install it with\"                 \"`pip install langchain-google-community[docai]`\"             ) from exc          return [             self._client.get_operation(request=GetOperationRequest(name=name))             for name in operation_names         ]      def is_running(self, operations: List[\"Operation\"]) -&gt; bool:         return any(not op.done() for op in operations)      def docai_parse(         self,         blobs: Sequence[Blob],         *,         gcs_output_path: Optional[str] = None,         processor_name: Optional[str] = None,         batch_size: int = 1000,         enable_native_pdf_parsing: bool = True,         field_mask: Optional[str] = None,         chunk_size: Optional[int] = 500,         include_ancestor_headings: Optional[bool] = True,     ) -&gt; List[\"Operation\"]:         \"\"\"Runs Google Document AI PDF Batch Processing on a list of blobs.          Args:             blobs: a list of blobs to be parsed             gcs_output_path: a path (folder) on GCS to store results             processor_name: name of a Document AI processor.             batch_size: amount of documents per batch             enable_native_pdf_parsing: a config option for the parser             field_mask: a comma-separated list of which fields to include in the                 Document AI response.                 suggested: \"text,pages.pageNumber,pages.layout\"             chunking_config: Serving config for chunking when using layout                 parser processor. Specify config parameters as dictionary elements.                 https://cloud.google.com/document-ai/docs/reference/rpc/google.cloud.documentai.v1beta3#chunkingconfig          Document AI has a 1000 file limit per batch, so batches larger than that need         to be split into multiple requests.         Batch processing is an async long-running operation         and results are stored in a output GCS bucket.         \"\"\"         try:             from google.cloud import documentai             from google.cloud.documentai_v1.types import OcrConfig, ProcessOptions         except ImportError as exc:             raise ImportError(                 \"documentai package not found, please install it with \"                 \"`pip install langchain-google-community[docai]`\"             ) from exc          output_path = gcs_output_path or self._gcs_output_path         if output_path is None:             raise ValueError(                 \"An output path on Google Cloud Storage should be provided.\"             )         processor_name = processor_name or self._processor_name         if processor_name is None:             raise ValueError(\"A Document AI processor name should be provided.\")          operations = []         for batch in batch_iterate(size=batch_size, iterable=blobs):             input_config = documentai.BatchDocumentsInputConfig(                 gcs_documents=documentai.GcsDocuments(                     documents=[                         documentai.GcsDocument(                             gcs_uri=blob.path,                             mime_type=blob.mimetype or \"application/pdf\",                         )                         for blob in batch                     ]                 )             )              output_config = documentai.DocumentOutputConfig(                 gcs_output_config=documentai.DocumentOutputConfig.GcsOutputConfig(                     gcs_uri=output_path, field_mask=field_mask                 )             )              if self._use_layout_parser:                 layout_config = ProcessOptions.LayoutConfig(                     chunking_config=ProcessOptions.LayoutConfig.ChunkingConfig(                         chunk_size=chunk_size,                         include_ancestor_headings=include_ancestor_headings,                     )                 )                 process_options = ProcessOptions(layout_config=layout_config)             else:                 process_options = (                     ProcessOptions(                         ocr_config=OcrConfig(                             enable_native_pdf_parsing=enable_native_pdf_parsing                         )                     )                     if enable_native_pdf_parsing                     else None                 )             operations.append(                 self._client.batch_process_documents(                     documentai.BatchProcessRequest(                         name=processor_name,                         input_documents=input_config,                         document_output_config=output_config,                         process_options=process_options,                         skip_human_review=True,                     )                 )             )         return operations      def get_results(self, operations: List[\"Operation\"]) -&gt; List[DocAIParsingResults]:         try:             from google.cloud.documentai_v1 import (  # type: ignore[import]                 BatchProcessMetadata,             )         except ImportError as exc:             raise ImportError(                 \"documentai package not found, please install it with \"                 \"`pip install langchain-google-community[docai]`\"             ) from exc          return [             DocAIParsingResults(                 source_path=status.input_gcs_source,                 parsed_path=status.output_gcs_destination,             )             for op in operations             for status in (                 op.metadata.individual_process_statuses                 if isinstance(op.metadata, BatchProcessMetadata)                 else BatchProcessMetadata.deserialize(                     op.metadata.value                 ).individual_process_statuses             )         ] In\u00a0[\u00a0]: Copied! <pre># @title Custom Cloud Storage Loader\n\nimport logging\nfrom langchain_community.document_loaders.base import BaseLoader\nfrom langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader\nfrom langchain_community.document_loaders.gcs_file import GCSFileLoader\nfrom langchain_community.utilities.vertexai import get_client_info\nimport re\n\nlogger = logging.getLogger(__name__)\n\n\nclass CustomGCSDirectoryLoader(GCSDirectoryLoader, BaseLoader):\n    def load(self, file_pattern=None) -&gt; List[Document]:\n        \"\"\"Load documents.\"\"\"\n        try:\n            from google.cloud import storage\n        except ImportError:\n            raise ImportError(\n                \"Could not import google-cloud-storage python package. \"\n                \"Please install it with `pip install google-cloud-storage`.\"\n            )\n        client = storage.Client(\n            project=self.project_name,\n            client_info=get_client_info(module=\"google-cloud-storage\"),\n        )\n\n        regex = None\n        if file_pattern:\n            regex = re.compile(r'{}'.format(file_pattern))\n\n        docs = []\n        for blob in client.list_blobs(self.bucket, prefix=self.prefix):\n            # we shall just skip directories since GCSFileLoader creates\n            # intermediate directories on the fly\n            if blob.name.endswith(\"/\"):\n                continue\n            if regex and not regex.match(blob.name):\n                continue\n            # Use the try-except block here\n            try:\n                logger.info(f\"Processing {blob.name}\")\n                temp_blob = Blob(path=f\"gs://{blob.bucket.name}/{blob.name}\")\n                docs.append(temp_blob)\n            except Exception as e:\n                if self.continue_on_failure:\n                    logger.warning(f\"Problem processing blob {blob.name}, message: {e}\")\n                    continue\n                else:\n                    raise e\n        return docs\n</pre> # @title Custom Cloud Storage Loader  import logging from langchain_community.document_loaders.base import BaseLoader from langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader from langchain_community.document_loaders.gcs_file import GCSFileLoader from langchain_community.utilities.vertexai import get_client_info import re  logger = logging.getLogger(__name__)   class CustomGCSDirectoryLoader(GCSDirectoryLoader, BaseLoader):     def load(self, file_pattern=None) -&gt; List[Document]:         \"\"\"Load documents.\"\"\"         try:             from google.cloud import storage         except ImportError:             raise ImportError(                 \"Could not import google-cloud-storage python package. \"                 \"Please install it with `pip install google-cloud-storage`.\"             )         client = storage.Client(             project=self.project_name,             client_info=get_client_info(module=\"google-cloud-storage\"),         )          regex = None         if file_pattern:             regex = re.compile(r'{}'.format(file_pattern))          docs = []         for blob in client.list_blobs(self.bucket, prefix=self.prefix):             # we shall just skip directories since GCSFileLoader creates             # intermediate directories on the fly             if blob.name.endswith(\"/\"):                 continue             if regex and not regex.match(blob.name):                 continue             # Use the try-except block here             try:                 logger.info(f\"Processing {blob.name}\")                 temp_blob = Blob(path=f\"gs://{blob.bucket.name}/{blob.name}\")                 docs.append(temp_blob)             except Exception as e:                 if self.continue_on_failure:                     logger.warning(f\"Problem processing blob {blob.name}, message: {e}\")                     continue                 else:                     raise e         return docs In\u00a0[\u00a0]: Copied! <pre># @title Utility function to create resources\nimport hashlib\nimport uuid\n\nfrom google.cloud import storage\nfrom google.cloud import aiplatform\nfrom google.cloud import documentai\nfrom google.api_core.client_options import ClientOptions\nfrom google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n\n\ndef create_uuid(name: str) -&gt; str:\n    hex_string = hashlib.md5(name.encode(\"UTF-8\")).hexdigest()\n    return str(uuid.UUID(hex=hex_string))\n\n\ndef create_bucket(bucket_name: str) -&gt; storage.Bucket:\n    # create Cloud Storage bucket if does not exists\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n\n    if bucket.exists():\n        print(f\"Bucket {bucket.name} exists\")\n        return bucket\n\n    if not CREATE_RESOURCES:\n        return bucket\n\n    bucket = storage_client.create_bucket(bucket_name, project=PROJECT_ID)\n    print(f\"Bucket {bucket.name} created\")\n    return bucket\n\n\ndef create_index() -&gt; Optional[MatchingEngineIndex]:\n    index_names = [\n        index.resource_name\n        for index in MatchingEngineIndex.list(filter=f\"display_name={VS_INDEX_NAME}\")\n    ]\n\n    if len(index_names) &gt; 0:\n        vs_index = MatchingEngineIndex(index_name=index_names[0])\n        print(\n            f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n        )\n        return vs_index\n\n    if not CREATE_RESOURCES:\n        print(\n            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n        )\n        return None\n\n    print(f\"Creating Vector Search index {VS_INDEX_NAME} ...\")\n    vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n        display_name=VS_INDEX_NAME,\n        dimensions=VS_DIMENSIONS,\n        approximate_neighbors_count=VS_APPROX_NEIGHBORS,\n        distance_measure_type=VS_DISTANCE_MEASURE_TYPE,\n        leaf_node_embedding_count=VS_LEAF_NODE_EMB_COUNT,\n        leaf_nodes_to_search_percent=VS_LEAF_SEARCH_PERCENT,\n        description=VS_DESCRIPTION,\n        shard_size=VS_INDEX_SHARD_SIZE,\n        index_update_method=VS_INDEX_UPDATE_METHOD,\n        project=PROJECT_ID,\n        location=REGION,\n    )\n    print(\n        f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n    )\n    return vs_index\n\n\ndef create_index_endpoint() -&gt; Optional[MatchingEngineIndexEndpoint]:\n    endpoint_names = [\n        endpoint.resource_name\n        for endpoint in MatchingEngineIndexEndpoint.list(\n            filter=f\"display_name={VS_INDEX_ENDPOINT_NAME}\"\n        )\n    ]\n\n    if len(endpoint_names) &gt; 0:\n        vs_endpoint = MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_names[0])\n        print(\n            f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n        )\n        return vs_endpoint\n\n    if not CREATE_RESOURCES:\n        print(\n            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n        )\n        return None\n\n    print(f\"Creating Vector Search index endpoint {VS_INDEX_ENDPOINT_NAME} ...\")\n    vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n        display_name=VS_INDEX_ENDPOINT_NAME,\n        public_endpoint_enabled=True,\n        description=VS_DESCRIPTION,\n        project=PROJECT_ID,\n        location=REGION,\n    )\n    print(\n        f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n    )\n    return vs_endpoint\n\n\ndef deploy_index(\n    index: MatchingEngineIndex, endpoint: MatchingEngineIndexEndpoint\n) -&gt; Optional[MatchingEngineIndexEndpoint]:\n    index_endpoints = []\n    if index is not None:\n        index_endpoints = [\n            (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n            for deployed_index in index.deployed_indexes\n        ]\n\n    if len(index_endpoints) &gt; 0:\n        vs_deployed_index = MatchingEngineIndexEndpoint(\n            index_endpoint_name=index_endpoints[0][0]\n        )\n        print(\n            f\"Vector Search index {index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n        )\n        return vs_deployed_index\n\n    if not CREATE_RESOURCES:\n        print(\n            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n        )\n        return None\n\n    print(\n        f\"Deploying Vector Search index {index.display_name} at endpoint {endpoint.display_name} ...\"\n    )\n    deployed_index_id = (\n        f'{VS_INDEX_NAME}_{create_uuid(VS_INDEX_NAME).split(\"-\")[-1]}'.replace(\"-\", \"_\")\n    )\n    vs_deployed_index = endpoint.deploy_index(\n        index=index,\n        deployed_index_id=deployed_index_id,\n        display_name=VS_INDEX_NAME,\n        machine_type=VS_MACHINE_TYPE,\n        min_replica_count=VS_MIN_REPLICAS,\n        max_replica_count=VS_MAX_REPLICAS,\n    )\n    print(\n        f\"Vector Search index {index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n    )\n    return vs_deployed_index\n\n\ndef create_docai_processor(\n    processor_display_name: str = DOCAI_PROCESSOR_NAME,\n    processor_type: str = \"LAYOUT_PARSER_PROCESSOR\",\n) -&gt; Optional[documentai.Processor]:\n    # Set the api_endpoint if you use a location other than 'us'\n    opts = ClientOptions(api_endpoint=f\"{DOCAI_LOCATION}-documentai.googleapis.com\")\n    docai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n    parent = docai_client.common_location_path(PROJECT_ID, DOCAI_LOCATION)\n    # Check if processor exists\n    processor_list = docai_client.list_processors(parent=parent)\n    processors = [\n        processor.name\n        for processor in processor_list\n        if (\n            processor.display_name == processor_display_name\n            and processor.type_ == processor_type\n        )\n    ]\n\n    if len(processors) &gt; 0:\n        docai_processor = docai_client.get_processor(name=processors[0])\n        print(\n            f\"Document AI processor {docai_processor.display_name} is already created\"\n        )\n        return docai_processor\n\n    if not CREATE_RESOURCES:\n        print(\n            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n        )\n        return None\n\n    # Create a processor\n    print(\n        f\"Creating Document AI processor {processor_display_name} of type {processor_type} ...\"\n    )\n    docai_processor = docai_client.create_processor(\n        parent=parent,\n        processor=documentai.Processor(\n            display_name=processor_display_name, type_=processor_type\n        ),\n    )\n    print(\n        f\"Document AI processor {processor_display_name} of type {processor_type} is created.\"\n    )\n    return docai_processor\n</pre> # @title Utility function to create resources import hashlib import uuid  from google.cloud import storage from google.cloud import aiplatform from google.cloud import documentai from google.api_core.client_options import ClientOptions from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint   def create_uuid(name: str) -&gt; str:     hex_string = hashlib.md5(name.encode(\"UTF-8\")).hexdigest()     return str(uuid.UUID(hex=hex_string))   def create_bucket(bucket_name: str) -&gt; storage.Bucket:     # create Cloud Storage bucket if does not exists     storage_client = storage.Client()     bucket = storage_client.bucket(bucket_name)      if bucket.exists():         print(f\"Bucket {bucket.name} exists\")         return bucket      if not CREATE_RESOURCES:         return bucket      bucket = storage_client.create_bucket(bucket_name, project=PROJECT_ID)     print(f\"Bucket {bucket.name} created\")     return bucket   def create_index() -&gt; Optional[MatchingEngineIndex]:     index_names = [         index.resource_name         for index in MatchingEngineIndex.list(filter=f\"display_name={VS_INDEX_NAME}\")     ]      if len(index_names) &gt; 0:         vs_index = MatchingEngineIndex(index_name=index_names[0])         print(             f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"         )         return vs_index      if not CREATE_RESOURCES:         print(             f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"         )         return None      print(f\"Creating Vector Search index {VS_INDEX_NAME} ...\")     vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(         display_name=VS_INDEX_NAME,         dimensions=VS_DIMENSIONS,         approximate_neighbors_count=VS_APPROX_NEIGHBORS,         distance_measure_type=VS_DISTANCE_MEASURE_TYPE,         leaf_node_embedding_count=VS_LEAF_NODE_EMB_COUNT,         leaf_nodes_to_search_percent=VS_LEAF_SEARCH_PERCENT,         description=VS_DESCRIPTION,         shard_size=VS_INDEX_SHARD_SIZE,         index_update_method=VS_INDEX_UPDATE_METHOD,         project=PROJECT_ID,         location=REGION,     )     print(         f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"     )     return vs_index   def create_index_endpoint() -&gt; Optional[MatchingEngineIndexEndpoint]:     endpoint_names = [         endpoint.resource_name         for endpoint in MatchingEngineIndexEndpoint.list(             filter=f\"display_name={VS_INDEX_ENDPOINT_NAME}\"         )     ]      if len(endpoint_names) &gt; 0:         vs_endpoint = MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_names[0])         print(             f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"         )         return vs_endpoint      if not CREATE_RESOURCES:         print(             f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"         )         return None      print(f\"Creating Vector Search index endpoint {VS_INDEX_ENDPOINT_NAME} ...\")     vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(         display_name=VS_INDEX_ENDPOINT_NAME,         public_endpoint_enabled=True,         description=VS_DESCRIPTION,         project=PROJECT_ID,         location=REGION,     )     print(         f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"     )     return vs_endpoint   def deploy_index(     index: MatchingEngineIndex, endpoint: MatchingEngineIndexEndpoint ) -&gt; Optional[MatchingEngineIndexEndpoint]:     index_endpoints = []     if index is not None:         index_endpoints = [             (deployed_index.index_endpoint, deployed_index.deployed_index_id)             for deployed_index in index.deployed_indexes         ]      if len(index_endpoints) &gt; 0:         vs_deployed_index = MatchingEngineIndexEndpoint(             index_endpoint_name=index_endpoints[0][0]         )         print(             f\"Vector Search index {index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"         )         return vs_deployed_index      if not CREATE_RESOURCES:         print(             f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"         )         return None      print(         f\"Deploying Vector Search index {index.display_name} at endpoint {endpoint.display_name} ...\"     )     deployed_index_id = (         f'{VS_INDEX_NAME}_{create_uuid(VS_INDEX_NAME).split(\"-\")[-1]}'.replace(\"-\", \"_\")     )     vs_deployed_index = endpoint.deploy_index(         index=index,         deployed_index_id=deployed_index_id,         display_name=VS_INDEX_NAME,         machine_type=VS_MACHINE_TYPE,         min_replica_count=VS_MIN_REPLICAS,         max_replica_count=VS_MAX_REPLICAS,     )     print(         f\"Vector Search index {index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"     )     return vs_deployed_index   def create_docai_processor(     processor_display_name: str = DOCAI_PROCESSOR_NAME,     processor_type: str = \"LAYOUT_PARSER_PROCESSOR\", ) -&gt; Optional[documentai.Processor]:     # Set the api_endpoint if you use a location other than 'us'     opts = ClientOptions(api_endpoint=f\"{DOCAI_LOCATION}-documentai.googleapis.com\")     docai_client = documentai.DocumentProcessorServiceClient(client_options=opts)     parent = docai_client.common_location_path(PROJECT_ID, DOCAI_LOCATION)     # Check if processor exists     processor_list = docai_client.list_processors(parent=parent)     processors = [         processor.name         for processor in processor_list         if (             processor.display_name == processor_display_name             and processor.type_ == processor_type         )     ]      if len(processors) &gt; 0:         docai_processor = docai_client.get_processor(name=processors[0])         print(             f\"Document AI processor {docai_processor.display_name} is already created\"         )         return docai_processor      if not CREATE_RESOURCES:         print(             f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"         )         return None      # Create a processor     print(         f\"Creating Document AI processor {processor_display_name} of type {processor_type} ...\"     )     docai_processor = docai_client.create_processor(         parent=parent,         processor=documentai.Processor(             display_name=processor_display_name, type_=processor_type         ),     )     print(         f\"Document AI processor {processor_display_name} of type {processor_type} is created.\"     )     return docai_processor In\u00a0[\u00a0]: Copied! <pre># @title Utility methods for adding index to Vertex AI Vector Search\ndef get_batches(items: List, n: int = 1000) -&gt; List[List]:\n    n = max(1, n)\n    return [items[i : i + n] for i in range(0, len(items), n)]\n\n\ndef add_data(vector_store, chunks) -&gt; None:\n    if RUN_INGESTION:\n        batch_size = 1000\n        texts = get_batches([chunk.page_content for chunk in chunks], n=batch_size)\n        metadatas = get_batches([chunk.metadata for chunk in chunks], n=batch_size)\n\n        for i, (b_texts, b_metadatas) in enumerate(zip(texts, metadatas)):\n            print(f\"Adding {len(b_texts)} data points to index\")\n            is_complete_overwrite = bool(i == 0)\n            vector_store.add_texts(\n                texts=b_texts,\n                metadatas=b_metadatas,\n                is_complete_overwrite=is_complete_overwrite,\n            )\n    else:\n        print(\"Skipping ingestion. Enable `RUN_INGESTION` flag\")\n</pre> # @title Utility methods for adding index to Vertex AI Vector Search def get_batches(items: List, n: int = 1000) -&gt; List[List]:     n = max(1, n)     return [items[i : i + n] for i in range(0, len(items), n)]   def add_data(vector_store, chunks) -&gt; None:     if RUN_INGESTION:         batch_size = 1000         texts = get_batches([chunk.page_content for chunk in chunks], n=batch_size)         metadatas = get_batches([chunk.metadata for chunk in chunks], n=batch_size)          for i, (b_texts, b_metadatas) in enumerate(zip(texts, metadatas)):             print(f\"Adding {len(b_texts)} data points to index\")             is_complete_overwrite = bool(i == 0)             vector_store.add_texts(                 texts=b_texts,                 metadatas=b_metadatas,                 is_complete_overwrite=is_complete_overwrite,             )     else:         print(\"Skipping ingestion. Enable `RUN_INGESTION` flag\") In\u00a0[\u00a0]: Copied! <pre># @title Utility methods for displaying rich content results\nfrom IPython.display import display, HTML\nimport markdown as md\n\n\ndef get_chunk_content(results: List) -&gt; List:\n    return [\n        doc.page_content.replace(\"\\n\", \"&lt;br&gt;\")\n        + f'&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;a href=\"\"&gt;Source: {doc.metadata.get(\"source\")}&lt;/a&gt;&lt;/b&gt;'\n        for doc in results\n    ][:5]\n\n\nCONTRASTING_COLORS = [\n    \"rgba(255, 0, 0, 0.2)\",  # Semi-transparent red\n    \"rgba(0, 255, 0, 0.2)\",  # Semi-transparent green\n    \"rgba(0, 0, 255, 0.2)\",  # Semi-transparent blue\n    \"rgba(255, 255, 0, 0.2)\",  # Semi-transparent yellow\n    \"rgba(0, 255, 255, 0.2)\",  # Semi-transparent cyan\n    \"rgba(255, 0, 255, 0.2)\",  # Semi-transparent magenta\n    \"rgba(255, 165, 0, 0.2)\",  # Semi-transparent orange\n    \"rgba(255, 105, 180, 0.2)\",  # Semi-transparent pink\n    \"rgba(75, 0, 130, 0.2)\",  # Semi-transparent indigo\n    \"rgba(255, 192, 203, 0.2)\",  # Semi-transparent light pink\n    \"rgba(64, 224, 208, 0.2)\",  # Semi-transparent turquoise\n    \"rgba(128, 0, 128, 0.2)\",  # Semi-transparent purple\n    \"rgba(210, 105, 30, 0.2)\",  # Semi-transparent chocolate\n    \"rgba(220, 20, 60, 0.2)\",  # Semi-transparent crimson\n    \"rgba(95, 158, 160, 0.2)\",  # Semi-transparent cadet blue\n    \"rgba(255, 99, 71, 0.2)\",  # Semi-transparent tomato\n    \"rgba(144, 238, 144, 0.2)\",  # Semi-transparent light green\n    \"rgba(70, 130, 180, 0.2)\",  # Semi-transparent steel blue\n]\n\n\ndef convert_markdown_to_html(text: str) -&gt; str:\n    # Convert Markdown to HTML, ensuring embedded HTML is preserved and interpreted correctly.\n    md_extensions = [\n        \"extra\",\n        \"abbr\",\n        \"attr_list\",\n        \"def_list\",\n        \"fenced_code\",\n        \"footnotes\",\n        \"md_in_html\",\n        \"tables\",\n        \"admonition\",\n        \"codehilite\",\n        \"legacy_attrs\",\n        \"legacy_em\",\n        \"meta\",\n        \"nl2br\",\n        \"sane_lists\",\n        \"smarty\",\n        \"toc\",\n        \"wikilinks\",\n    ]\n    return str(md.markdown(text, extensions=md_extensions))\n\n\n# Utility function to create HTML table with colored results\ndef display_html_table(simple_results: List[str], reranked_results: List[str]) -&gt; None:\n    # Find all unique values in both lists\n    unique_values = set(simple_results + reranked_results)\n\n    # Ensure we have enough colors for all unique values\n    # If not, colors will repeat, which might not be ideal but is necessary if the number of unique values exceeds the number of colors\n    colors = CONTRASTING_COLORS * (len(unique_values) // len(CONTRASTING_COLORS) + 1)\n\n    # Create a dictionary to map each unique value to a color\n    color_map = dict(zip(unique_values, colors))\n\n    # Initialize the HTML table with style for equal column widths\n    html = \"\"\"\n    &lt;style&gt;\n    td, th {\n        padding: 8px;\n        text-align: left;\n        border-bottom: 1px solid #ddd;\n        color: #000;\n    }\n    tr {background-color: #ffffff;}\n    /* Set table layout to fixed to respect column widths */\n    table {\n        table-layout: fixed;\n        width: 100%; /* You can adjust the overall table width as needed */\n        max-height: 100vh !important; /* Set the maximum height of the table */\n        overflow-y: auto; /* Add a vertical scrollbar if the content exceeds the maximum height */\n    }\n    /* Set equal width for both columns */\n    td, th {\n        width: 50%;\n    }\n    .text-black {\n        color: #000; /* Set the text color to black */\n    }\n    &lt;/style&gt;\n    &lt;table&gt;\n    &lt;tr&gt;&lt;th&gt;Retriever Results&lt;/th&gt;&lt;th&gt;Reranked Results&lt;/th&gt;&lt;/tr&gt;\n    \"\"\"\n    # Iterate over the results and assign the corresponding color to each cell\n    for simple, reranked in zip(simple_results, reranked_results):\n        html += f\"\"\"\n        &lt;tr&gt;\n            &lt;td style='color: black; background-color: {color_map[simple]}; font-size: 8px;'&gt;\n                &lt;p class='text-black'&gt;{convert_markdown_to_html(simple)}&lt;/p&gt;\n            &lt;/td&gt;\n            &lt;td style='color: black; background-color: {color_map[reranked]}; font-size: 8px;'&gt;\n                &lt;p class='text-black'&gt;{convert_markdown_to_html(reranked)}&lt;/p&gt;\n            &lt;/td&gt;\n        &lt;/tr&gt;\n        \"\"\"\n    html += \"&lt;/table&gt;\"\n    display(HTML(html))\n\n\ndef get_sxs_comparison(\n    simple_retriever, reranking_api_retriever, query, search_kwargs\n) -&gt; List:\n    simple_results = get_chunk_content(\n        simple_retriever.invoke(query, search_kwargs=search_kwargs)\n    )\n    reranked_results = get_chunk_content(\n        reranking_api_retriever.invoke(query, search_kwargs=search_kwargs)\n    )\n    display_html_table(simple_results, reranked_results)\n\n    return reranked_results\n\n\ndef display_grounded_generation(response) -&gt; None:\n    # Extract the answer with citations and cited chunks\n    answer_with_citations = response.answer_with_citations\n    cited_chunks = response.cited_chunks\n\n    # Build HTML for the chunks\n    chunks_html = \"\".join(\n        [\n            f\"&lt;div id='chunk-{index}' class='chunk'&gt;\"\n            + f\"&lt;div class='source'&gt;Source {index}: &lt;a href='{chunk['source'].metadata['source']}' target='_blank'&gt;{chunk['source'].metadata['source']}&lt;/a&gt;&lt;/div&gt;\"\n            + f\"&lt;p&gt;{chunk['chunk_text']}&lt;/p&gt;\"\n            + \"&lt;/div&gt;\"\n            for index, chunk in enumerate(cited_chunks)\n        ]\n    )\n\n    # Replace citation indices with hoverable spans\n    for index in range(len(cited_chunks)):\n        answer_with_citations = answer_with_citations.replace(\n            f\"[{index}]\",\n            f\"&lt;span class='citation' onmouseover='highlight({index})' onmouseout='unhighlight({index})'&gt;[{index}]&lt;/span&gt;\",\n        )\n\n    # The complete HTML\n    html_content = f\"\"\"\n    &lt;style&gt;\n    .answer-box {{\n        background-color: #f8f9fa;\n        border-left: 4px solid #0056b3;\n        padding: 20px;\n        margin-bottom: 20px;\n        color: #000;\n    }}\n    .citation {{\n        background-color: transparent;\n        cursor: pointer;\n    }}\n    .chunk {{\n        background-color: #ffffff;\n        border-left: 4px solid #007bff;\n        padding: 10px;\n        margin-bottom: 10px;\n        transition: background-color 0.3s;\n        color: #000;\n    }}\n    .source {{\n        font-weight: bold;\n        margin-bottom: 5px;\n    }}\n    a {{\n        text-decoration: none;\n        color: #0056b3;\n    }}\n    a:hover {{\n        text-decoration: underline;\n    }}\n    &lt;/style&gt;\n    &lt;div class='answer-box'&gt;{answer_with_citations}&lt;/div&gt;\n    &lt;div class='chunks-box'&gt;{chunks_html}&lt;/div&gt;\n    &lt;script&gt;\n    function highlight(index) {{\n        // Highlight the citation in the answer\n        document.querySelectorAll('.citation').forEach(function(citation) {{\n            if (citation.textContent === '[' + index + ']') {{\n                citation.style.backgroundColor = '#ffff99';\n            }}\n        }});\n        // Highlight the corresponding chunk\n        document.getElementById('chunk-' + index).style.backgroundColor = '#ffff99';\n    }}\n    function unhighlight(index) {{\n        // Unhighlight the citation in the answer\n        document.querySelectorAll('.citation').forEach(function(citation) {{\n            if (citation.textContent === '[' + index + ']') {{\n                citation.style.backgroundColor = 'transparent';\n            }}\n        }});\n        // Unhighlight the corresponding chunk\n        document.getElementById('chunk-' + index).style.backgroundColor = '#ffffff';\n    }}\n    &lt;/script&gt;\n    \"\"\"\n    display(HTML(html_content))\n</pre> # @title Utility methods for displaying rich content results from IPython.display import display, HTML import markdown as md   def get_chunk_content(results: List) -&gt; List:     return [         doc.page_content.replace(\"\\n\", \"\")         + f' Source: {doc.metadata.get(\"source\")}'         for doc in results     ][:5]   CONTRASTING_COLORS = [     \"rgba(255, 0, 0, 0.2)\",  # Semi-transparent red     \"rgba(0, 255, 0, 0.2)\",  # Semi-transparent green     \"rgba(0, 0, 255, 0.2)\",  # Semi-transparent blue     \"rgba(255, 255, 0, 0.2)\",  # Semi-transparent yellow     \"rgba(0, 255, 255, 0.2)\",  # Semi-transparent cyan     \"rgba(255, 0, 255, 0.2)\",  # Semi-transparent magenta     \"rgba(255, 165, 0, 0.2)\",  # Semi-transparent orange     \"rgba(255, 105, 180, 0.2)\",  # Semi-transparent pink     \"rgba(75, 0, 130, 0.2)\",  # Semi-transparent indigo     \"rgba(255, 192, 203, 0.2)\",  # Semi-transparent light pink     \"rgba(64, 224, 208, 0.2)\",  # Semi-transparent turquoise     \"rgba(128, 0, 128, 0.2)\",  # Semi-transparent purple     \"rgba(210, 105, 30, 0.2)\",  # Semi-transparent chocolate     \"rgba(220, 20, 60, 0.2)\",  # Semi-transparent crimson     \"rgba(95, 158, 160, 0.2)\",  # Semi-transparent cadet blue     \"rgba(255, 99, 71, 0.2)\",  # Semi-transparent tomato     \"rgba(144, 238, 144, 0.2)\",  # Semi-transparent light green     \"rgba(70, 130, 180, 0.2)\",  # Semi-transparent steel blue ]   def convert_markdown_to_html(text: str) -&gt; str:     # Convert Markdown to HTML, ensuring embedded HTML is preserved and interpreted correctly.     md_extensions = [         \"extra\",         \"abbr\",         \"attr_list\",         \"def_list\",         \"fenced_code\",         \"footnotes\",         \"md_in_html\",         \"tables\",         \"admonition\",         \"codehilite\",         \"legacy_attrs\",         \"legacy_em\",         \"meta\",         \"nl2br\",         \"sane_lists\",         \"smarty\",         \"toc\",         \"wikilinks\",     ]     return str(md.markdown(text, extensions=md_extensions))   # Utility function to create HTML table with colored results def display_html_table(simple_results: List[str], reranked_results: List[str]) -&gt; None:     # Find all unique values in both lists     unique_values = set(simple_results + reranked_results)      # Ensure we have enough colors for all unique values     # If not, colors will repeat, which might not be ideal but is necessary if the number of unique values exceeds the number of colors     colors = CONTRASTING_COLORS * (len(unique_values) // len(CONTRASTING_COLORS) + 1)      # Create a dictionary to map each unique value to a color     color_map = dict(zip(unique_values, colors))      # Initialize the HTML table with style for equal column widths     html = \"\"\"      Retriever ResultsReranked Results     \"\"\"     # Iterate over the results and assign the corresponding color to each cell     for simple, reranked in zip(simple_results, reranked_results):         html += f\"\"\"          <p>{convert_markdown_to_html(simple)}</p> <p>{convert_markdown_to_html(reranked)}</p>          \"\"\"     html += \"\"     display(HTML(html))   def get_sxs_comparison(     simple_retriever, reranking_api_retriever, query, search_kwargs ) -&gt; List:     simple_results = get_chunk_content(         simple_retriever.invoke(query, search_kwargs=search_kwargs)     )     reranked_results = get_chunk_content(         reranking_api_retriever.invoke(query, search_kwargs=search_kwargs)     )     display_html_table(simple_results, reranked_results)      return reranked_results   def display_grounded_generation(response) -&gt; None:     # Extract the answer with citations and cited chunks     answer_with_citations = response.answer_with_citations     cited_chunks = response.cited_chunks      # Build HTML for the chunks     chunks_html = \"\".join(         [             f\"\"             + f\"Source {index}: {chunk['source'].metadata['source']}\"             + f\"<p>{chunk['chunk_text']}</p>\"             + \"\"             for index, chunk in enumerate(cited_chunks)         ]     )      # Replace citation indices with hoverable spans     for index in range(len(cited_chunks)):         answer_with_citations = answer_with_citations.replace(             f\"[{index}]\",             f\"[{index}]\",         )      # The complete HTML     html_content = f\"\"\"      {answer_with_citations} {chunks_html}      \"\"\"     display(HTML(html_content)) In\u00a0[\u00a0]: Copied! <pre>if CREATE_RESOURCES:\n    print(\"Creating new resources.\")\nelse:\n    print(\"Resource creation is skipped.\")\n\n# Create bucket if not exists\nbucket = create_bucket(GCS_BUCKET_NAME)\n\n# Create vector search index if not exists else return index resource name\nvs_index = create_index()\n\n# Create vector search index endpoint if not exists else return index endpoint resource name\nvs_endpoint = create_index_endpoint()\n\n# Deploy index to the index endpoint\ndeploy_index(vs_index, vs_endpoint)\n\n# Create Document Layout Processor\ndocai_processor = create_docai_processor(processor_display_name=DOCAI_PROCESSOR_NAME)\nPROCESSOR_NAME = docai_processor.name  # DocAI Layout Parser Processor Name\n</pre> if CREATE_RESOURCES:     print(\"Creating new resources.\") else:     print(\"Resource creation is skipped.\")  # Create bucket if not exists bucket = create_bucket(GCS_BUCKET_NAME)  # Create vector search index if not exists else return index resource name vs_index = create_index()  # Create vector search index endpoint if not exists else return index endpoint resource name vs_endpoint = create_index_endpoint()  # Deploy index to the index endpoint deploy_index(vs_index, vs_endpoint)  # Create Document Layout Processor docai_processor = create_docai_processor(processor_display_name=DOCAI_PROCESSOR_NAME) PROCESSOR_NAME = docai_processor.name  # DocAI Layout Parser Processor Name <p>1.1 Read document paths from Cloud Storage bucket</p> <p>Here we are reading documents from a public Cloud Storage bucket with Alphabet investor reports for years 2021, 2022 and 2023. You can replace them with your own documents hosted in Cloud Storage bucket.</p> In\u00a0[\u00a0]: Copied! <pre>loader = CustomGCSDirectoryLoader(\n    project_name=PROJECT_ID,\n    bucket=\"cloud-samples-data\",\n    prefix=\"gen-app-builder/search/alphabet-investor-pdfs\",\n)\n\ndoc_blobs = loader.load(file_pattern=\".*/202[1-3]\")[:2]\n</pre> loader = CustomGCSDirectoryLoader(     project_name=PROJECT_ID,     bucket=\"cloud-samples-data\",     prefix=\"gen-app-builder/search/alphabet-investor-pdfs\", )  doc_blobs = loader.load(file_pattern=\".*/202[1-3]\")[:2] <p>1.2 Parse raw documents and chunk them</p> <p>We will be utilizing the Document AI Layout Parser to read files from Cloud Storage bucket as Blobs and then convert them as layout-aware chunks. Layout Parser extracts document content elements like text, tables, and lists, and creates context-aware chunks that are incredibly useful for building RAG applications.</p> <ul> <li>Define Document AI Layout Parser</li> </ul> In\u00a0[\u00a0]: Copied! <pre>parser = DocAIParser(\n    project_id=PROJECT_ID,\n    location=DOCAI_LOCATION,\n    processor_name=PROCESSOR_NAME,\n    gcs_output_path=GCS_OUTPUT_PATH,\n)\n</pre> parser = DocAIParser(     project_id=PROJECT_ID,     location=DOCAI_LOCATION,     processor_name=PROCESSOR_NAME,     gcs_output_path=GCS_OUTPUT_PATH, ) <ul> <li>Process the documents</li> </ul> In\u00a0[\u00a0]: Copied! <pre>docs = list(\n    parser.batch_parse(\n        doc_blobs,  # filter only last 40 for docs after 2020\n        chunk_size=500,\n        include_ancestor_headings=True,\n    )\n)\n</pre> docs = list(     parser.batch_parse(         doc_blobs,  # filter only last 40 for docs after 2020         chunk_size=500,         include_ancestor_headings=True,     ) ) <ul> <li>Examine a chunk</li> </ul> <p>Let's examine one of the chunks. Notice that the document is parsed into different sections like title, subtitle and even a markdown table (especially a complex table with merged cells!).</p> <p>This makes it easy for retrieval as well for the downstream generation tasks. For example, LLM can now reason more effectively and more accurate.</p> In\u00a0[\u00a0]: Copied! <pre>print(docs[1].page_content)\n</pre> print(docs[1].page_content) <pre></pre> <p>2.1 Define the model for creating embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n\nembedding_model = VertexAIEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n</pre> from langchain_google_vertexai.embeddings import VertexAIEmbeddings  embedding_model = VertexAIEmbeddings(model_name=EMBEDDINGS_MODEL_NAME) <p>2.2 Initialize the Vertex AI Vector Search retriever.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_google_vertexai.vectorstores.vectorstores import VectorSearchVectorStore\n\nvector_store = VectorSearchVectorStore.from_components(\n    project_id=PROJECT_ID,\n    region=REGION,\n    gcs_bucket_name=GCS_BUCKET_NAME,\n    index_id=vs_index.resource_name,\n    endpoint_id=vs_endpoint.resource_name,\n    embedding=embedding_model,\n    stream_update=True,\n)\n</pre> from langchain_google_vertexai.vectorstores.vectorstores import VectorSearchVectorStore  vector_store = VectorSearchVectorStore.from_components(     project_id=PROJECT_ID,     region=REGION,     gcs_bucket_name=GCS_BUCKET_NAME,     index_id=vs_index.resource_name,     endpoint_id=vs_endpoint.resource_name,     embedding=embedding_model,     stream_update=True, ) <p>2.3 Store chunks as embeddings in the Vector Search index and raw texts in the Cloud Storage bucket.</p> \u26a0\ufe0f To skip ingestion and query pre-indexed documents, set  <code>RUN_INGESTION</code> <code>False</code>. \u26a0\ufe0f In\u00a0[\u00a0]: Copied! <pre>add_data(vector_store, docs)\n</pre> add_data(vector_store, docs) <pre></pre> <pre>INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Upserting datapoints MatchingEngineIndex index: projects/503991587623/locations/us-central1/indexes/687806095825043456\nINFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex index Upserted datapoints. Resource name: projects/503991587623/locations/us-central1/indexes/687806095825043456\n</pre> <p></p> <p>More on the Vertex Search Ranking API:</p> <p>The Vertex AI Search Ranking API is one of the standalone APIs in Vertex AI Agent Builder. It takes a list of documents and reranks those documents based on how relevant the documents are to a query. Compared to embeddings, which look only at the semantic similarity of a document and a query, the ranking API can give you precise scores for how well a document answers a given query. The ranking API can be used to improve the quality of search results after retrieving an initial set of candidate documents.</p> <p>The ranking API is stateless so there's no need to index documents before calling the API. All you need to do is pass in the query and documents. This makes the API well suited for reranking documents from any document retrievers.</p> <p>For more information, see Rank and rerank documents.</p> <p>3.1 Define and combine retriever using Vector Search and reranker using the Vertex AI Ranking API.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\nfrom langchain_google_community import VertexAIRank\n\n# Instantiate the VertexAIReranker with the SDK manager\nreranker = VertexAIRank(\n    project_id=PROJECT_ID,\n    location_id=\"global\",\n    ranking_config=\"default_ranking_config\",\n    title_field=\"source\",  # metadata field to preserve with reranked results\n    top_n=5,\n)\n\nbasic_retriever = vector_store.as_retriever(\n    search_kwargs={\"k\": 5}\n)  # fetch top 5 documents\n\n# Create the ContextualCompressionRetriever with the VertexAIRanker as a Reranker\nretriever_with_reranker = ContextualCompressionRetriever(\n    base_compressor=reranker, base_retriever=basic_retriever\n)\n</pre> from langchain.retrievers.contextual_compression import ContextualCompressionRetriever from langchain_google_community import VertexAIRank  # Instantiate the VertexAIReranker with the SDK manager reranker = VertexAIRank(     project_id=PROJECT_ID,     location_id=\"global\",     ranking_config=\"default_ranking_config\",     title_field=\"source\",  # metadata field to preserve with reranked results     top_n=5, )  basic_retriever = vector_store.as_retriever(     search_kwargs={\"k\": 5} )  # fetch top 5 documents  # Create the ContextualCompressionRetriever with the VertexAIRanker as a Reranker retriever_with_reranker = ContextualCompressionRetriever(     base_compressor=reranker, base_retriever=basic_retriever ) <p>3.2 Examine results before and after re-ranking</p> <p>See the difference reranking makes! By prioritizing semantically relevant documents, the Ranking API improves the LLM's context, leading to more accurate and well-reasoned answers. Compare the <code>Retriever Results</code> and the <code>Reranked Results</code> side-by-side to see the improvement.</p> In\u00a0[\u00a0]: Copied! <pre>reranked_results = get_sxs_comparison(\n    simple_retriever=basic_retriever,\n    reranking_api_retriever=retriever_with_reranker,\n    query=\"what was google cloud revenue in 2023 ?\",\n    search_kwargs={\"k\": 5},\n)\n</pre> reranked_results = get_sxs_comparison(     simple_retriever=basic_retriever,     reranking_api_retriever=retriever_with_reranker,     query=\"what was google cloud revenue in 2023 ?\",     search_kwargs={\"k\": 5}, ) Retriever ResultsReranked Results <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>4.1 Define and configure retrieval and answer generation chain</p> <ul> <li>Configure retreiver from the vector store previously defined</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from typing import List\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\n\nfrom langchain.docstore.document import Document\nfrom langchain_core.runnables import chain\n\nfrom langchain_google_vertexai import VertexAI\nfrom langchain.prompts import PromptTemplate\n\nfrom langchain_google_community import VertexAICheckGroundingWrapper\n\nfrom rich import print\n\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n</pre> from typing import List  from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnableParallel, RunnablePassthrough  from langchain.docstore.document import Document from langchain_core.runnables import chain  from langchain_google_vertexai import VertexAI from langchain.prompts import PromptTemplate  from langchain_google_community import VertexAICheckGroundingWrapper  from rich import print  retriever = vector_store.as_retriever(search_kwargs={\"k\": 5}) <ul> <li>Configure LLM with prompt template to generate answer</li> </ul> In\u00a0[\u00a0]: Copied! <pre>llm = VertexAI(model_name=LLM_MODEL_NAME, max_output_tokens=1024)\ntemplate = \"\"\"\nAnswer the question based only on the following context:\n{context}\n\nQuestion:\n{query}\n\"\"\"\nprompt = PromptTemplate.from_template(template)\n\ncreate_answer = prompt | llm\n</pre> llm = VertexAI(model_name=LLM_MODEL_NAME, max_output_tokens=1024) template = \"\"\" Answer the question based only on the following context: {context}  Question: {query} \"\"\" prompt = PromptTemplate.from_template(template)  create_answer = prompt | llm <ul> <li>Define wrapper to call Vertex AI Check Grounding API on the generated answer</li> </ul> In\u00a0[\u00a0]: Copied! <pre>output_parser = VertexAICheckGroundingWrapper(\n    project_id=PROJECT_ID,\n    location_id=\"global\",\n    grounding_config=\"default_grounding_config\",\n    top_n=3,\n)\n</pre> output_parser = VertexAICheckGroundingWrapper(     project_id=PROJECT_ID,     location_id=\"global\",     grounding_config=\"default_grounding_config\",     top_n=3, ) <ul> <li>Define QA chain with Check Grounding</li> </ul> In\u00a0[\u00a0]: Copied! <pre>@chain\ndef check_grounding_output_parser(answer_candidate: str, documents: List[Document]):\n    return output_parser.with_config(configurable={\"documents\": documents}).invoke(\n        answer_candidate\n    )\n\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"query\": RunnablePassthrough()}\n)\n\n\n@chain\ndef qa_with_check_grounding(query):\n    docs = setup_and_retrieval.invoke(query)\n    answer_candidate = create_answer.invoke(docs)\n    check_grounding_output = check_grounding_output_parser.invoke(\n        answer_candidate, documents=docs[\"context\"]\n    )\n    return check_grounding_output\n</pre> @chain def check_grounding_output_parser(answer_candidate: str, documents: List[Document]):     return output_parser.with_config(configurable={\"documents\": documents}).invoke(         answer_candidate     )   setup_and_retrieval = RunnableParallel(     {\"context\": retriever, \"query\": RunnablePassthrough()} )   @chain def qa_with_check_grounding(query):     docs = setup_and_retrieval.invoke(query)     answer_candidate = create_answer.invoke(docs)     check_grounding_output = check_grounding_output_parser.invoke(         answer_candidate, documents=docs[\"context\"]     )     return check_grounding_output <p>4.2 Invoke Generation Generation API Chain.</p> In\u00a0[\u00a0]: Copied! <pre>result = qa_with_check_grounding.invoke(\"what was google cloud revenue in Q1 2021 ?\")\nprint(result)\n</pre> result = qa_with_check_grounding.invoke(\"what was google cloud revenue in Q1 2021 ?\") print(result) <pre>CheckGroundingResponse(\n    support_score=0.6199437975883484,\n    cited_chunks=[\n        {\n            'chunk_text': '# Alphabet Announces Second Quarter 2021 Results\\n\\nMOUNTAIN VIEW, Calif. \u2013 July 27, \n2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended June 30, 2021. \nSundar Pichai, CEO of Google and Alphabet, said: \"In Q2, there was a rising tide of online activity in many parts \nof the world, and we\\'re proud that our services helped so many consumers and businesses. Our long-term investments\nin Al and Google Cloud are helping us drive significant improvements in everyone\\'s digital experience.\" \"Our \nstrong second quarter revenues of $61.9 billion reflect elevated consumer online activity and broad-based strength \nin advertiser spend. Again, we benefited from excellent execution across the board by our teams,\u201d said Ruth Porat, \nCFO of Google and Alphabet.\\n\\n## Q2 2021 financial highlights\\n\\nThe following table summarizes our consolidated \nfinancial results for the quarters ended June 30, 2020 and 2021 (in millions, except for per share information and \npercentages; unaudited).\\n\\n|-|-|\\n|  | Quarter Ended June 30, |\\n|  | 2020 | 2021 |\\n| Revenues | $ $ 38,297 | \n61,880 |\\n| Change in revenues year over year | (2)% | 62% |\\n| Change in constant currency revenues year over \nyear(1) | 0% | 57% |\\n| Operating income | $ 6,383 $ | 19,361 |\\n| Operating margin | 17% | 31 % |\\n| Other income \n(expense), net | $ $ 1,894 | 2,624 |\\n| Net income | $ 6,959 | $ 18,525 |\\n| Diluted EPS | $ 10.13 | $ 27.26 \n|\\n\\n(1) Non-GAAP measure. See the table captioned \"Reconciliation from GAAP revenues to non-GAAP constant currency\nrevenues\" for more details. Q2 2021 supplemental information (in millions, except for number of employees; \nunaudited)\\n\\n## Revenues, Traffic Acquisition Costs (TAC) and number of employees\\n\\n## Segment Operating \nResults\\n\\n|-|-|\\n|  | Quarter Ended June 30, |\\n|  | 2020 | 2021 |\\n| Google Search &amp; other | 21,319 $ | $ 35,845 \n|\\n| YouTube ads | 3,812 | 7,002 |\\n| Google Network | 4,736 | 7,597 |\\n| Google advertising | 29,867 | 50,444 |\\n|\nGoogle other | 5,124 | 6,623 |\\n| Google Services total | 34,991 | 57,067 |\\n| Google Cloud | 3,007 | 4,628 |\\n| \nOther Bets | 148 | 192 |\\n| Hedging gains (losses) | 151 | (7) |\\n| Total revenues | $ 38,297 | $ 61,880 |\\n| Total\nTAC | 6,694 $ | $ 10,929 |\\n| Number of employees | 127,498 | 144,056 |\\n\\n',\n            'source': Document(\n                metadata={\n                    'chunk_id': 'c1',\n                    'source': \n'gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2_alphabet_earnings_release.pdf'\n                },\n                page_content='# Alphabet Announces Second Quarter 2021 Results\\n\\nMOUNTAIN VIEW, Calif. \u2013 July 27, \n2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended June 30, 2021. \nSundar Pichai, CEO of Google and Alphabet, said: \"In Q2, there was a rising tide of online activity in many parts \nof the world, and we\\'re proud that our services helped so many consumers and businesses. Our long-term investments\nin Al and Google Cloud are helping us drive significant improvements in everyone\\'s digital experience.\" \"Our \nstrong second quarter revenues of $61.9 billion reflect elevated consumer online activity and broad-based strength \nin advertiser spend. Again, we benefited from excellent execution across the board by our teams,\u201d said Ruth Porat, \nCFO of Google and Alphabet.\\n\\n## Q2 2021 financial highlights\\n\\nThe following table summarizes our consolidated \nfinancial results for the quarters ended June 30, 2020 and 2021 (in millions, except for per share information and \npercentages; unaudited).\\n\\n|-|-|\\n|  | Quarter Ended June 30, |\\n|  | 2020 | 2021 |\\n| Revenues | $ $ 38,297 | \n61,880 |\\n| Change in revenues year over year | (2)% | 62% |\\n| Change in constant currency revenues year over \nyear(1) | 0% | 57% |\\n| Operating income | $ 6,383 $ | 19,361 |\\n| Operating margin | 17% | 31 % |\\n| Other income \n(expense), net | $ $ 1,894 | 2,624 |\\n| Net income | $ 6,959 | $ 18,525 |\\n| Diluted EPS | $ 10.13 | $ 27.26 \n|\\n\\n(1) Non-GAAP measure. See the table captioned \"Reconciliation from GAAP revenues to non-GAAP constant currency\nrevenues\" for more details. Q2 2021 supplemental information (in millions, except for number of employees; \nunaudited)\\n\\n## Revenues, Traffic Acquisition Costs (TAC) and number of employees\\n\\n## Segment Operating \nResults\\n\\n|-|-|\\n|  | Quarter Ended June 30, |\\n|  | 2020 | 2021 |\\n| Google Search &amp; other | 21,319 $ | $ 35,845 \n|\\n| YouTube ads | 3,812 | 7,002 |\\n| Google Network | 4,736 | 7,597 |\\n| Google advertising | 29,867 | 50,444 |\\n|\nGoogle other | 5,124 | 6,623 |\\n| Google Services total | 34,991 | 57,067 |\\n| Google Cloud | 3,007 | 4,628 |\\n| \nOther Bets | 148 | 192 |\\n| Hedging gains (losses) | 151 | (7) |\\n| Total revenues | $ 38,297 | $ 61,880 |\\n| Total\nTAC | 6,694 $ | $ 10,929 |\\n| Number of employees | 127,498 | 144,056 |\\n\\n'\n            )\n        }\n    ],\n    claims=[\n        {\n            'start_pos': 0,\n            'end_pos': 51,\n            'claim_text': 'Google Cloud revenue in Q1 2021 was $4.047 billion.',\n            'citation_indices': [0]\n        }\n    ],\n    answer_with_citations='Google Cloud revenue in Q1 2021 was $4.047 billion.[0]'\n)\n</pre> <p>4.3 Check grounding</p> In\u00a0[\u00a0]: Copied! <pre>display_grounded_generation(result)\n</pre> display_grounded_generation(result) Google Cloud revenue in Q1 2021 was $4.047 billion.[0] Source 0: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2_alphabet_earnings_release.pdf<p># Alphabet Announces Second Quarter 2021 Results  MOUNTAIN VIEW, Calif. \u2013 July 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended June 30, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \"In Q2, there was a rising tide of online activity in many parts of the world, and we're proud that our services helped so many consumers and businesses. Our long-term investments in Al and Google Cloud are helping us drive significant improvements in everyone's digital experience.\" \"Our strong second quarter revenues of $61.9 billion reflect elevated consumer online activity and broad-based strength in advertiser spend. Again, we benefited from excellent execution across the board by our teams,\u201d said Ruth Porat, CFO of Google and Alphabet.  ## Q2 2021 financial highlights  The following table summarizes our consolidated financial results for the quarters ended June 30, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).  |-|-| |  | Quarter Ended June 30, | |  | 2020 | 2021 | | Revenues | $ $ 38,297 | 61,880 | | Change in revenues year over year | (2)% | 62% | | Change in constant currency revenues year over year(1) | 0% | 57% | | Operating income | $ 6,383 $ | 19,361 | | Operating margin | 17% | 31 % | | Other income (expense), net | $ $ 1,894 | 2,624 | | Net income | $ 6,959 | $ 18,525 | | Diluted EPS | $ 10.13 | $ 27.26 |  (1) Non-GAAP measure. See the table captioned \"Reconciliation from GAAP revenues to non-GAAP constant currency revenues\" for more details. Q2 2021 supplemental information (in millions, except for number of employees; unaudited)  ## Revenues, Traffic Acquisition Costs (TAC) and number of employees  ## Segment Operating Results  |-|-| |  | Quarter Ended June 30, | |  | 2020 | 2021 | | Google Search &amp; other | 21,319 $ | $ 35,845 | | YouTube ads | 3,812 | 7,002 | | Google Network | 4,736 | 7,597 | | Google advertising | 29,867 | 50,444 | | Google other | 5,124 | 6,623 | | Google Services total | 34,991 | 57,067 | | Google Cloud | 3,007 | 4,628 | | Other Bets | 148 | 192 | | Hedging gains (losses) | 151 | (7) | | Total revenues | $ 38,297 | $ 61,880 | | Total TAC | 6,694 $ | $ 10,929 | | Number of employees | 127,498 | 144,056 |  </p> In\u00a0[\u00a0]: Copied! <pre>result = qa_with_check_grounding.invoke(\n    \"what are the main influencing factors on Alphabet revenue in Q1 2021 ?\"\n)\ndisplay_grounded_generation(result)\n</pre> result = qa_with_check_grounding.invoke(     \"what are the main influencing factors on Alphabet revenue in Q1 2021 ?\" ) display_grounded_generation(result) The provided documents mention elevated consumer activity online and broad-based growth in advertiser revenue as the main influencing factors for Alphabet's revenue in Q1 2021.[0][1] Source 0: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1_alphabet_earnings_release.pdf<p># Alphabet Announces First Quarter 2021 Results  MOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We've continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\" Ruth Porat, CFO of Google and Alphabet, said: \"Total revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We're very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\"  ## Q1 2021 financial highlights  The following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).  |-|-| |  | Quarter Ended March 31, | |  | 2020 2021 | | Revenues | $ $ 41,159 55,314 | | Increase in revenues year over year | 13% 34% | | Increase in constant currency revenues year over year(1) | 32% 15% | | Operating income | $ 7,977 $ 16,437 | | Operating margin | 19% 30% | | Other income (expense), net | (220) $ $ 4,846 | | Net income | $ 6,836 $ 17,930 | | Diluted EPS | $ 9.87 $ 26.29 |  (1) Non-GAAP measure. See the table captioned \"Reconciliation from GAAP revenues to non-GAAP constant currency revenues\" for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)  ## Revenues, Traffic Acquisition Costs (TAC) and number of employees  Segment Operating Results  |-|-| |  | Quarter Ended March 31, | |  | 2020 | 2021 | | Google Search &amp; other | 24,502 $ | $ 31,879 | | YouTube ads | 4,038 | 6,005 | | Google Network | 5,223 | 6,800 | | Google advertising | 33,763 | 44,684 | | Google other | 4,435 | 6,494 | | Google Services total | 38,198 | 51,178 | | Google Cloud | 2,777 | 4,047 | | Other Bets | 135 | 198 | | Hedging gains (losses) | 49 | (109) | | Total revenues | 41,159 $ | $ 55,314 | | Total TAC | 7,452 $ | $ 9,712 | | Number of employees | 123,048 | 139,995 |  </p>Source 1: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1_alphabet_earnings_release.pdf<p># Alphabet Announces First Quarter 2021 Results  MOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We've continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\" Ruth Porat, CFO of Google and Alphabet, said: \"Total revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We're very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\"  ## Q1 2021 financial highlights  The following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).  |-|-| |  | Quarter Ended March 31, | |  | 2020 2021 | | Revenues | $ $ 41,159 55,314 | | Increase in revenues year over year | 13% 34% | | Increase in constant currency revenues year over year(1) | 32% 15% | | Operating income | $ 7,977 $ 16,437 | | Operating margin | 19% 30% | | Other income (expense), net | (220) $ $ 4,846 | | Net income | $ 6,836 $ 17,930 | | Diluted EPS | $ 9.87 $ 26.29 |  (1) Non-GAAP measure. See the table captioned \"Reconciliation from GAAP revenues to non-GAAP constant currency revenues\" for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)  ## Revenues, Traffic Acquisition Costs (TAC) and number of employees  Segment Operating Results  |-|-| |  | Quarter Ended March 31, | |  | 2020 | 2021 | | Google Search &amp; other | 24,502 $ | $ 31,879 | | YouTube ads | 4,038 | 6,005 | | Google Network | 5,223 | 6,800 | | Google advertising | 33,763 | 44,684 | | Google other | 4,435 | 6,494 | | Google Services total | 38,198 | 51,178 | | Google Cloud | 2,777 | 4,047 | | Other Bets | 135 | 198 | | Hedging gains (losses) | 49 | (109) | | Total revenues | 41,159 $ | $ 55,314 | | Total TAC | 7,452 $ | $ 9,712 | | Number of employees | 123,048 | 139,995 |  </p> <p>Congratulations!  You created a search engine from source documents, and wired in a real time RAG pipeline to retrieve only the most relevant facts and include them in your LLM generated responses, and you included a grounding verification step to ensure high quality results.</p> <p>If you would like to evaluate your generated answered on more dimensions, take a look at the Vertex Eval Service metrics for RAG and you can get scores and explanationals on many metrics like <code>question_answering_quality</code>, <code>question_answering_relevance</code>, <code>question_answering_helpfulness</code>, <code>groundedness</code>, <code>fulfillment</code>, <code>coherence</code>, <code>toxicity</code>, and more.</p> In\u00a0[\u00a0]: Copied! <pre>DELETE_DOCAI_PROCESSOR = False\nDELETE_INDEX = False\nDELETE_BUCKET = False\n</pre> DELETE_DOCAI_PROCESSOR = False DELETE_INDEX = False DELETE_BUCKET = False <ul> <li>Delete datapoints from Vector Search index</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Delete datapoints from Vertex AI Vector Store\n\n\ndef delete_from_vector_search(\n    vs_index: MatchingEngineIndex,\n    vs_endpoint: MatchingEngineIndexEndpoint,\n    delete: bool = False,\n):\n    neighbors = vs_endpoint.find_neighbors(\n        deployed_index_id=vs_index.deployed_indexes[0].deployed_index_id,\n        queries=[[0.0] * VS_DIMENSIONS],\n        num_neighbors=5000,\n        return_full_datapoint=False,\n    )\n\n    datapoint_ids = [neighbor.id for neighbor in neighbors[0]]\n\n    # Delete datapoints\n    if delete:\n        print(f\"Deleting {len(datapoint_ids)} datapoints\")\n        response = vs_index.remove_datapoints(datapoint_ids=datapoint_ids)\n        print(response)\n\n\ndelete_from_vector_search(vs_index, vs_endpoint, delete=DELETE_INDEX)\n</pre> # Delete datapoints from Vertex AI Vector Store   def delete_from_vector_search(     vs_index: MatchingEngineIndex,     vs_endpoint: MatchingEngineIndexEndpoint,     delete: bool = False, ):     neighbors = vs_endpoint.find_neighbors(         deployed_index_id=vs_index.deployed_indexes[0].deployed_index_id,         queries=[[0.0] * VS_DIMENSIONS],         num_neighbors=5000,         return_full_datapoint=False,     )      datapoint_ids = [neighbor.id for neighbor in neighbors[0]]      # Delete datapoints     if delete:         print(f\"Deleting {len(datapoint_ids)} datapoints\")         response = vs_index.remove_datapoints(datapoint_ids=datapoint_ids)         print(response)   delete_from_vector_search(vs_index, vs_endpoint, delete=DELETE_INDEX) <ul> <li>\ud83d\uddd1\ufe0f Remove Vertex AI Vector Search Index and Endpoint</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if DELETE_INDEX:\n    print(f\"Undeploying all indexes and deleting the index endpoint {vs_endpoint}\")\n    vs_endpoint.undeploy_all()\n    vs_endpoint.delete()\n    print(f\"Deleting the index {vs_index}\")\n    vs_index.delete()\n</pre> if DELETE_INDEX:     print(f\"Undeploying all indexes and deleting the index endpoint {vs_endpoint}\")     vs_endpoint.undeploy_all()     vs_endpoint.delete()     print(f\"Deleting the index {vs_index}\")     vs_index.delete() <ul> <li>\ud83d\uddd1\ufe0f Remove Document AI Processor</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if DELETE_DOCAI_PROCESSOR:\n    docai_client = documentai.DocumentProcessorServiceClient()\n    request = documentai.DeleteProcessorRequest(name=docai_processor.name)\n    operation = docai_client.delete_processor(request=request)\n    print(\"Waiting for delete processor operation to complete...\")\n    response = operation.result()\n    print(response)\n</pre> if DELETE_DOCAI_PROCESSOR:     docai_client = documentai.DocumentProcessorServiceClient()     request = documentai.DeleteProcessorRequest(name=docai_processor.name)     operation = docai_client.delete_processor(request=request)     print(\"Waiting for delete processor operation to complete...\")     response = operation.result()     print(response) <ul> <li>\ud83d\uddd1\ufe0f Remove Google Cloud Storage bucket</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if DELETE_BUCKET:\n    ! gsutil -m rm -r $STAGING_BUCKET_URI\n</pre> if DELETE_BUCKET:     ! gsutil -m rm -r $STAGING_BUCKET_URI"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#build-your-own-grounded-rag-application-using-vertex-ai-apis-for-rag-and-langchain","title":"Build your own Grounded RAG application using Vertex AI APIs for RAG and Langchain\u00b6","text":"Run in Colab       Run in Colab Enterprise       View on GitHub       Open in Vertex AI Workbench"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#overview","title":"\ud83d\udccc Overview\u00b6","text":"<p>In this notebook, we show you how to use Vertex AI Builder APIs for RAG to build a custom search solution on your own documents.</p> <p>Building a robust custom (DIY) Retrieval Augmented Generation (RAG) system for grounding can be challenging.  Vertex AI simplifies the process with a suite of flexible standalone APIs to help your create your own search solutions.</p> <ul> <li>Document AI Layout Parser: Transforms documents into structured representations, making content easily accessible. Creates context-aware chunks for improved information retrieval in generative AI and discovery applications.</li> <li>Ranking API: Re-ranks search results based on relevance to the original query. Enhances RAG accuracy by optimizing retrieval beyond initial nearest neighbor search.</li> <li>Check Grounding API: Acts as a \"validator\" to determine whether statements or claims are supported by provided facts (essentially how grounded a given piece of text is in a given set of reference text). Enables online flagging of ungrounded responses and offline evaluation of generative responses.</li> </ul> <p>Key Features:</p> <ul> <li>Leverage Vertex AI Search technology:  Build custom RAG and Grounded Generation solutions using the same technology that powers Vertex AI Search.</li> <li>Granular control: Tailor your RAG system to specific use cases and offer greater control to your users.</li> <li>Seamless integration: Combine these APIs with core services like Embeddings API and Vector Search for advanced grounded AI applications.</li> </ul> <p>These builder APIS give you full flexibility and control on the design of your RAG application while at the same time offering accelerated time to market and high quality by relying on these lower-level Vertex AI APIs. Refer to the documentation to learn more.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#architecture","title":"\ud83d\udcd0 Architecture\u00b6","text":"<p>Following is a high-level architecture of what we will build in this notebook.</p> <p>You will perform the following steps:</p> <ul> <li><p>Step 1. Data Ingestion: Ingest documents from Cloud Storage bucket to Vertex AI Vector Search (vector database). You parse the documents in Cloud Storage bucket using Cloud Document AI Layout Parser and convert the raw text chunks as embeddings using Vertex AI Embeddings API. The generated embeddings power semantic search using Vector search.</p> </li> <li><p>Step 2. Retrieval: Retrieve relevant chunks from the Vertex AI vector Search for a given user query and re-rank the chunks using Vertex AI Ranking API.</p> </li> <li><p>Step 3. Answer generation: You would use Vertex AI Gemini API to generate an answer for the given user query based on the re-ranked chunks retrieved from the vector search. The generated answer is validated with Vertex AI Check Grounding API to dertermine how grounded the answer is to the relevant chunks retrieved.</p> </li> </ul> <p>The notebook uses LangChain and Google Cloud + LangChain integrations to orcherate the pipeline.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#getting-started","title":"\ud83c\udfac Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.</li> <li>Make sure that billing is enabled for your project.</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API.</li> <li>Enable the Cloud Storage API.</li> <li>Enable the Cloud Document AI API.</li> <li>Enable the Discovery Engine API for your project.</li> </ol>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, including the optional section, you will need to have the Owner role for your project.</p> <p>If you want to skip the optional section, you need at least the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> <li><code>roles/documentai.admin</code> to create and use Document AI Processors</li> <li><code>roles/discoveryengine.admin</code> to modify Vertex AI Search assets</li> </ul>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#install-vertex-ai-sdk-and-other-required-packages","title":"Install Vertex AI SDK and Other Required Packages\u00b6","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and Initialize Vertex AI SDK\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>REGION</code> unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#initialize-variables","title":"Initialize variables\u00b6","text":"<p>Set the values for the name of your project.</p>  \u24d8 You might already have all of these resources created in which case you should use their names and set <code>CREATE_RESOURCES=False</code>. If you do not already have this all created, you should set new names for your cloud storage bucket, index, index endpoint, and docai processor.  <p>TIP: stick to <code>hyphenated-lower-case</code> naming conventions, and use the same project name as a component of each of these names.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#utility-functions-and-custom-langchain-components","title":"Utility functions and Custom LangChain Components\u00b6","text":"<p>We define a few custom LangChain components until these components are merged in the Google Cloud + LangChain integrations.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#initialize-resources","title":"\u2699\ufe0f Initialize resources\u00b6","text":"<p>The DIY RAG application requires the following resources, which will be provisioned by this step if not already present:</p> <ul> <li>Document AI Layout Parser processor to parse the input documents</li> <li>Vertex AI Vector Search index and endpoint to host the index for vector search</li> <li>Cloud Storage bucket to store documents</li> </ul> \u26a0\ufe0f Resource creation will be skipped if <code>CREATE_RESOURCES</code> flag is set to <code>False</code> in the Initialize Variables section.  \u26a0\ufe0f"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#data-ingestion","title":"\ud83d\udce5 Data Ingestion\u00b6","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#document-processing-and-indexing","title":"\ud83d\udcc4 Document Processing and Indexing\u00b6","text":"<p>This steps reads documents from Cloud Storage bucket, parses them using Document AI layout processor, extracts chunks from the parsed document, generates emebeddings using Vertex AI Embeddings API and add them to the Vertex AI Vector Search index.</p> <p>These are some sample public datasets available in GCS for usage.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#step-1-process-documents","title":"Step 1. Process Documents\u00b6","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#step-2-index-the-chunk-embeddings","title":"Step 2: Index the chunk embeddings\u00b6","text":"<p>The previous chunks of text are still just text. This step creates embeddings of the text chunks returned from the layout parser and upserts them into Vertex AI Vector Search index.</p> <p>Next up we will then use Vertex AI Vector Search as a retriever for the RAG pipeline. Vector Search offers blazing fast retrieval that scales to billions of vectors with high recall, resulting in better searches at speed.</p> \u26a0\ufe0f Remember to run the Initialize Resources section to create and configure Vector Search index. \u26a0\ufe0f"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#serving","title":"\ud83e\udd16 Serving\u00b6","text":"<p>All of the setup is done.  You retrieved source documents, processed and chunked them, embedded them into vectors and upserted them into Vector Search.</p> <p>Now it's time to do some searches and generate grounded text.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#retrieval-and-ranking","title":"\ud83d\udd0e Retrieval and Ranking\u00b6","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#step-3-retrieve-and-rerank-chunks","title":"Step 3. Retrieve and Rerank Chunks\u00b6","text":"<p>In this step, Vertex AI Vector Search retrieves the top-k relevant results, which are then reranked by Vertex AI Ranking API based on chunk content and semantic similarity to the query.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-first-quarter-2021-resultsmountain-view-calif-april-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-march-31-2021-sundar-pichai-ceo-of-google-and-alphabet-said-over-the-last-year-people-have-turned-to-google-search-and-many-online-services-to-stay-informed-connected-and-entertained-weve-continued-our-focus-on-delivering-trusted-services-to-help-people-around-the-world-our-cloud-services-are-helping-businesses-big-and-small-accelerate-their-digital-transformations-ruth-porat-cfo-of-google-and-alphabet-said-total-revenues-of-553-billion-in-the-first-quarter-reflect-elevated-consumer-activity-online-and-broad-based-growth-in-advertiser-revenue-were-very-pleased-with-the-ongoing-momentum-in-google-cloud-with-revenues-of-40-billion-in-the-quarter-reflecting-strength-and-opportunity-in-both-gcp-and-workspace-q1-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-march-31-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-march-31-2020-2021-revenues-41159-55314-increase-in-revenues-year-over-year-13-34-increase-in-constant-currency-revenues-year-over-year1-32-15-operating-income-7977-16437-operating-margin-19-30-other-income-expense-net-220-4846-net-income-6836-17930-diluted-eps-987-2629-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q1-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employeessegment-operating-results-quarter-ended-march-31-2020-2021-google-search-other-24502-31879-youtube-ads-4038-6005-google-network-5223-6800-google-advertising-33763-44684-google-other-4435-6494-google-services-total-38198-51178-google-cloud-2777-4047-other-bets-135-198-hedging-gains-losses-49-109-total-revenues-41159-55314-total-tac-7452-9712-number-of-employees-123048-139995-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q1alphabetearnings_releasepdf","title":"Alphabet Announces First Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We\u2019ve continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\u201d Ruth Porat, CFO of Google and Alphabet, said: \u201cTotal revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We\u2019re very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\u201d## Q1 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended March 31, ||  | 2020 2021 || Revenues | $ $ 41,159 55,314 || Increase in revenues year over year | 13% 34% || Increase in constant currency revenues year over year(1) | 32% 15% || Operating income | $ 7,977 $ 16,437 || Operating margin | 19% 30% || Other income (expense), net | (220) $ $ 4,846 || Net income | $ 6,836 $ 17,930 || Diluted EPS | $ 9.87 $ 26.29 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employeesSegment Operating Results|-|-||  | Quarter Ended March 31, ||  | 2020 | 2021 || Google Search &amp; other | 24,502 $ | $ 31,879 || YouTube ads | 4,038 | 6,005 || Google Network | 5,223 | 6,800 || Google advertising | 33,763 | 44,684 || Google other | 4,435 | 6,494 || Google Services total | 38,198 | 51,178 || Google Cloud | 2,777 | 4,047 || Other Bets | 135 | 198 || Hedging gains (losses) | 49 | (109) || Total revenues | 41,159 $ | $ 55,314 || Total TAC | 7,452 $ | $ 9,712 || Number of employees | 123,048 | 139,995 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#segment-resultsthe-following-table-presents-our-revenues-and-operating-income-loss-in-millions-unaudited-we-report-our-segment-results-as-google-services-google-cloud-and-other-bets-quarter-ended-june-30-revenues-2020-2021-google-services-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-quarter-ended-june-30-2020-2021-operating-income-loss-google-services-9539-22343-google-cloud-1426-591-other-bets-1116-1398-corporate-costs-unallocated-614-993-total-income-from-operations-6383-19361-google-services-includes-products-and-services-such-as-ads-android-chrome-hardware-google-maps-google-play-search-and-youtube-google-services-generates-revenues-primarily-from-advertising-sales-of-apps-in-app-purchases-digital-content-products-and-hardware-and-fees-received-for-subscription-based-products-such-as-youtube-premium-and-youtube-tv-google-cloud-includes-googles-infrastructure-and-data-analytics-platforms-collaboration-tools-and-other-services-for-enterprise-customers-google-cloud-generates-revenues-primarily-from-fees-received-for-google-cloud-platform-services-and-google-workspace-collaboration-tools-other-bets-is-a-combination-of-multiple-operating-segments-that-are-not-individually-material-revenues-from-the-other-bets-are-derived-primarily-through-the-sale-of-internet-services-as-well-as-licensing-and-rd-services-unallocated-corporate-costs-primarily-include-corporate-initiatives-corporate-shared-costs-such-as-finance-and-legal-including-certain-fines-and-settlements-as-well-as-costs-associated-with-certain-shared-research-and-development-activities-additionally-hedging-gains-losses-related-to-revenue-are-included-in-corporate-costs-10-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Segment resultsThe following table presents our revenues and operating income (loss) (in millions; unaudited): We report our segment results as Google Services, Google Cloud, and Other Bets:|-|-||  | Quarter Ended June 30, || Revenues: | 2020 | 2021 || Google Services | 34,991 $ | $ 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | 38,297 $ | $ 61,880 ||-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Operating income (loss): |  |  || Google Services | 9,539 $ | $ 22,343 || Google Cloud | (1,426) | (591) || Other Bets | (1,116) | (1,398) || Corporate costs, unallocated | (614) | (993) || Total income from operations | 6,383 $ | $ 19,361 |\u2022 Google Services includes products and services such as ads, Android, Chrome, hardware, Google Maps, Google Play, Search, and YouTube. Google Services generates revenues primarily from advertising; sales of apps, in-app purchases, digital content products, and hardware; and fees received for subscription-based products such as YouTube Premium and YouTube TV. \u2022 Google Cloud includes Google\u2019s infrastructure and data analytics platforms, collaboration tools, and other services for enterprise customers. Google Cloud generates revenues primarily from fees received for Google Cloud Platform services and Google Workspace collaboration tools. Other Bets is a combination of multiple operating segments that are not individually material. Revenues from the Other Bets are derived primarily through the sale of internet services as well as licensing and R&amp;D services. Unallocated corporate costs primarily include corporate initiatives, corporate shared costs, such as finance and legal, including certain fines and settlements, as well as costs associated with certain shared research and development activities. Additionally, hedging gains (losses) related to revenue are included in corporate costs. 10 Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-first-quarter-2021-resultsmountain-view-calif-april-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-march-31-2021-sundar-pichai-ceo-of-google-and-alphabet-said-over-the-last-year-people-have-turned-to-google-search-and-many-online-services-to-stay-informed-connected-and-entertained-weve-continued-our-focus-on-delivering-trusted-services-to-help-people-around-the-world-our-cloud-services-are-helping-businesses-big-and-small-accelerate-their-digital-transformations-ruth-porat-cfo-of-google-and-alphabet-said-total-revenues-of-553-billion-in-the-first-quarter-reflect-elevated-consumer-activity-online-and-broad-based-growth-in-advertiser-revenue-were-very-pleased-with-the-ongoing-momentum-in-google-cloud-with-revenues-of-40-billion-in-the-quarter-reflecting-strength-and-opportunity-in-both-gcp-and-workspace-q1-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-march-31-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-march-31-2020-2021-revenues-41159-55314-increase-in-revenues-year-over-year-13-34-increase-in-constant-currency-revenues-year-over-year1-32-15-operating-income-7977-16437-operating-margin-19-30-other-income-expense-net-220-4846-net-income-6836-17930-diluted-eps-987-2629-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q1-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employeessegment-operating-results-quarter-ended-march-31-2020-2021-google-search-other-24502-31879-youtube-ads-4038-6005-google-network-5223-6800-google-advertising-33763-44684-google-other-4435-6494-google-services-total-38198-51178-google-cloud-2777-4047-other-bets-135-198-hedging-gains-losses-49-109-total-revenues-41159-55314-total-tac-7452-9712-number-of-employees-123048-139995-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q1alphabetearnings_releasepdf","title":"Alphabet Announces First Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We\u2019ve continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\u201d Ruth Porat, CFO of Google and Alphabet, said: \u201cTotal revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We\u2019re very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\u201d## Q1 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended March 31, ||  | 2020 2021 || Revenues | $ $ 41,159 55,314 || Increase in revenues year over year | 13% 34% || Increase in constant currency revenues year over year(1) | 32% 15% || Operating income | $ 7,977 $ 16,437 || Operating margin | 19% 30% || Other income (expense), net | (220) $ $ 4,846 || Net income | $ 6,836 $ 17,930 || Diluted EPS | $ 9.87 $ 26.29 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employeesSegment Operating Results|-|-||  | Quarter Ended March 31, ||  | 2020 | 2021 || Google Search &amp; other | 24,502 $ | $ 31,879 || YouTube ads | 4,038 | 6,005 || Google Network | 5,223 | 6,800 || Google advertising | 33,763 | 44,684 || Google other | 4,435 | 6,494 || Google Services total | 38,198 | 51,178 || Google Cloud | 2,777 | 4,047 || Other Bets | 135 | 198 || Hedging gains (losses) | 49 | (109) || Total revenues | 41,159 $ | $ 55,314 || Total TAC | 7,452 $ | $ 9,712 || Number of employees | 123,048 | 139,995 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#segment-resultsthe-following-table-presents-our-revenues-and-operating-income-loss-in-millions-unaudited-we-report-our-segment-results-as-google-services-google-cloud-and-other-bets-quarter-ended-june-30-revenues-2020-2021-google-services-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-quarter-ended-june-30-2020-2021-operating-income-loss-google-services-9539-22343-google-cloud-1426-591-other-bets-1116-1398-corporate-costs-unallocated-614-993-total-income-from-operations-6383-19361-google-services-includes-products-and-services-such-as-ads-android-chrome-hardware-google-maps-google-play-search-and-youtube-google-services-generates-revenues-primarily-from-advertising-sales-of-apps-in-app-purchases-digital-content-products-and-hardware-and-fees-received-for-subscription-based-products-such-as-youtube-premium-and-youtube-tv-google-cloud-includes-googles-infrastructure-and-data-analytics-platforms-collaboration-tools-and-other-services-for-enterprise-customers-google-cloud-generates-revenues-primarily-from-fees-received-for-google-cloud-platform-services-and-google-workspace-collaboration-tools-other-bets-is-a-combination-of-multiple-operating-segments-that-are-not-individually-material-revenues-from-the-other-bets-are-derived-primarily-through-the-sale-of-internet-services-as-well-as-licensing-and-rd-services-unallocated-corporate-costs-primarily-include-corporate-initiatives-corporate-shared-costs-such-as-finance-and-legal-including-certain-fines-and-settlements-as-well-as-costs-associated-with-certain-shared-research-and-development-activities-additionally-hedging-gains-losses-related-to-revenue-are-included-in-corporate-costs-10-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Segment resultsThe following table presents our revenues and operating income (loss) (in millions; unaudited): We report our segment results as Google Services, Google Cloud, and Other Bets:|-|-||  | Quarter Ended June 30, || Revenues: | 2020 | 2021 || Google Services | 34,991 $ | $ 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | 38,297 $ | $ 61,880 ||-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Operating income (loss): |  |  || Google Services | 9,539 $ | $ 22,343 || Google Cloud | (1,426) | (591) || Other Bets | (1,116) | (1,398) || Corporate costs, unallocated | (614) | (993) || Total income from operations | 6,383 $ | $ 19,361 |\u2022 Google Services includes products and services such as ads, Android, Chrome, hardware, Google Maps, Google Play, Search, and YouTube. Google Services generates revenues primarily from advertising; sales of apps, in-app purchases, digital content products, and hardware; and fees received for subscription-based products such as YouTube Premium and YouTube TV. \u2022 Google Cloud includes Google\u2019s infrastructure and data analytics platforms, collaboration tools, and other services for enterprise customers. Google Cloud generates revenues primarily from fees received for Google Cloud Platform services and Google Workspace collaboration tools. Other Bets is a combination of multiple operating segments that are not individually material. Revenues from the Other Bets are derived primarily through the sale of internet services as well as licensing and R&amp;D services. Unallocated corporate costs primarily include corporate initiatives, corporate shared costs, such as finance and legal, including certain fines and settlements, as well as costs associated with certain shared research and development activities. Additionally, hedging gains (losses) related to revenue are included in corporate costs. 10 Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#segment-resultsthe-following-table-presents-our-revenues-and-operating-income-loss-in-millions-unaudited-we-report-our-segment-results-as-google-services-google-cloud-and-other-bets-quarter-ended-june-30-revenues-2020-2021-google-services-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-quarter-ended-june-30-2020-2021-operating-income-loss-google-services-9539-22343-google-cloud-1426-591-other-bets-1116-1398-corporate-costs-unallocated-614-993-total-income-from-operations-6383-19361-google-services-includes-products-and-services-such-as-ads-android-chrome-hardware-google-maps-google-play-search-and-youtube-google-services-generates-revenues-primarily-from-advertising-sales-of-apps-in-app-purchases-digital-content-products-and-hardware-and-fees-received-for-subscription-based-products-such-as-youtube-premium-and-youtube-tv-google-cloud-includes-googles-infrastructure-and-data-analytics-platforms-collaboration-tools-and-other-services-for-enterprise-customers-google-cloud-generates-revenues-primarily-from-fees-received-for-google-cloud-platform-services-and-google-workspace-collaboration-tools-other-bets-is-a-combination-of-multiple-operating-segments-that-are-not-individually-material-revenues-from-the-other-bets-are-derived-primarily-through-the-sale-of-internet-services-as-well-as-licensing-and-rd-services-unallocated-corporate-costs-primarily-include-corporate-initiatives-corporate-shared-costs-such-as-finance-and-legal-including-certain-fines-and-settlements-as-well-as-costs-associated-with-certain-shared-research-and-development-activities-additionally-hedging-gains-losses-related-to-revenue-are-included-in-corporate-costs-10-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Segment resultsThe following table presents our revenues and operating income (loss) (in millions; unaudited): We report our segment results as Google Services, Google Cloud, and Other Bets:|-|-||  | Quarter Ended June 30, || Revenues: | 2020 | 2021 || Google Services | 34,991 $ | $ 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | 38,297 $ | $ 61,880 ||-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Operating income (loss): |  |  || Google Services | 9,539 $ | $ 22,343 || Google Cloud | (1,426) | (591) || Other Bets | (1,116) | (1,398) || Corporate costs, unallocated | (614) | (993) || Total income from operations | 6,383 $ | $ 19,361 |\u2022 Google Services includes products and services such as ads, Android, Chrome, hardware, Google Maps, Google Play, Search, and YouTube. Google Services generates revenues primarily from advertising; sales of apps, in-app purchases, digital content products, and hardware; and fees received for subscription-based products such as YouTube Premium and YouTube TV. \u2022 Google Cloud includes Google\u2019s infrastructure and data analytics platforms, collaboration tools, and other services for enterprise customers. Google Cloud generates revenues primarily from fees received for Google Cloud Platform services and Google Workspace collaboration tools. Other Bets is a combination of multiple operating segments that are not individually material. Revenues from the Other Bets are derived primarily through the sale of internet services as well as licensing and R&amp;D services. Unallocated corporate costs primarily include corporate initiatives, corporate shared costs, such as finance and legal, including certain fines and settlements, as well as costs associated with certain shared research and development activities. Additionally, hedging gains (losses) related to revenue are included in corporate costs. 10 Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-first-quarter-2021-resultsmountain-view-calif-april-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-march-31-2021-sundar-pichai-ceo-of-google-and-alphabet-said-over-the-last-year-people-have-turned-to-google-search-and-many-online-services-to-stay-informed-connected-and-entertained-weve-continued-our-focus-on-delivering-trusted-services-to-help-people-around-the-world-our-cloud-services-are-helping-businesses-big-and-small-accelerate-their-digital-transformations-ruth-porat-cfo-of-google-and-alphabet-said-total-revenues-of-553-billion-in-the-first-quarter-reflect-elevated-consumer-activity-online-and-broad-based-growth-in-advertiser-revenue-were-very-pleased-with-the-ongoing-momentum-in-google-cloud-with-revenues-of-40-billion-in-the-quarter-reflecting-strength-and-opportunity-in-both-gcp-and-workspace-q1-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-march-31-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-march-31-2020-2021-revenues-41159-55314-increase-in-revenues-year-over-year-13-34-increase-in-constant-currency-revenues-year-over-year1-32-15-operating-income-7977-16437-operating-margin-19-30-other-income-expense-net-220-4846-net-income-6836-17930-diluted-eps-987-2629-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q1-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employeessegment-operating-results-quarter-ended-march-31-2020-2021-google-search-other-24502-31879-youtube-ads-4038-6005-google-network-5223-6800-google-advertising-33763-44684-google-other-4435-6494-google-services-total-38198-51178-google-cloud-2777-4047-other-bets-135-198-hedging-gains-losses-49-109-total-revenues-41159-55314-total-tac-7452-9712-number-of-employees-123048-139995-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q1alphabetearnings_releasepdf","title":"Alphabet Announces First Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We\u2019ve continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\u201d Ruth Porat, CFO of Google and Alphabet, said: \u201cTotal revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We\u2019re very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\u201d## Q1 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended March 31, ||  | 2020 2021 || Revenues | $ $ 41,159 55,314 || Increase in revenues year over year | 13% 34% || Increase in constant currency revenues year over year(1) | 32% 15% || Operating income | $ 7,977 $ 16,437 || Operating margin | 19% 30% || Other income (expense), net | (220) $ $ 4,846 || Net income | $ 6,836 $ 17,930 || Diluted EPS | $ 9.87 $ 26.29 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employeesSegment Operating Results|-|-||  | Quarter Ended March 31, ||  | 2020 | 2021 || Google Search &amp; other | 24,502 $ | $ 31,879 || YouTube ads | 4,038 | 6,005 || Google Network | 5,223 | 6,800 || Google advertising | 33,763 | 44,684 || Google other | 4,435 | 6,494 || Google Services total | 38,198 | 51,178 || Google Cloud | 2,777 | 4,047 || Other Bets | 135 | 198 || Hedging gains (losses) | 49 | (109) || Total revenues | 41,159 $ | $ 55,314 || Total TAC | 7,452 $ | $ 9,712 || Number of employees | 123,048 | 139,995 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#segment-resultsthe-following-table-presents-our-revenues-and-operating-income-loss-in-millions-unaudited-we-report-our-segment-results-as-google-services-google-cloud-and-other-bets-quarter-ended-june-30-revenues-2020-2021-google-services-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-quarter-ended-june-30-2020-2021-operating-income-loss-google-services-9539-22343-google-cloud-1426-591-other-bets-1116-1398-corporate-costs-unallocated-614-993-total-income-from-operations-6383-19361-google-services-includes-products-and-services-such-as-ads-android-chrome-hardware-google-maps-google-play-search-and-youtube-google-services-generates-revenues-primarily-from-advertising-sales-of-apps-in-app-purchases-digital-content-products-and-hardware-and-fees-received-for-subscription-based-products-such-as-youtube-premium-and-youtube-tv-google-cloud-includes-googles-infrastructure-and-data-analytics-platforms-collaboration-tools-and-other-services-for-enterprise-customers-google-cloud-generates-revenues-primarily-from-fees-received-for-google-cloud-platform-services-and-google-workspace-collaboration-tools-other-bets-is-a-combination-of-multiple-operating-segments-that-are-not-individually-material-revenues-from-the-other-bets-are-derived-primarily-through-the-sale-of-internet-services-as-well-as-licensing-and-rd-services-unallocated-corporate-costs-primarily-include-corporate-initiatives-corporate-shared-costs-such-as-finance-and-legal-including-certain-fines-and-settlements-as-well-as-costs-associated-with-certain-shared-research-and-development-activities-additionally-hedging-gains-losses-related-to-revenue-are-included-in-corporate-costs-10-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Segment resultsThe following table presents our revenues and operating income (loss) (in millions; unaudited): We report our segment results as Google Services, Google Cloud, and Other Bets:|-|-||  | Quarter Ended June 30, || Revenues: | 2020 | 2021 || Google Services | 34,991 $ | $ 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | 38,297 $ | $ 61,880 ||-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Operating income (loss): |  |  || Google Services | 9,539 $ | $ 22,343 || Google Cloud | (1,426) | (591) || Other Bets | (1,116) | (1,398) || Corporate costs, unallocated | (614) | (993) || Total income from operations | 6,383 $ | $ 19,361 |\u2022 Google Services includes products and services such as ads, Android, Chrome, hardware, Google Maps, Google Play, Search, and YouTube. Google Services generates revenues primarily from advertising; sales of apps, in-app purchases, digital content products, and hardware; and fees received for subscription-based products such as YouTube Premium and YouTube TV. \u2022 Google Cloud includes Google\u2019s infrastructure and data analytics platforms, collaboration tools, and other services for enterprise customers. Google Cloud generates revenues primarily from fees received for Google Cloud Platform services and Google Workspace collaboration tools. Other Bets is a combination of multiple operating segments that are not individually material. Revenues from the Other Bets are derived primarily through the sale of internet services as well as licensing and R&amp;D services. Unallocated corporate costs primarily include corporate initiatives, corporate shared costs, such as finance and legal, including certain fines and settlements, as well as costs associated with certain shared research and development activities. Additionally, hedging gains (losses) related to revenue are included in corporate costs. 10 Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-first-quarter-2021-resultsmountain-view-calif-april-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-march-31-2021-sundar-pichai-ceo-of-google-and-alphabet-said-over-the-last-year-people-have-turned-to-google-search-and-many-online-services-to-stay-informed-connected-and-entertained-weve-continued-our-focus-on-delivering-trusted-services-to-help-people-around-the-world-our-cloud-services-are-helping-businesses-big-and-small-accelerate-their-digital-transformations-ruth-porat-cfo-of-google-and-alphabet-said-total-revenues-of-553-billion-in-the-first-quarter-reflect-elevated-consumer-activity-online-and-broad-based-growth-in-advertiser-revenue-were-very-pleased-with-the-ongoing-momentum-in-google-cloud-with-revenues-of-40-billion-in-the-quarter-reflecting-strength-and-opportunity-in-both-gcp-and-workspace-q1-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-march-31-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-march-31-2020-2021-revenues-41159-55314-increase-in-revenues-year-over-year-13-34-increase-in-constant-currency-revenues-year-over-year1-32-15-operating-income-7977-16437-operating-margin-19-30-other-income-expense-net-220-4846-net-income-6836-17930-diluted-eps-987-2629-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q1-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employeessegment-operating-results-quarter-ended-march-31-2020-2021-google-search-other-24502-31879-youtube-ads-4038-6005-google-network-5223-6800-google-advertising-33763-44684-google-other-4435-6494-google-services-total-38198-51178-google-cloud-2777-4047-other-bets-135-198-hedging-gains-losses-49-109-total-revenues-41159-55314-total-tac-7452-9712-number-of-employees-123048-139995-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q1alphabetearnings_releasepdf","title":"Alphabet Announces First Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 April 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended March 31, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cOver the last year, people have turned to Google Search and many online services to stay informed, connected and entertained. We\u2019ve continued our focus on delivering trusted services to help people around the world. Our Cloud services are helping businesses, big and small, accelerate their digital transformations.\u201d Ruth Porat, CFO of Google and Alphabet, said: \u201cTotal revenues of $55.3 billion in the first quarter reflect elevated consumer activity online and broad based growth in advertiser revenue. We\u2019re very pleased with the ongoing momentum in Google Cloud, with revenues of $4.0 billion in the quarter reflecting strength and opportunity in both GCP and Workspace.\u201d## Q1 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended March 31, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended March 31, ||  | 2020 2021 || Revenues | $ $ 41,159 55,314 || Increase in revenues year over year | 13% 34% || Increase in constant currency revenues year over year(1) | 32% 15% || Operating income | $ 7,977 $ 16,437 || Operating margin | 19% 30% || Other income (expense), net | (220) $ $ 4,846 || Net income | $ 6,836 $ 17,930 || Diluted EPS | $ 9.87 $ 26.29 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q1 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employeesSegment Operating Results|-|-||  | Quarter Ended March 31, ||  | 2020 | 2021 || Google Search &amp; other | 24,502 $ | $ 31,879 || YouTube ads | 4,038 | 6,005 || Google Network | 5,223 | 6,800 || Google advertising | 33,763 | 44,684 || Google other | 4,435 | 6,494 || Google Services total | 38,198 | 51,178 || Google Cloud | 2,777 | 4,047 || Other Bets | 135 | 198 || Hedging gains (losses) | 49 | (109) || Total revenues | 41,159 $ | $ 55,314 || Total TAC | 7,452 $ | $ 9,712 || Number of employees | 123,048 | 139,995 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q1alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-second-quarter-2021-resultsmountain-view-calif-july-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-june-30-2021-sundar-pichai-ceo-of-google-and-alphabet-said-in-q2-there-was-a-rising-tide-of-online-activity-in-many-parts-of-the-world-and-were-proud-that-our-services-helped-so-many-consumers-and-businesses-our-long-term-investments-in-al-and-google-cloud-are-helping-us-drive-significant-improvements-in-everyones-digital-experience-our-strong-second-quarter-revenues-of-619-billion-reflect-elevated-consumer-online-activity-and-broad-based-strength-in-advertiser-spend-again-we-benefited-from-excellent-execution-across-the-board-by-our-teams-said-ruth-porat-cfo-of-google-and-alphabet-q2-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-june-30-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-june-30-2020-2021-revenues-38297-61880-change-in-revenues-year-over-year-2-62-change-in-constant-currency-revenues-year-over-year1-0-57-operating-income-6383-19361-operating-margin-17-31-other-income-expense-net-1894-2624-net-income-6959-18525-diluted-eps-1013-2726-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q2-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employees-segment-operating-results-quarter-ended-june-30-2020-2021-google-search-other-21319-35845-youtube-ads-3812-7002-google-network-4736-7597-google-advertising-29867-50444-google-other-5124-6623-google-services-total-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-total-tac-6694-10929-number-of-employees-127498-144056-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Alphabet Announces Second Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 July 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended June 30, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cIn Q2, there was a rising tide of online activity in many parts of the world, and we\u2019re proud that our services helped so many consumers and businesses. Our long-term investments in Al and Google Cloud are helping us drive significant improvements in everyone\u2019s digital experience.\u201d \u201cOur strong second quarter revenues of $61.9 billion reflect elevated consumer online activity and broad-based strength in advertiser spend. Again, we benefited from excellent execution across the board by our teams,\u201d said Ruth Porat, CFO of Google and Alphabet.## Q2 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended June 30, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Revenues | $ $ 38,297 | 61,880 || Change in revenues year over year | (2)% | 62% || Change in constant currency revenues year over year(1) | 0% | 57% || Operating income | $ 6,383 $ | 19,361 || Operating margin | 17% | 31 % || Other income (expense), net | $ $ 1,894 | 2,624 || Net income | $ 6,959 | $ 18,525 || Diluted EPS | $ 10.13 | $ 27.26 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q2 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employees## Segment Operating Results|-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Google Search &amp; other | 21,319 $ | $ 35,845 || YouTube ads | 3,812 | 7,002 || Google Network | 4,736 | 7,597 || Google advertising | 29,867 | 50,444 || Google other | 5,124 | 6,623 || Google Services total | 34,991 | 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | $ 38,297 | $ 61,880 || Total TAC | 6,694 $ | $ 10,929 || Number of employees | 127,498 | 144,056 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#alphabet-announces-second-quarter-2021-resultsmountain-view-calif-july-27-2021-alphabet-inc-nasdaq-goog-googl-today-announced-financial-results-for-the-quarter-ended-june-30-2021-sundar-pichai-ceo-of-google-and-alphabet-said-in-q2-there-was-a-rising-tide-of-online-activity-in-many-parts-of-the-world-and-were-proud-that-our-services-helped-so-many-consumers-and-businesses-our-long-term-investments-in-al-and-google-cloud-are-helping-us-drive-significant-improvements-in-everyones-digital-experience-our-strong-second-quarter-revenues-of-619-billion-reflect-elevated-consumer-online-activity-and-broad-based-strength-in-advertiser-spend-again-we-benefited-from-excellent-execution-across-the-board-by-our-teams-said-ruth-porat-cfo-of-google-and-alphabet-q2-2021-financial-highlightsthe-following-table-summarizes-our-consolidated-financial-results-for-the-quarters-ended-june-30-2020-and-2021-in-millions-except-for-per-share-information-and-percentages-unaudited-quarter-ended-june-30-2020-2021-revenues-38297-61880-change-in-revenues-year-over-year-2-62-change-in-constant-currency-revenues-year-over-year1-0-57-operating-income-6383-19361-operating-margin-17-31-other-income-expense-net-1894-2624-net-income-6959-18525-diluted-eps-1013-2726-1-non-gaap-measure-see-the-table-captioned-reconciliation-from-gaap-revenues-to-non-gaap-constant-currency-revenues-for-more-details-q2-2021-supplemental-information-in-millions-except-for-number-of-employees-unaudited-revenues-traffic-acquisition-costs-tac-and-number-of-employees-segment-operating-results-quarter-ended-june-30-2020-2021-google-search-other-21319-35845-youtube-ads-3812-7002-google-network-4736-7597-google-advertising-29867-50444-google-other-5124-6623-google-services-total-34991-57067-google-cloud-3007-4628-other-bets-148-192-hedging-gains-losses-151-7-total-revenues-38297-61880-total-tac-6694-10929-number-of-employees-127498-144056-source-gscloud-samples-datagen-app-buildersearchalphabet-investor-pdfs2021q2alphabetearnings_releasepdf","title":"Alphabet Announces Second Quarter 2021 ResultsMOUNTAIN VIEW, Calif. \u2013 July 27, 2021 \u2013 Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial results for the quarter ended June 30, 2021. Sundar Pichai, CEO of Google and Alphabet, said: \u201cIn Q2, there was a rising tide of online activity in many parts of the world, and we\u2019re proud that our services helped so many consumers and businesses. Our long-term investments in Al and Google Cloud are helping us drive significant improvements in everyone\u2019s digital experience.\u201d \u201cOur strong second quarter revenues of $61.9 billion reflect elevated consumer online activity and broad-based strength in advertiser spend. Again, we benefited from excellent execution across the board by our teams,\u201d said Ruth Porat, CFO of Google and Alphabet.## Q2 2021 financial highlightsThe following table summarizes our consolidated financial results for the quarters ended June 30, 2020 and 2021 (in millions, except for per share information and percentages; unaudited).|-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Revenues | $ $ 38,297 | 61,880 || Change in revenues year over year | (2)% | 62% || Change in constant currency revenues year over year(1) | 0% | 57% || Operating income | $ 6,383 $ | 19,361 || Operating margin | 17% | 31 % || Other income (expense), net | $ $ 1,894 | 2,624 || Net income | $ 6,959 | $ 18,525 || Diluted EPS | $ 10.13 | $ 27.26 |(1) Non-GAAP measure. See the table captioned \u201cReconciliation from GAAP revenues to non-GAAP constant currency revenues\u201d for more details. Q2 2021 supplemental information (in millions, except for number of employees; unaudited)## Revenues, Traffic Acquisition Costs (TAC) and number of employees## Segment Operating Results|-|-||  | Quarter Ended June 30, ||  | 2020 | 2021 || Google Search &amp; other | 21,319 $ | $ 35,845 || YouTube ads | 3,812 | 7,002 || Google Network | 4,736 | 7,597 || Google advertising | 29,867 | 50,444 || Google other | 5,124 | 6,623 || Google Services total | 34,991 | 57,067 || Google Cloud | 3,007 | 4,628 || Other Bets | 148 | 192 || Hedging gains (losses) | 151 | (7) || Total revenues | $ 38,297 | $ 61,880 || Total TAC | 6,694 $ | $ 10,929 || Number of employees | 127,498 | 144,056 | Source: gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2alphabetearnings_release.pdf","text":""},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#answer-generation","title":"\ud83d\udcac Answer Generation\u00b6","text":"<p>You have retrieved the most relevant facts from the all of your indexed source data.  Now we pass those facts into the LLM for answer generation, which will be grounded on the facts.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#step-4-query-in-real-time-and-check-grounding","title":"Step 4. Query in Real Time and Check Grounding\u00b6","text":"<p>Let's now configure a standard retrieval and answer generation chain that follows: <code>query</code> -&gt; <code>vector search</code> -&gt; <code>retrieve documents</code> -&gt; <code>LLM for answer generation</code> with a couple of changes:</p> <ol> <li><p>We will pass retrieved documents to the reranker API via the <code>VertexAIRank</code> and get the reranked documents to generate the answer.</p> </li> <li><p>After the answer is generated by the LLM, pass the answer and the retrieved documents from vector search as facts to the <code>VertexAICheckGroundingWrapper</code> to check how grounded the response from the LLM is.</p> </li> </ol> <p>More on the Vertex AI Check Grounding API:</p> <p>The Vertex AI Check Grounding API is one of the standalone APIs in Vertex AI Agent Builder. It is used to determine how grounded a piece of text (called an answer candidate) is in a given set of reference texts (called facts).</p> <p>The Check Grounding API returns an overall support score of 0 to 1, which indicates how much the answer candidate agrees with the given facts. The response also includes citations to the facts supporting each claim in the answer candidate.</p> <p>You can use the Check Grounding API for checking any piece of text. It could be a human-generated blurb or a machine-generated response. A typical use case would be to check an LLM-generated response with respect to a given set of facts. Among other things, the citations generated by the API would help distinguish hallucinated claim in the response from grounded claims.</p> <p>For more information, see Check Grounding.</p>"},{"location":"genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai_apis/build_grounded_rag_app_with_vertex/#cleaning-up","title":"\ud83e\uddf9 Cleaning up\u00b6","text":"<p>Clean up resources created in this notebook.</p>"},{"location":"genai-on-vertex-ai/vertex_ai_search/","title":"MOVED TO https://github.com/GoogleCloudPlatform/generative-ai/tree/main/search/vais-building-blocks","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/","title":"Vertex AI LLM Evaluation Services","text":"<p>We offer a comprehensive set of notebooks that demonstrate how to use Vertex AI LLM Evaluation Services in conjunction with other Vertex AI services. Additionally, we have provided notebooks that delve into the theory behind evaluation metrics.</p> <p>Theory notebooks:  - Metrics for Classification  - Metrics for Summarization  - Metrics for Text Generation  - Metrics for Q&amp;A</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/#requirements","title":"Requirements","text":"<p>To run the walkthrough and demonstration in the notebook you'll need access to a Google Cloud project with the Vertex AI API enabled.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/#getting-help","title":"Getting Help","text":"<p>If you have any questions or find any problems, please report through GitHub issues.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/","title":"RAG Evaluation Dataset Curation","text":"<p>Once businesses start to expose a Chatbot or RAG application to real end-users, they will start to observe real queries and conversations. The questions users ask these bots can reveal so much about users, the products they are using/purchasing, and the business itself. This data is valuable for both understanding users and their needs and assessing if the Chatbot you've built is working properly over time. </p> <p>The goal of this notebook is to use Gemini to turbo-charge analysis and summarization of real user queries and conversations from an in-production RAG system/Chatbot. We can then use this analysis to identify a representative set of questions which can be used as an evaluation dataset for the RAG system. This notebook can serve as the foundation of a continuous evaluation practice for RAG systems. </p> <p></p> <p>Along the way, we want to learn: - What are the general classes of questions people are asking?   - What problems do people have? - What topics are being discussed? - What sentiments are being expressed?</p> <p>This notebook was inspired by an article by Weights and Biases. We take some of the ideas and go a few steps further by using Gemini to analyze and extract metadata about the clusters we find and use that to inform our choosing of an evaluation dataset. Gemini's large context allows us to perform EDA on clusters and questions extremely quickly, even for very large question sets. </p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/","title":"\ud83c\udfac Getting Started","text":"In\u00a0[1]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      Author(s) Ken Lee Reviewers(s) Abhishek Bhagwat Last updated 2024-10-07 <p>Deploying a chatbot or retrieval augmented generation (RAG) application to real users provides a wealth of valuable data.  User queries reveal insights into their needs, the products they engage with, and the effectiveness of the chatbot itself. This data is crucial for both understanding your users and continuously evaluating the performance of your deployed system.</p> <p></p> <p>This notebook demonstrates how to leverage Gemini to accelerate the analysis and summarization of real user queries from a production RAG system or chatbot. By analyzing these queries, we can identify a representative set of questions to form an evaluation dataset, establishing a foundation for continuous evaluation.</p> <p>This process aims to answer the following questions:</p> <ul> <li><p>What general categories of questions are users asking? What problems are they encountering?</p> </li> <li><p>What topics are prevalent in user conversations?</p> </li> <li><p>What sentiments are users expressing?</p> </li> </ul> <p>Inspired by a Weights and Biases article, this notebook extends those concepts by utilizing Gemini's capabilities.  Gemini's large context window allows for rapid exploratory data analysis (EDA) of clustered questions, even with extensive datasets, facilitating efficient metadata extraction and informed selection of an evaluation dataset.  This, in turn, enables the construction of a robust and representative evaluation set for the RAG system.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -qqq llama-index \\\nllama-index-llms-vertex \\\nllama-index-embeddings-vertex \\\npython-louvain \\\ntiktoken \\\naiofiles \\\nannotated-types \\\npython-fasthtml\n</pre> !pip install -qqq llama-index \\ llama-index-llms-vertex \\ llama-index-embeddings-vertex \\ python-louvain \\ tiktoken \\ aiofiles \\ annotated-types \\ python-fasthtml In\u00a0[\u00a0]: Copied! <pre># Restart kernel after installs so that your environment can access the new packages\nimport IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> # Restart kernel after installs so that your environment can access the new packages import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\n    print('Authenticated')\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth     auth.authenticate_user()     print('Authenticated') In\u00a0[2]: Copied! <pre>import vertexai\n\nPROJECT_ID = \"&lt;enter-your-project-id&gt;\"\nREGION = \"us-central1\"\nCSV_PATH = \"./curate_evals_example.csv\"\nTEST_RUN = False\nCLUSTERING_NEIGHBORHOOD_SIZE = 5\n\nvertexai.init(\n    project=PROJECT_ID,\n    location=REGION\n)\n</pre> import vertexai  PROJECT_ID = \"\" REGION = \"us-central1\" CSV_PATH = \"./curate_evals_example.csv\" TEST_RUN = False CLUSTERING_NEIGHBORHOOD_SIZE = 5  vertexai.init(     project=PROJECT_ID,     location=REGION ) In\u00a0[3]: Copied! <pre>import pandas as pd\nimport numpy as np\nif TEST_RUN:\n  df = pd.DataFrame({\"Prompt\": [\"What is RAG?\", \"What is life?\", \"What is football?\", \"Who am I?\"],\n                   \"answer\": [\"Retrieval Augmented Generation\", \"Love\", \"National Football League\", \"Human\"]})\nelse:\n  df = pd.read_csv(CSV_PATH)\n</pre> import pandas as pd import numpy as np if TEST_RUN:   df = pd.DataFrame({\"Prompt\": [\"What is RAG?\", \"What is life?\", \"What is football?\", \"Who am I?\"],                    \"answer\": [\"Retrieval Augmented Generation\", \"Love\", \"National Football League\", \"Human\"]}) else:   df = pd.read_csv(CSV_PATH) In\u00a0[4]: Copied! <pre>df\n</pre> df Out[4]: Topic Question 0 Compute Engine How can I create a virtual machine instance on... 1 Compute Engine \"What are the different machine types availabl... 2 Compute Engine \"Can you explain the different pricing options... 3 Compute Engine \"How do I connect to my Compute Engine instanc... 4 Compute Engine \"What are preemptible instances, and how can t... ... ... ... 95 Cost Management \"How can I track and manage my Google Cloud co... 96 Cost Management \"What are the different pricing models for Goo... 97 Cost Management \"How can I optimize my Google Cloud costs?\" 98 Cost Management \"What tools are available for cost management ... 99 Cost Management \"How can I set budgets and alerts for my Googl... <p>100 rows \u00d7 2 columns</p> In\u00a0[5]: Copied! <pre>df[\"question_len\"] = df[\"Question\"].apply(lambda x: len(x))\n</pre> df[\"question_len\"] = df[\"Question\"].apply(lambda x: len(x)) In\u00a0[6]: Copied! <pre># Discard questions with too little or too many characters\ndf = df[(df.question_len &gt; 5) &amp; (df.question_len &lt; 1000)]\n</pre> # Discard questions with too little or too many characters df = df[(df.question_len &gt; 5) &amp; (df.question_len &lt; 1000)] In\u00a0[7]: Copied! <pre>df\n</pre> df Out[7]: Topic Question question_len 0 Compute Engine How can I create a virtual machine instance on... 63 1 Compute Engine \"What are the different machine types availabl... 115 2 Compute Engine \"Can you explain the different pricing options... 77 3 Compute Engine \"How do I connect to my Compute Engine instanc... 59 4 Compute Engine \"What are preemptible instances, and how can t... 65 ... ... ... ... 95 Cost Management \"How can I track and manage my Google Cloud co... 51 96 Cost Management \"What are the different pricing models for Goo... 66 97 Cost Management \"How can I optimize my Google Cloud costs?\" 43 98 Cost Management \"What tools are available for cost management ... 63 99 Cost Management \"How can I set budgets and alerts for my Googl... 64 <p>100 rows \u00d7 3 columns</p> In\u00a0[8]: Copied! <pre>df.question_len.hist(bins=25)\n</pre> df.question_len.hist(bins=25) Out[8]: <pre>&lt;Axes: &gt;</pre> In\u00a0[9]: Copied! <pre>import asyncio\nfrom tqdm.asyncio import tqdm_asyncio\nfrom typing import List, Optional,  Tuple\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\nfrom google.cloud import storage\nfrom vertexai.generative_models import GenerativeModel\n\nasync def embed_text_async(\n    model: TextEmbeddingModel,\n    texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],\n    task: str = \"RETRIEVAL_DOCUMENT\",\n    dimensionality: Optional[int] = 768,):\n    inputs = [TextEmbeddingInput(text, task) for text in texts]\n    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n    embeddings = await model.get_embeddings_async(texts, **kwargs)\n    return [embedding.values for embedding in embeddings]\n\n# embedding model to use\nmodel_name = \"text-embedding-005\"\nembedding_model = TextEmbeddingModel.from_pretrained(model_name)\n\n# embed questions from the dataset asynchronously\nembedded_qs = await tqdm_asyncio.gather(*[embed_text_async(embedding_model,\n                                        [x[\"Question\"]]) for i, x in df.iterrows()])\n</pre> import asyncio from tqdm.asyncio import tqdm_asyncio from typing import List, Optional,  Tuple from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel from google.cloud import storage from vertexai.generative_models import GenerativeModel  async def embed_text_async(     model: TextEmbeddingModel,     texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],     task: str = \"RETRIEVAL_DOCUMENT\",     dimensionality: Optional[int] = 768,):     inputs = [TextEmbeddingInput(text, task) for text in texts]     kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}     embeddings = await model.get_embeddings_async(texts, **kwargs)     return [embedding.values for embedding in embeddings]  # embedding model to use model_name = \"text-embedding-005\" embedding_model = TextEmbeddingModel.from_pretrained(model_name)  # embed questions from the dataset asynchronously embedded_qs = await tqdm_asyncio.gather(*[embed_text_async(embedding_model,                                         [x[\"Question\"]]) for i, x in df.iterrows()]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 302.49it/s]\n</pre> In\u00a0[10]: Copied! <pre>embedded_qs_flattened = [q[0] for q in embedded_qs]\n</pre> embedded_qs_flattened = [q[0] for q in embedded_qs] In\u00a0[11]: Copied! <pre>from llama_index.core import (\n    VectorStoreIndex,\n    Settings,\n    SimpleDirectoryReader,\n    load_index_from_storage,\n    StorageContext,\n    Document\n)\nfrom llama_index.llms.vertex import Vertex\nfrom llama_index.embeddings.vertex import VertexTextEmbedding\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\nimport networkx as nx\nfrom community import community_louvain # pip install python-louvain\nimport google.auth\nimport google.auth.transport.requests\n\ncredentials = google.auth.default()[0]\nrequest = google.auth.transport.requests.Request()\ncredentials.refresh(request)\n\n\nquery_list = df[\"Question\"].tolist()\nquery_docs = [Document(text=t) for t in query_list] # To make it LlamaIndex compatible\nembed_model = VertexTextEmbedding(credentials=credentials, model_name=\"text-embedding-005\")\nllm = Vertex(model=\"gemini-2.0-flash-001\",\n             temperature=0.2,\n             max_tokens=8192,\n             safety_settings={\n                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n        }\n)\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n\n# Form a local vector index with all our questions\nvector_index = VectorStoreIndex.from_documents(query_docs)\nvector_retriever = vector_index.as_retriever(similarity_top_k=CLUSTERING_NEIGHBORHOOD_SIZE)\n\n\n# Create a similarity graph\nG = nx.Graph()\n\n# Get a neighborhood of similar questions by querying the vector index\nsimilar_texts = await tqdm_asyncio.gather(*[vector_retriever.aretrieve(text) for i, text in enumerate(query_list)])\n\nfor i, text in enumerate(query_list):\n  for s in similar_texts[i]:\n    G.add_edge(text, s.text)\n</pre> from llama_index.core import (     VectorStoreIndex,     Settings,     SimpleDirectoryReader,     load_index_from_storage,     StorageContext,     Document ) from llama_index.llms.vertex import Vertex from llama_index.embeddings.vertex import VertexTextEmbedding from vertexai.generative_models import HarmCategory, HarmBlockThreshold import networkx as nx from community import community_louvain # pip install python-louvain import google.auth import google.auth.transport.requests  credentials = google.auth.default()[0] request = google.auth.transport.requests.Request() credentials.refresh(request)   query_list = df[\"Question\"].tolist() query_docs = [Document(text=t) for t in query_list] # To make it LlamaIndex compatible embed_model = VertexTextEmbedding(credentials=credentials, model_name=\"text-embedding-005\") llm = Vertex(model=\"gemini-2.0-flash-001\",              temperature=0.2,              max_tokens=8192,              safety_settings={                     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,                     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,                     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,                     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,         } ) Settings.llm = llm Settings.embed_model = embed_model   # Form a local vector index with all our questions vector_index = VectorStoreIndex.from_documents(query_docs) vector_retriever = vector_index.as_retriever(similarity_top_k=CLUSTERING_NEIGHBORHOOD_SIZE)   # Create a similarity graph G = nx.Graph()  # Get a neighborhood of similar questions by querying the vector index similar_texts = await tqdm_asyncio.gather(*[vector_retriever.aretrieve(text) for i, text in enumerate(query_list)])  for i, text in enumerate(query_list):   for s in similar_texts[i]:     G.add_edge(text, s.text) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01&lt;00:00, 77.81it/s]\n</pre> In\u00a0[12]: Copied! <pre># Apply Louvain Community Detection\npartition = community_louvain.best_partition(G)\ndf[\"cluster_idx\"] = df[\"Question\"].map(partition)\n</pre> # Apply Louvain Community Detection partition = community_louvain.best_partition(G) df[\"cluster_idx\"] = df[\"Question\"].map(partition) In\u00a0[13]: Copied! <pre>grouped_df = pd.DataFrame(df.groupby(\"cluster_idx\")['Question'].apply(list)).reset_index()\n</pre> grouped_df = pd.DataFrame(df.groupby(\"cluster_idx\")['Question'].apply(list)).reset_index() In\u00a0[14]: Copied! <pre>grouped_df\n</pre> grouped_df Out[14]: cluster_idx Question 0 0 [How can I create a virtual machine instance o... 1 1 [\"What is BigQuery, and how can I use it to an... 2 2 [\"What are the different tools available for d... 3 3 [\"I need to increase the storage space on my C... 4 4 [\"My application is experiencing performance i... 5 5 [\"What are preemptible instances, and how can ... 6 6 [\"What is Cloud Load Balancing, and how does i... 7 7 [\"I need to transfer a large amount of data to... 8 8 [\"I'm trying to train a machine learning model... 9 9 [\"I'm concerned about the security of my sensi... In\u00a0[15]: Copied! <pre>grouped_df[\"num_questions\"] = grouped_df[\"Question\"].apply(len)\ngrouped_df\n</pre> grouped_df[\"num_questions\"] = grouped_df[\"Question\"].apply(len) grouped_df Out[15]: cluster_idx Question num_questions 0 0 [How can I create a virtual machine instance o... 9 1 1 [\"What is BigQuery, and how can I use it to an... 5 2 2 [\"What are the different tools available for d... 12 3 3 [\"I need to increase the storage space on my C... 11 4 4 [\"My application is experiencing performance i... 8 5 5 [\"What are preemptible instances, and how can ... 13 6 6 [\"What is Cloud Load Balancing, and how does i... 12 7 7 [\"I need to transfer a large amount of data to... 6 8 8 [\"I'm trying to train a machine learning model... 4 9 9 [\"I'm concerned about the security of my sensi... 20 In\u00a0[16]: Copied! <pre>from vertexai.generative_models import GenerativeModel, GenerationConfig\nfrom vertexai.generative_models import HarmCategory, HarmBlockThreshold\nfrom llama_index.core.program import LLMTextCompletionProgram\nfrom llama_index.core.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field\nfrom typing import Annotated\nfrom enum import Enum\nfrom annotated_types import Len\n\nnum_clusters = grouped_df.shape[0]\n\nclass Sentiment(Enum):\n  POSITIVE = \"positive\"\n  NEGATIVE = \"negative\"\n  NEUTRAL = \"neutral\"\n\nclass ClusterSummary(BaseModel):\n  '''A cluster summary, list of topics, most representative questions, and sentiment associated with a cluster of questions from chat sessions.'''\n  summary_desc: str\n  topics: List[str]\n  most_representative_qs: Annotated[List[str], Len(3, 8)]\n  sentiment: Sentiment\n\n\nboring_prompt = \"\"\"Please provide a brief summary which captures the nature of the given cluster of questions below in the form of \"Questions concerning ____\".\n                  \\n Cluster questions:\n                  \\n {questions_list}\n                  \\n The clusters titles should not be generic such as \"Google Cloud AI\" or \"Gemini\".\n                  \\n They need to be specific in order to distinguish the clusters from others which may be similar.\n                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format.\"\"\"\n\nmovie_prompt = \"\"\"You are an expert movie producer for famous movies.\n                  \\n Please provide a quipy, movie title which captures the essence of the given cluster of questions below.\n                  \\n Example:\n                  \\n How does RAG work on Vertex?\n                  \\n Where can I find documentation on Vertex AI Generative model API?\n                  \\n What are the pitfals of Gemini vs. Gemma?\n                  \\n Answer:\n                  \\n movie title: \"Into the Vertex\"\n                  \\n representative qs: How does RAG work on Vertex?\n                  \\n topics: Vertex AI, Vertex AI Generative Model\n                  \\n sentiment: neutral\n                  \\n Cluster questions:\n                  \\n {questions_list}\n                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format. \"\"\"\n\nasync def summarize_cluster(questions: List[str]):\n  questions_list = \"\\n\".join(questions)\n  llm_program = LLMTextCompletionProgram.from_defaults(\n        output_parser=PydanticOutputParser(ClusterSummary),\n        prompt_template_str=boring_prompt,\n        verbose=True,\n    )\n  try:\n    cluster_summary = await llm_program.acall(questions_list=questions_list)\n  except Exception as e:\n    print(e)\n    return None\n  return cluster_summary\n</pre> from vertexai.generative_models import GenerativeModel, GenerationConfig from vertexai.generative_models import HarmCategory, HarmBlockThreshold from llama_index.core.program import LLMTextCompletionProgram from llama_index.core.output_parsers import PydanticOutputParser from pydantic import BaseModel, Field from typing import Annotated from enum import Enum from annotated_types import Len  num_clusters = grouped_df.shape[0]  class Sentiment(Enum):   POSITIVE = \"positive\"   NEGATIVE = \"negative\"   NEUTRAL = \"neutral\"  class ClusterSummary(BaseModel):   '''A cluster summary, list of topics, most representative questions, and sentiment associated with a cluster of questions from chat sessions.'''   summary_desc: str   topics: List[str]   most_representative_qs: Annotated[List[str], Len(3, 8)]   sentiment: Sentiment   boring_prompt = \"\"\"Please provide a brief summary which captures the nature of the given cluster of questions below in the form of \"Questions concerning ____\".                   \\n Cluster questions:                   \\n {questions_list}                   \\n The clusters titles should not be generic such as \"Google Cloud AI\" or \"Gemini\".                   \\n They need to be specific in order to distinguish the clusters from others which may be similar.                   \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format.\"\"\"  movie_prompt = \"\"\"You are an expert movie producer for famous movies.                   \\n Please provide a quipy, movie title which captures the essence of the given cluster of questions below.                   \\n Example:                   \\n How does RAG work on Vertex?                   \\n Where can I find documentation on Vertex AI Generative model API?                   \\n What are the pitfals of Gemini vs. Gemma?                   \\n Answer:                   \\n movie title: \"Into the Vertex\"                   \\n representative qs: How does RAG work on Vertex?                   \\n topics: Vertex AI, Vertex AI Generative Model                   \\n sentiment: neutral                   \\n Cluster questions:                   \\n {questions_list}                   \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format. \"\"\"  async def summarize_cluster(questions: List[str]):   questions_list = \"\\n\".join(questions)   llm_program = LLMTextCompletionProgram.from_defaults(         output_parser=PydanticOutputParser(ClusterSummary),         prompt_template_str=boring_prompt,         verbose=True,     )   try:     cluster_summary = await llm_program.acall(questions_list=questions_list)   except Exception as e:     print(e)     return None   return cluster_summary In\u00a0[17]: Copied! <pre># Summarize each cluster individually\ncluster_summaries = await tqdm_asyncio.gather(*[summarize_cluster(q[\"Question\"]) for idx, q in grouped_df.iterrows()])\n</pre> # Summarize each cluster individually cluster_summaries = await tqdm_asyncio.gather(*[summarize_cluster(q[\"Question\"]) for idx, q in grouped_df.iterrows()]) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&lt;00:00,  1.70it/s]\n</pre> In\u00a0[18]: Copied! <pre>cluster_summaries\n</pre> cluster_summaries Out[18]: <pre>[ClusterSummary(summary_desc='Questions concerning the practical usage and troubleshooting of Google Compute Engine virtual machine instances, including instance creation, selection, connection, deletion recovery, clustering for high availability, firewall issues, and pricing.', topics=['Google Compute Engine', 'Virtual Machine Instances', 'Instance Creation', 'Machine Types', 'Pricing', 'SSH Connection', 'Instance Deletion Recovery', 'High Availability Clustering', 'Firewall Troubleshooting', 'Discounts'], most_representative_qs=['How can I create a virtual machine instance on Compute Engine?', 'What are the different machine types available on Compute Engine, and how do I choose the right one for my needs?', 'Can you explain the different pricing options for Compute Engine instances?', 'How do I connect to my Compute Engine instance using SSH?', 'I accidentally deleted my Compute Engine instance. How can I recover it?', 'I want to set up a cluster of Compute Engine instances for high availability. Can you guide me through the process?', \"I'm having trouble connecting to my Virtual Machine instance. I think there's a firewall issue. How can I troubleshoot this?\"], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning the practical application and optimization of BigQuery for large dataset analysis, including data loading, query performance, visualization, and machine learning integration.', topics=['BigQuery', 'large datasets', 'data analysis', 'data loading', 'query optimization', 'performance', 'visualization', 'Data Studio', 'machine learning'], most_representative_qs=['What is BigQuery, and how can I use it to analyze large datasets?', 'I have a large dataset that I want to analyze using BigQuery. How can I load my data into BigQuery?', 'My BigQuery queries are taking a long time to run. How can I optimize my queries for better performance?', 'I want to visualize my data in BigQuery using Data Studio. How can I connect Data Studio to my BigQuery dataset?', 'How can I use machine learning with BigQuery to gain insights from my data?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc=\"Questions concerning the practical application of Google Cloud's Machine Learning and Data Processing tools for building, deploying, and monitoring models.\", topics=['Data Processing on Google Cloud', 'Data Pipelines', 'Data Visualization', 'Real-time Data Processing', 'Pre-trained Models for Image Recognition', 'AutoML', 'Machine Learning Model Deployment', 'Machine Learning Model Monitoring', 'AI and ML Services on Google Cloud'], most_representative_qs=['How can I use Google Cloud to build a data pipeline?', 'How can I use Google Cloud to visualize my data?', 'How can I use AutoML to build a machine learning model without writing any code?', 'I need to monitor the performance of my deployed machine learning model. What tools are available on Google Cloud?', 'How can I use Google Cloud to build a machine learning model?', 'How can I use Google Cloud to deploy my machine learning model?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning Google Cloud database services, particularly storage, migration, scaling, and performance optimization for Cloud SQL and Cloud Spanner.', topics=['Google Cloud Storage', 'Database Services', 'Cloud SQL', 'Cloud Spanner', 'Database Migration', 'Database Scaling', 'Storage Capacity', 'Database Replication', 'Query Performance', 'Database Security'], most_representative_qs=['What database services are available on Google Cloud?', 'What is the difference between Cloud SQL and Cloud Spanner?', 'How do I migrate my existing database to Google Cloud?', 'How can I scale my database on Google Cloud?', 'My Cloud SQL database is running out of storage space. How can I increase the storage capacity?', 'I need to replicate my Cloud SQL database to another region for disaster recovery. How can I set up database replication?', \"I'm experiencing slow query performance on my Cloud Spanner database. How can I optimize my database and queries?\"], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning the monitoring, troubleshooting, and optimization of application performance on Google Cloud Platform, particularly focusing on diagnosing latency, utilizing logging and monitoring tools, and understanding specific services like Compute Engine and Cloud Run.', topics=['application performance', 'troubleshooting', 'optimization', 'Compute Engine', 'network latency', 'Google Kubernetes Engine', 'Cloud Logging', 'Cloud Monitoring', 'Cloud Run'], most_representative_qs=['My application is experiencing performance issues. How can I troubleshoot and optimize my Compute Engine instance?', 'My application is experiencing high latency. Could it be a networking issue? How can I diagnose and resolve network latency problems?', 'I want to monitor the performance of my applications running on Google Kubernetes Engine. What tools can I use?', 'How can I monitor the performance and logs of my Cloud Run services?', 'What is Cloud Monitoring, and how does it work?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning cost optimization strategies and troubleshooting unexpected expenses within Google Cloud Platform.', topics=['Preemptible Instances', 'Cost Optimization', 'Unused Resources', 'Cloud Storage Costs', 'High CPU Usage', 'Cost Allocation', 'Budgeting', 'Cost Analysis', 'Resource Management'], most_representative_qs=['What are preemptible instances, and how can they save me money?', \"I'm getting billed for a Compute Engine instance that I'm not using. How can I identify and shut down unused instances?\", 'My Cloud Storage costs are higher than expected. How can I analyze my usage and optimize my storage costs?', 'My Google Cloud bill is higher than expected this month. How can I identify the source of the increased cost?', 'I want to track the cost of my Google Cloud resources by department. How can I set up cost allocation?', 'What are some best practices for optimizing my Google Cloud costs?'], sentiment=&lt;Sentiment.NEGATIVE: 'negative'&gt;),\n ClusterSummary(summary_desc='Questions concerning the automation of application deployment, management, and scaling on Google Cloud, particularly focusing on serverless technologies like Cloud Functions and Cloud Run.', topics=['Cloud Load Balancing', 'application deployment automation', 'Google Cloud resource management', 'task automation on Google Cloud', 'serverless computing', 'serverless platforms on Google Cloud', 'serverless application development and deployment', 'Cloud Functions', 'Google Cloud Run', 'containerized applications', 'API development with Cloud Functions', 'Cloud Function timeout limits', 'Cloud Run deployment configuration'], most_representative_qs=['I need to automate the deployment of my applications on Google Cloud. What tools and services can I use?', 'What tools are available for managing my Google Cloud resources?', 'How can I automate tasks on Google Cloud?', 'What is serverless computing, and what are its benefits?', 'What serverless platforms are available on Google Cloud?', 'How can I build and deploy a serverless application on Google Cloud?', 'What is Cloud Functions, and how does it work?', 'How can I use Google Cloud Run to deploy containerized applications?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning the practical aspects of using Google Cloud Storage, such as data transfer methods, file recovery, access management, and pricing.', topics=['data transfer', 'file recovery', 'public access', 'data upload', 'data access', 'pricing'], most_representative_qs=[\"I need to transfer a large amount of data to Google Cloud Storage. What's the most efficient way to do this?\", 'I accidentally deleted some files from my Cloud Storage bucket. How can I recover them?', 'I want to make my data in Cloud Storage available to the public. How can I configure public access?', 'How much does it cost to store data in Google Cloud Storage?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning practical challenges and usage of Vertex AI for machine learning tasks.', topics=['Vertex AI', 'Machine Learning', 'Model Training', 'Error Troubleshooting', 'Model Deployment', 'API', 'IAM Configuration'], most_representative_qs=[\"I'm trying to train a machine learning model on Vertex AI, but I'm getting errors. How can I troubleshoot these errors?\", 'I want to deploy my trained machine learning model as an API. How can I do this using Vertex AI?', 'What is Vertex AI, and how can I use it?', \"I'm having trouble configuring IAM to use Vertex AI. What do I do?\"], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;),\n ClusterSummary(summary_desc='Questions concerning securing Google Cloud resources and infrastructure, particularly focusing on networking, access control, and data protection.', topics=['Cloud Storage Security', 'Virtual Private Cloud (VPC)', 'Firewall Rules', 'Network Connectivity', 'Network Security', 'Data Security', 'Identity and Access Management (IAM)', 'Multi-Factor Authentication', 'Security Best Practices', 'Security Monitoring', 'Vulnerability Remediation', 'Secure Application Development'], most_representative_qs=['What are the security features of Google Cloud Storage?', 'How do I create a Virtual Private Cloud (VPC) on Google Cloud?', 'What are firewalls, and how do I configure them in Google Cloud?', 'How can I connect my on-premises network to Google Cloud?', 'How can I secure my applications and data on Google Cloud?', 'What is Identity and Access Management (IAM), and how does it work?', 'How can I implement multi-factor authentication on Google Cloud?', 'What are security best practices for Google Cloud?'], sentiment=&lt;Sentiment.NEUTRAL: 'neutral'&gt;)]</pre> In\u00a0[19]: Copied! <pre>just_summaries = [c.summary_desc if c else None for c in cluster_summaries]\n</pre> just_summaries = [c.summary_desc if c else None for c in cluster_summaries] In\u00a0[20]: Copied! <pre>df_grouped_by_cluster = df.groupby(\"cluster_idx\").agg(\"count\")\ndf_grouped_by_cluster[\"cluster_summary\"] = cluster_summaries\ndf_grouped_by_cluster[\"just_summary\"] = just_summaries\ndf_grouped_by_cluster[\"questions_list\"] = grouped_df[\"Question\"]\n</pre> df_grouped_by_cluster = df.groupby(\"cluster_idx\").agg(\"count\") df_grouped_by_cluster[\"cluster_summary\"] = cluster_summaries df_grouped_by_cluster[\"just_summary\"] = just_summaries df_grouped_by_cluster[\"questions_list\"] = grouped_df[\"Question\"] In\u00a0[21]: Copied! <pre>from fasthtml.common import *\nfrom fasthtml.fastapp import *\nfrom random import sample\nfrom fasthtml.components import Zero_md\n\ntlink = Script(src=\"https://cdn.tailwindcss.com\")\ndlink = Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\")\napp = FastHTML(hdrs=(dlink, tlink))\n\ndef Markdown(md, css = ''):\n    css_template = Template(Style(css), data_append=True)\n    return Zero_md(css_template, Script(md, type=\"text/markdown\"))\n\ndef MarkdownWOutBackground(md: str):\n    css = '.markdown-body {background-color: unset !important; color: unset !important;} .markdown-body table {color: black !important;}'\n    markdown_wout_background = partial(Markdown, css=css)\n    return markdown_wout_background(md)\n\ndef stat_card(num_questions: int):\n  return Div(\n    Div('Total Questions', cls='stat-title'),\n    Div(f'{num_questions}', cls='stat-value'),\n    cls='stat'\n  )\n\ndef cluster_card(cluster_summary: ClusterSummary, questions_list: List[str]):\n  if cluster_summary.sentiment == Sentiment.NEGATIVE:\n    badge_color = \"error\"\n  elif cluster_summary.sentiment == Sentiment.NEUTRAL:\n    badge_color = \"neutral\"\n  else:\n    badge_color = \"success\"\n  return Div(\n              Div(\n                  H2(cluster_summary.summary_desc, cls='card-title'),\n                  Div(\n                      stat_card(len(questions_list)),\n                      Div(cluster_summary.sentiment, cls=f'badge badge-{badge_color}'),\n                      cls=\"flex flex-row items-center\"\n                  ),\n                  H4(\"Representative Questions:\", cls=\"font-bold\"),\n                  Ul(\n                      *[Li(q) for q in cluster_summary.most_representative_qs],\n                      cls='list-disc list-inside mt-2'\n                  ),\n                  H4(\"Topics Discussed:\", cls=\"font-bold\"),\n                  Ul(\n                      *[Li(t) for t in cluster_summary.topics],\n                      cls='list-disc list-inside mt-2'\n                  ),\n                  cls='card-body'\n              ),\n              cls='card bg-base-100 shadow-xl'\n          )\n\n@app.get(\"/\")\ndef cluster_analysis():\n    return Div(\n              *[cluster_card(c, q) for c, q in zip(cluster_summaries, df_grouped_by_cluster[\"questions_list\"])],\n              cls=\"grid grid-cols-2 gap-2\"\n            )\n</pre> from fasthtml.common import * from fasthtml.fastapp import * from random import sample from fasthtml.components import Zero_md  tlink = Script(src=\"https://cdn.tailwindcss.com\") dlink = Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\") app = FastHTML(hdrs=(dlink, tlink))  def Markdown(md, css = ''):     css_template = Template(Style(css), data_append=True)     return Zero_md(css_template, Script(md, type=\"text/markdown\"))  def MarkdownWOutBackground(md: str):     css = '.markdown-body {background-color: unset !important; color: unset !important;} .markdown-body table {color: black !important;}'     markdown_wout_background = partial(Markdown, css=css)     return markdown_wout_background(md)  def stat_card(num_questions: int):   return Div(     Div('Total Questions', cls='stat-title'),     Div(f'{num_questions}', cls='stat-value'),     cls='stat'   )  def cluster_card(cluster_summary: ClusterSummary, questions_list: List[str]):   if cluster_summary.sentiment == Sentiment.NEGATIVE:     badge_color = \"error\"   elif cluster_summary.sentiment == Sentiment.NEUTRAL:     badge_color = \"neutral\"   else:     badge_color = \"success\"   return Div(               Div(                   H2(cluster_summary.summary_desc, cls='card-title'),                   Div(                       stat_card(len(questions_list)),                       Div(cluster_summary.sentiment, cls=f'badge badge-{badge_color}'),                       cls=\"flex flex-row items-center\"                   ),                   H4(\"Representative Questions:\", cls=\"font-bold\"),                   Ul(                       *[Li(q) for q in cluster_summary.most_representative_qs],                       cls='list-disc list-inside mt-2'                   ),                   H4(\"Topics Discussed:\", cls=\"font-bold\"),                   Ul(                       *[Li(t) for t in cluster_summary.topics],                       cls='list-disc list-inside mt-2'                   ),                   cls='card-body'               ),               cls='card bg-base-100 shadow-xl'           )  @app.get(\"/\") def cluster_analysis():     return Div(               *[cluster_card(c, q) for c, q in zip(cluster_summaries, df_grouped_by_cluster[\"questions_list\"])],               cls=\"grid grid-cols-2 gap-2\"             ) In\u00a0[22]: Copied! <pre>from starlette.testclient import TestClient\nclient = TestClient(app)\nr = client.get(\"/\")\nshow(r.content)\n</pre> from starlette.testclient import TestClient client = TestClient(app) r = client.get(\"/\") show(r.content) Out[22]: FastHTML page Questions concerning the practical usage and troubleshooting of Google Compute Engine virtual machine instances, including instance creation, selection, connection, deletion recovery, clustering for high availability, firewall issues, and pricing. Total Questions 9 Sentiment.NEUTRAL Representative Questions: <ul> <li>How can I create a virtual machine instance on Compute Engine?</li> <li>What are the different machine types available on Compute Engine, and how do I choose the right one for my needs?</li> <li>Can you explain the different pricing options for Compute Engine instances?</li> <li>How do I connect to my Compute Engine instance using SSH?</li> <li>I accidentally deleted my Compute Engine instance. How can I recover it?</li> <li>I want to set up a cluster of Compute Engine instances for high availability. Can you guide me through the process?</li> <li>I'm having trouble connecting to my Virtual Machine instance. I think there's a firewall issue. How can I troubleshoot this?</li> </ul> Topics Discussed: <ul> <li>Google Compute Engine</li> <li>Virtual Machine Instances</li> <li>Instance Creation</li> <li>Machine Types</li> <li>Pricing</li> <li>SSH Connection</li> <li>Instance Deletion Recovery</li> <li>High Availability Clustering</li> <li>Firewall Troubleshooting</li> <li>Discounts</li> </ul> Questions concerning the practical application and optimization of BigQuery for large dataset analysis, including data loading, query performance, visualization, and machine learning integration. Total Questions 5 Sentiment.NEUTRAL Representative Questions: <ul> <li>What is BigQuery, and how can I use it to analyze large datasets?</li> <li>I have a large dataset that I want to analyze using BigQuery. How can I load my data into BigQuery?</li> <li>My BigQuery queries are taking a long time to run. How can I optimize my queries for better performance?</li> <li>I want to visualize my data in BigQuery using Data Studio. How can I connect Data Studio to my BigQuery dataset?</li> <li>How can I use machine learning with BigQuery to gain insights from my data?</li> </ul> Topics Discussed: <ul> <li>BigQuery</li> <li>large datasets</li> <li>data analysis</li> <li>data loading</li> <li>query optimization</li> <li>performance</li> <li>visualization</li> <li>Data Studio</li> <li>machine learning</li> </ul> Questions concerning the practical application of Google Cloud's Machine Learning and Data Processing tools for building, deploying, and monitoring models. Total Questions 12 Sentiment.NEUTRAL Representative Questions: <ul> <li>How can I use Google Cloud to build a data pipeline?</li> <li>How can I use Google Cloud to visualize my data?</li> <li>How can I use AutoML to build a machine learning model without writing any code?</li> <li>I need to monitor the performance of my deployed machine learning model. What tools are available on Google Cloud?</li> <li>How can I use Google Cloud to build a machine learning model?</li> <li>How can I use Google Cloud to deploy my machine learning model?</li> </ul> Topics Discussed: <ul> <li>Data Processing on Google Cloud</li> <li>Data Pipelines</li> <li>Data Visualization</li> <li>Real-time Data Processing</li> <li>Pre-trained Models for Image Recognition</li> <li>AutoML</li> <li>Machine Learning Model Deployment</li> <li>Machine Learning Model Monitoring</li> <li>AI and ML Services on Google Cloud</li> </ul> Questions concerning Google Cloud database services, particularly storage, migration, scaling, and performance optimization for Cloud SQL and Cloud Spanner. Total Questions 11 Sentiment.NEUTRAL Representative Questions: <ul> <li>What database services are available on Google Cloud?</li> <li>What is the difference between Cloud SQL and Cloud Spanner?</li> <li>How do I migrate my existing database to Google Cloud?</li> <li>How can I scale my database on Google Cloud?</li> <li>My Cloud SQL database is running out of storage space. How can I increase the storage capacity?</li> <li>I need to replicate my Cloud SQL database to another region for disaster recovery. How can I set up database replication?</li> <li>I'm experiencing slow query performance on my Cloud Spanner database. How can I optimize my database and queries?</li> </ul> Topics Discussed: <ul> <li>Google Cloud Storage</li> <li>Database Services</li> <li>Cloud SQL</li> <li>Cloud Spanner</li> <li>Database Migration</li> <li>Database Scaling</li> <li>Storage Capacity</li> <li>Database Replication</li> <li>Query Performance</li> <li>Database Security</li> </ul> Questions concerning the monitoring, troubleshooting, and optimization of application performance on Google Cloud Platform, particularly focusing on diagnosing latency, utilizing logging and monitoring tools, and understanding specific services like Compute Engine and Cloud Run. Total Questions 8 Sentiment.NEUTRAL Representative Questions: <ul> <li>My application is experiencing performance issues. How can I troubleshoot and optimize my Compute Engine instance?</li> <li>My application is experiencing high latency. Could it be a networking issue? How can I diagnose and resolve network latency problems?</li> <li>I want to monitor the performance of my applications running on Google Kubernetes Engine. What tools can I use?</li> <li>How can I monitor the performance and logs of my Cloud Run services?</li> <li>What is Cloud Monitoring, and how does it work?</li> </ul> Topics Discussed: <ul> <li>application performance</li> <li>troubleshooting</li> <li>optimization</li> <li>Compute Engine</li> <li>network latency</li> <li>Google Kubernetes Engine</li> <li>Cloud Logging</li> <li>Cloud Monitoring</li> <li>Cloud Run</li> </ul> Questions concerning cost optimization strategies and troubleshooting unexpected expenses within Google Cloud Platform. Total Questions 13 Sentiment.NEGATIVE Representative Questions: <ul> <li>What are preemptible instances, and how can they save me money?</li> <li>I'm getting billed for a Compute Engine instance that I'm not using. How can I identify and shut down unused instances?</li> <li>My Cloud Storage costs are higher than expected. How can I analyze my usage and optimize my storage costs?</li> <li>My Google Cloud bill is higher than expected this month. How can I identify the source of the increased cost?</li> <li>I want to track the cost of my Google Cloud resources by department. How can I set up cost allocation?</li> <li>What are some best practices for optimizing my Google Cloud costs?</li> </ul> Topics Discussed: <ul> <li>Preemptible Instances</li> <li>Cost Optimization</li> <li>Unused Resources</li> <li>Cloud Storage Costs</li> <li>High CPU Usage</li> <li>Cost Allocation</li> <li>Budgeting</li> <li>Cost Analysis</li> <li>Resource Management</li> </ul> Questions concerning the automation of application deployment, management, and scaling on Google Cloud, particularly focusing on serverless technologies like Cloud Functions and Cloud Run. Total Questions 12 Sentiment.NEUTRAL Representative Questions: <ul> <li>I need to automate the deployment of my applications on Google Cloud. What tools and services can I use?</li> <li>What tools are available for managing my Google Cloud resources?</li> <li>How can I automate tasks on Google Cloud?</li> <li>What is serverless computing, and what are its benefits?</li> <li>What serverless platforms are available on Google Cloud?</li> <li>How can I build and deploy a serverless application on Google Cloud?</li> <li>What is Cloud Functions, and how does it work?</li> <li>How can I use Google Cloud Run to deploy containerized applications?</li> </ul> Topics Discussed: <ul> <li>Cloud Load Balancing</li> <li>application deployment automation</li> <li>Google Cloud resource management</li> <li>task automation on Google Cloud</li> <li>serverless computing</li> <li>serverless platforms on Google Cloud</li> <li>serverless application development and deployment</li> <li>Cloud Functions</li> <li>Google Cloud Run</li> <li>containerized applications</li> <li>API development with Cloud Functions</li> <li>Cloud Function timeout limits</li> <li>Cloud Run deployment configuration</li> </ul> Questions concerning the practical aspects of using Google Cloud Storage, such as data transfer methods, file recovery, access management, and pricing. Total Questions 6 Sentiment.NEUTRAL Representative Questions: <ul> <li>I need to transfer a large amount of data to Google Cloud Storage. What's the most efficient way to do this?</li> <li>I accidentally deleted some files from my Cloud Storage bucket. How can I recover them?</li> <li>I want to make my data in Cloud Storage available to the public. How can I configure public access?</li> <li>How much does it cost to store data in Google Cloud Storage?</li> </ul> Topics Discussed: <ul> <li>data transfer</li> <li>file recovery</li> <li>public access</li> <li>data upload</li> <li>data access</li> <li>pricing</li> </ul> Questions concerning practical challenges and usage of Vertex AI for machine learning tasks. Total Questions 4 Sentiment.NEUTRAL Representative Questions: <ul> <li>I'm trying to train a machine learning model on Vertex AI, but I'm getting errors. How can I troubleshoot these errors?</li> <li>I want to deploy my trained machine learning model as an API. How can I do this using Vertex AI?</li> <li>What is Vertex AI, and how can I use it?</li> <li>I'm having trouble configuring IAM to use Vertex AI. What do I do?</li> </ul> Topics Discussed: <ul> <li>Vertex AI</li> <li>Machine Learning</li> <li>Model Training</li> <li>Error Troubleshooting</li> <li>Model Deployment</li> <li>API</li> <li>IAM Configuration</li> </ul> Questions concerning securing Google Cloud resources and infrastructure, particularly focusing on networking, access control, and data protection. Total Questions 20 Sentiment.NEUTRAL Representative Questions: <ul> <li>What are the security features of Google Cloud Storage?</li> <li>How do I create a Virtual Private Cloud (VPC) on Google Cloud?</li> <li>What are firewalls, and how do I configure them in Google Cloud?</li> <li>How can I connect my on-premises network to Google Cloud?</li> <li>How can I secure my applications and data on Google Cloud?</li> <li>What is Identity and Access Management (IAM), and how does it work?</li> <li>How can I implement multi-factor authentication on Google Cloud?</li> <li>What are security best practices for Google Cloud?</li> </ul> Topics Discussed: <ul> <li>Cloud Storage Security</li> <li>Virtual Private Cloud (VPC)</li> <li>Firewall Rules</li> <li>Network Connectivity</li> <li>Network Security</li> <li>Data Security</li> <li>Identity and Access Management (IAM)</li> <li>Multi-Factor Authentication</li> <li>Security Best Practices</li> <li>Security Monitoring</li> <li>Vulnerability Remediation</li> <li>Secure Application Development</li> </ul> In\u00a0[24]: Copied! <pre># Calculate the total number of questions\ntotal_questions = df_grouped_by_cluster['question_len'].sum()\n\n# Calculate the fraction of questions for each row\ndf_grouped_by_cluster['cluster_fraction'] = df_grouped_by_cluster['question_len'] / total_questions\n\n# Function to sample from a list based on the fraction\ndef sample_questions(row, num_samples):\n    return np.random.choice(row['questions_list'],\n                            size=int(num_samples * row['cluster_fraction']),\n                            replace=False).tolist()\n\n# Specify the total number of samples you want\ntotal_samples = 50\n\n# Apply the sampling function to each row\ndf_grouped_by_cluster['proportional_sampled_questions'] = df_grouped_by_cluster.apply(lambda row: sample_questions(row, total_samples), axis=1)\n\n# Unroll the DataFrame\ndf_grouped_by_cluster = df_grouped_by_cluster.reset_index()\n\n# Print the resulting DataFrame\nunrolled_proportional_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n    'cluster_title': [x[\"just_summary\"]] * len(x['proportional_sampled_questions']),\n    'sampled_question': x['proportional_sampled_questions']\n}), axis=1)\n\n# Concatenate the series and reset the index\nunrolled_proportional_df = pd.concat([unrolled_proportional_df['cluster_title'].explode(),\n                         unrolled_proportional_df['sampled_question'].explode()],\n                        axis=1).reset_index(drop=True)\n</pre> # Calculate the total number of questions total_questions = df_grouped_by_cluster['question_len'].sum()  # Calculate the fraction of questions for each row df_grouped_by_cluster['cluster_fraction'] = df_grouped_by_cluster['question_len'] / total_questions  # Function to sample from a list based on the fraction def sample_questions(row, num_samples):     return np.random.choice(row['questions_list'],                             size=int(num_samples * row['cluster_fraction']),                             replace=False).tolist()  # Specify the total number of samples you want total_samples = 50  # Apply the sampling function to each row df_grouped_by_cluster['proportional_sampled_questions'] = df_grouped_by_cluster.apply(lambda row: sample_questions(row, total_samples), axis=1)  # Unroll the DataFrame df_grouped_by_cluster = df_grouped_by_cluster.reset_index()  # Print the resulting DataFrame unrolled_proportional_df = df_grouped_by_cluster.apply(lambda x: pd.Series({     'cluster_title': [x[\"just_summary\"]] * len(x['proportional_sampled_questions']),     'sampled_question': x['proportional_sampled_questions'] }), axis=1)  # Concatenate the series and reset the index unrolled_proportional_df = pd.concat([unrolled_proportional_df['cluster_title'].explode(),                          unrolled_proportional_df['sampled_question'].explode()],                         axis=1).reset_index(drop=True) In\u00a0[25]: Copied! <pre>unrolled_proportional_df\n</pre> unrolled_proportional_df Out[25]: cluster_title sampled_question 0 Questions concerning the practical usage and t... \"What are the different machine types availabl... 1 Questions concerning the practical usage and t... \"I accidentally deleted my Compute Engine inst... 2 Questions concerning the practical usage and t... \"Are there any discounts or sustained use disc... 3 Questions concerning the practical usage and t... \"I'm having trouble connecting to my Virtual M... 4 Questions concerning the practical application... \"How can I use machine learning with BigQuery ... 5 Questions concerning the practical application... \"What is BigQuery, and how can I use it to ana... 6 Questions concerning the practical application... \"What is Dataflow, and how does it work?\" 7 Questions concerning the practical application... \"What are the different tools available for da... 8 Questions concerning the practical application... \"How can I use Google Cloud to build a machine... 9 Questions concerning the practical application... \"I need to monitor the performance of my deplo... 10 Questions concerning the practical application... \"How can I use AutoML to build a machine learn... 11 Questions concerning the practical application... \"How can I use Google Cloud to visualize my da... 12 Questions concerning Google Cloud database ser... \"What are the different storage options availa... 13 Questions concerning Google Cloud database ser... \"What database services are available on Googl... 14 Questions concerning Google Cloud database ser... \"I need to increase the storage space on my Co... 15 Questions concerning Google Cloud database ser... \"How can I scale my database on Google Cloud?\" 16 Questions concerning Google Cloud database ser... \"How do I migrate my existing database to Goog... 17 Questions concerning the monitoring, troublesh... \"How can I monitor the performance and logs of... 18 Questions concerning the monitoring, troublesh... \"My application is experiencing performance is... 19 Questions concerning the monitoring, troublesh... \"What is Cloud Logging, and how can I use it t... 20 Questions concerning the monitoring, troublesh... \"I'm trying to troubleshoot an issue with my a... 21 Questions concerning cost optimization strateg... \"What are some best practices for optimizing m... 22 Questions concerning cost optimization strateg... \"My Cloud Storage costs are higher than expect... 23 Questions concerning cost optimization strateg... \"I'm not using some of my Google Cloud resourc... 24 Questions concerning cost optimization strateg... \"How can I track and manage my Google Cloud co... 25 Questions concerning cost optimization strateg... \"What tools are available for cost management ... 26 Questions concerning cost optimization strateg... \"My Google Cloud bill is higher than expected ... 27 Questions concerning the automation of applica... \"My Cloud Function is timing out. How can I in... 28 Questions concerning the automation of applica... \"What is serverless computing, and what are it... 29 Questions concerning the automation of applica... \"How can I use Google Cloud Run to deploy cont... 30 Questions concerning the automation of applica... \"I need to deploy a containerized application ... 31 Questions concerning the automation of applica... \"What is Cloud Functions, and how does it work?\" 32 Questions concerning the automation of applica... \"I want to build a simple API using Cloud Func... 33 Questions concerning the practical aspects of ... \"How can I upload data to Google Cloud Storage?\" 34 Questions concerning the practical aspects of ... \"I want to make my data in Cloud Storage avail... 35 Questions concerning the practical aspects of ... \"I need to transfer a large amount of data to ... 36 Questions concerning practical challenges and ... \"I'm trying to train a machine learning model ... 37 Questions concerning practical challenges and ... \"I'm having trouble configuring IAM to use Ver... 38 Questions concerning securing Google Cloud res... \"How can I improve the security of my network ... 39 Questions concerning securing Google Cloud res... \"What is Identity and Access Management (IAM),... 40 Questions concerning securing Google Cloud res... \"I want to ensure that only authorized users c... 41 Questions concerning securing Google Cloud res... \"I'm concerned about the security of my sensit... 42 Questions concerning securing Google Cloud res... \"What are firewalls, and how do I configure th... 43 Questions concerning securing Google Cloud res... \"How can I connect my on-premises network to G... 44 Questions concerning securing Google Cloud res... \"What are the security features of Google Clou... 45 Questions concerning securing Google Cloud res... \"I want to connect my Cloud Function to a Clou... 46 Questions concerning securing Google Cloud res... \"I want to ensure that my network traffic is s... 47 Questions concerning securing Google Cloud res... \"What are security best practices for Google C... In\u00a0[26]: Copied! <pre>df_grouped_by_cluster[\"gemini_representative_questions_len\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: len(x.most_representative_qs))\ndf_grouped_by_cluster[\"gemini_representative_questions\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: x.most_representative_qs)\n# Print the resulting DataFrame\nunrolled_gemini_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n    'cluster_title': [x[\"just_summary\"]] * len(x['gemini_representative_questions']),\n    'representative_question': x['gemini_representative_questions']\n}), axis=1)\n\n# Concatenate the series and reset the index\nunrolled_gemini_df = pd.concat([unrolled_gemini_df['cluster_title'].explode(),\n                         unrolled_gemini_df['representative_question'].explode()],\n                        axis=1).reset_index(drop=True)\n</pre> df_grouped_by_cluster[\"gemini_representative_questions_len\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: len(x.most_representative_qs)) df_grouped_by_cluster[\"gemini_representative_questions\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: x.most_representative_qs) # Print the resulting DataFrame unrolled_gemini_df = df_grouped_by_cluster.apply(lambda x: pd.Series({     'cluster_title': [x[\"just_summary\"]] * len(x['gemini_representative_questions']),     'representative_question': x['gemini_representative_questions'] }), axis=1)  # Concatenate the series and reset the index unrolled_gemini_df = pd.concat([unrolled_gemini_df['cluster_title'].explode(),                          unrolled_gemini_df['representative_question'].explode()],                         axis=1).reset_index(drop=True) In\u00a0[27]: Copied! <pre>unrolled_gemini_df\n</pre> unrolled_gemini_df Out[27]: cluster_title representative_question 0 Questions concerning the practical usage and t... How can I create a virtual machine instance on... 1 Questions concerning the practical usage and t... What are the different machine types available... 2 Questions concerning the practical usage and t... Can you explain the different pricing options ... 3 Questions concerning the practical usage and t... How do I connect to my Compute Engine instance... 4 Questions concerning the practical usage and t... I accidentally deleted my Compute Engine insta... 5 Questions concerning the practical usage and t... I want to set up a cluster of Compute Engine i... 6 Questions concerning the practical usage and t... I'm having trouble connecting to my Virtual Ma... 7 Questions concerning the practical application... What is BigQuery, and how can I use it to anal... 8 Questions concerning the practical application... I have a large dataset that I want to analyze ... 9 Questions concerning the practical application... My BigQuery queries are taking a long time to ... 10 Questions concerning the practical application... I want to visualize my data in BigQuery using ... 11 Questions concerning the practical application... How can I use machine learning with BigQuery t... 12 Questions concerning the practical application... How can I use Google Cloud to build a data pip... 13 Questions concerning the practical application... How can I use Google Cloud to visualize my data? 14 Questions concerning the practical application... How can I use AutoML to build a machine learni... 15 Questions concerning the practical application... I need to monitor the performance of my deploy... 16 Questions concerning the practical application... How can I use Google Cloud to build a machine ... 17 Questions concerning the practical application... How can I use Google Cloud to deploy my machin... 18 Questions concerning Google Cloud database ser... What database services are available on Google... 19 Questions concerning Google Cloud database ser... What is the difference between Cloud SQL and C... 20 Questions concerning Google Cloud database ser... How do I migrate my existing database to Googl... 21 Questions concerning Google Cloud database ser... How can I scale my database on Google Cloud? 22 Questions concerning Google Cloud database ser... My Cloud SQL database is running out of storag... 23 Questions concerning Google Cloud database ser... I need to replicate my Cloud SQL database to a... 24 Questions concerning Google Cloud database ser... I'm experiencing slow query performance on my ... 25 Questions concerning the monitoring, troublesh... My application is experiencing performance iss... 26 Questions concerning the monitoring, troublesh... My application is experiencing high latency. C... 27 Questions concerning the monitoring, troublesh... I want to monitor the performance of my applic... 28 Questions concerning the monitoring, troublesh... How can I monitor the performance and logs of ... 29 Questions concerning the monitoring, troublesh... What is Cloud Monitoring, and how does it work? 30 Questions concerning cost optimization strateg... What are preemptible instances, and how can th... 31 Questions concerning cost optimization strateg... I'm getting billed for a Compute Engine instan... 32 Questions concerning cost optimization strateg... My Cloud Storage costs are higher than expecte... 33 Questions concerning cost optimization strateg... My Google Cloud bill is higher than expected t... 34 Questions concerning cost optimization strateg... I want to track the cost of my Google Cloud re... 35 Questions concerning cost optimization strateg... What are some best practices for optimizing my... 36 Questions concerning the automation of applica... I need to automate the deployment of my applic... 37 Questions concerning the automation of applica... What tools are available for managing my Googl... 38 Questions concerning the automation of applica... How can I automate tasks on Google Cloud? 39 Questions concerning the automation of applica... What is serverless computing, and what are its... 40 Questions concerning the automation of applica... What serverless platforms are available on Goo... 41 Questions concerning the automation of applica... How can I build and deploy a serverless applic... 42 Questions concerning the automation of applica... What is Cloud Functions, and how does it work? 43 Questions concerning the automation of applica... How can I use Google Cloud Run to deploy conta... 44 Questions concerning the practical aspects of ... I need to transfer a large amount of data to G... 45 Questions concerning the practical aspects of ... I accidentally deleted some files from my Clou... 46 Questions concerning the practical aspects of ... I want to make my data in Cloud Storage availa... 47 Questions concerning the practical aspects of ... How much does it cost to store data in Google ... 48 Questions concerning practical challenges and ... I'm trying to train a machine learning model o... 49 Questions concerning practical challenges and ... I want to deploy my trained machine learning m... 50 Questions concerning practical challenges and ... What is Vertex AI, and how can I use it? 51 Questions concerning practical challenges and ... I'm having trouble configuring IAM to use Vert... 52 Questions concerning securing Google Cloud res... What are the security features of Google Cloud... 53 Questions concerning securing Google Cloud res... How do I create a Virtual Private Cloud (VPC) ... 54 Questions concerning securing Google Cloud res... What are firewalls, and how do I configure the... 55 Questions concerning securing Google Cloud res... How can I connect my on-premises network to Go... 56 Questions concerning securing Google Cloud res... How can I secure my applications and data on G... 57 Questions concerning securing Google Cloud res... What is Identity and Access Management (IAM), ... 58 Questions concerning securing Google Cloud res... How can I implement multi-factor authenticatio... 59 Questions concerning securing Google Cloud res... What are security best practices for Google Cl... In\u00a0[28]: Copied! <pre>unrolled_gemini_df.to_csv(\"representative_eval_questions.csv\")\n</pre> unrolled_gemini_df.to_csv(\"representative_eval_questions.csv\")"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#getting-started","title":"\ud83c\udfac Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook, including the optional section, you will need to have the Owner role for your project.</p> <p>If you want to skip the optional section, you need at least the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> </ul>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#install-vertex-ai-sdk-and-other-required-packages","title":"Install Vertex AI SDK and Other Required Packages\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and Initialize Vertex AI SDK\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>REGION</code> unless you have a specific reason to change them.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#prepare-the-dataset","title":"Prepare the dataset\u00b6","text":"<p>For this demo we are using a hypothetical dataset of questions about Google Cloud Services</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#dataset-preprocessing","title":"Dataset Preprocessing\u00b6","text":"<p>Real world RAG systems have some anomalies in terms of the search queries - often, you will encounter single word queries or typos. In this step, we will preprocess and clean the dataset to remove the following types of queries:</p> <ul> <li>Very short and very long queries</li> <li>Near duplicates</li> </ul>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#visualize-distribution-of-question-lengths","title":"Visualize distribution of question lengths\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#generating-the-embeddings-for-the-questions","title":"Generating the embeddings for the questions\u00b6","text":"<p>Vertex AI embeddings models can generate optimized embeddings for various task types, such as document retrieval, question and answering, and fact verification. Task types are labels that optimize the embeddings that the model generates based on your intended use case.</p> <p>In this example, we will set the <code>TASK_TYPE</code> as <code>RETRIEVAL_DOCUMENT</code> as this is used to generate embeddings that are optimized for information retrieval</p> <p>Read more about the various <code>TASK_TYPE</code> offered by Vertex AI Embedding models here</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#cluster-the-questions","title":"Cluster the Questions\u00b6","text":"<p>While various clustering algorithms can be applied, Louvain community detection is a particularly suitable choice for this task due to its speed and effectiveness.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#vector-based-retrieval-clustering","title":"Vector-based Retrieval Clustering\u00b6","text":"<ol> <li>Store your embedded question set in a vector index</li> <li>Query the vector index with each question in the dataset, retrieving a topk-sized neighborhood of questions around the query question.</li> <li>Form a graph of questions by adding an edge between the query question and each of the retrieved questions</li> <li>Perform Louvain or Leiden community detection on the graph to create clusters of questions</li> </ol>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#analyze-clusters-using-gemini","title":"Analyze Clusters Using Gemini\u00b6","text":"<p>We can use Gemini to extract summaries, topics, relevant questions, sentiment or any other required information from the cluster. This allows us to quickly identify higher level patterns about the various questions from users, understand different user problems and much more insightful information.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#gemini-generated-cluster-analysis","title":"Gemini-generated Cluster Analysis\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#sample-questions-from-each-cluster-to-create-the-eval-dataset","title":"Sample Questions from Each Cluster to create the Eval Dataset\u00b6","text":"<ul> <li>We can sample randomly proportional to each cluster's size</li> <li>Or we can take samples from the most representative questions Gemini identified</li> </ul> <p>Probably need to sit down with an SME and compare both:</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#save-results-to-csv","title":"Save Results to CSV\u00b6","text":"<ul> <li>We do need to obtain ground truth answers</li> <li>But we can be confident we are putting the effort towards relevant, representative questions</li> </ul>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/gemini-curate-evaluation-data/curate_new_evals/#conclusion","title":"Conclusion\u00b6","text":"<p>With this notebook you can go from a mass of user queries from a RAG system and get immediate insights into the types of queries people are asking with useful clusters of queries described and analyzed by Gemini. This analysis can help inform decisions around how to improve the RAG system or it may highlight other issues in the business or product beyond what the chatbot can address. Finally, you can sample queries from these clusters to get a representative set of evaluation questions with which you can use to continuously evaluate the RAG system over time.</p> <p>As a next step will be to take this set of representative questions and obtain ground truth from users or subject matter experts and then evaluating performance using a service like Vertex AI Evaluation Service.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/","title":"Beyond Single Runs: Tendency Analysis of Non-Deterministic Eval Tasks","text":"In\u00a0[31]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.  Open in Colab       Open in Colab Enterprise       Open in Vertex AI Workbench       View on GitHub      Author(s) Alejandro Ballesta <p>NOTE: This notebook has been tested in the following environment:</p> <ul> <li>Python version &gt;= 3.10</li> </ul> In\u00a0[32]: Copied! <pre>%pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]\n%pip install --quiet --upgrade nest_asyncio\n</pre> %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation] %pip install --quiet --upgrade nest_asyncio <pre>\n[notice] A new release of pip is available: 24.2 -&gt; 24.3.1\n[notice] To update, run: python3 -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n[notice] A new release of pip is available: 24.2 -&gt; 24.3.1\n[notice] To update, run: python3 -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[33]: Copied! <pre># import IPython\n\n# app = IPython.Application.instance()\n# app.kernel.do_shutdown(True)\n</pre> # import IPython  # app = IPython.Application.instance() # app.kernel.do_shutdown(True) In\u00a0[34]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n</pre> import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user() In\u00a0[36]: Copied! <pre>PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nLOCATION = \"[your-location]\"  # @param {type:\"string\"}\nPROJECT_ID = \"golf-demo-abr\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\n\nimport vertexai\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)\n</pre> PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} LOCATION = \"[your-location]\"  # @param {type:\"string\"} PROJECT_ID = \"golf-demo-abr\"  # @param {type:\"string\"} LOCATION = \"us-central1\"  # @param {type:\"string\"}  import vertexai  vertexai.init(project=PROJECT_ID, location=LOCATION) In\u00a0[37]: Copied! <pre># General\nfrom IPython.display import display, Markdown, HTML\nimport logging\nimport nest_asyncio\nimport warnings\n\n# Main\nfrom vertexai.preview.evaluation import EvalTask\nimport pandas as pd\n</pre> # General from IPython.display import display, Markdown, HTML import logging import nest_asyncio import warnings  # Main from vertexai.preview.evaluation import EvalTask import pandas as pd In\u00a0[38]: Copied! <pre>logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\nnest_asyncio.apply()\nwarnings.filterwarnings(\"ignore\")\n</pre> logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR) nest_asyncio.apply() warnings.filterwarnings(\"ignore\") <p>To evaluate the RAG generated answers, the evaluation dataset is required to contain the following fields:</p> <ul> <li>Question</li> <li>Context</li> <li>RAG Generated Answer</li> </ul> In\u00a0[39]: Copied! <pre>questions = [\n    \"Which part of the brain does short-term memory seem to rely on?\",\n    \"What provided the Roman senate with exuberance?\",\n    \"What area did the Hasan-jalalians command?\",\n    \"How many pencils could be made from the carbon in an average adult human body?\"\n]\n\nretrieved_contexts = [\n    \"Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.\",\n    \"In 62 BC, Pompey returned victorious from Asia. The Senate, elated by its successes against Catiline, refused to ratify the arrangements that Pompey had made. Pompey, in effect, became powerless. Thus, when Julius Caesar returned from a governorship in Spain in 61 BC, he found it easy to make an arrangement with Pompey. Caesar and Pompey, along with Crassus, established a private agreement, now known as the First Triumvirate. Under the agreement, Pompey's arrangements would be ratified. Caesar would be elected consul in 59 BC, and would then serve as governor of Gaul for five years. Crassus was promised a future consulship.\",\n    \"The Seljuk Empire soon started to collapse. In the early 12th century, Armenian princes of the Zakarid noble family drove out the Seljuk Turks and established a semi-independent Armenian principality in Northern and Eastern Armenia, known as Zakarid Armenia, which lasted under the patronage of the Georgian Kingdom. The noble family of Orbelians shared control with the Zakarids in various parts of the country, especially in Syunik and Vayots Dzor, while the Armenian family of Hasan-Jalalians controlled provinces of Artsakh and Utik as the Kingdom of Artsakh.\",\n    \"The average adult human body contains enough carbon to make about 900 pencils\"\n]\n\ngenerated_answers = [\n    \"frontal lobe and the parietal lobe\",\n    \"The Roman Senate was filled with exuberance due to successes against Catiline.\",\n    \"The Hasan-Jalalians commanded the area of Syunik and Vayots Dzor.\",\n    \"around 901\"\n]\n\n\neval_dataset = pd.DataFrame(\n    {\n        \"prompt\": questions,\n        \"context\": retrieved_contexts,\n        \"response\": generated_answers,\n    }\n)\n</pre> questions = [     \"Which part of the brain does short-term memory seem to rely on?\",     \"What provided the Roman senate with exuberance?\",     \"What area did the Hasan-jalalians command?\",     \"How many pencils could be made from the carbon in an average adult human body?\" ]  retrieved_contexts = [     \"Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.\",     \"In 62 BC, Pompey returned victorious from Asia. The Senate, elated by its successes against Catiline, refused to ratify the arrangements that Pompey had made. Pompey, in effect, became powerless. Thus, when Julius Caesar returned from a governorship in Spain in 61 BC, he found it easy to make an arrangement with Pompey. Caesar and Pompey, along with Crassus, established a private agreement, now known as the First Triumvirate. Under the agreement, Pompey's arrangements would be ratified. Caesar would be elected consul in 59 BC, and would then serve as governor of Gaul for five years. Crassus was promised a future consulship.\",     \"The Seljuk Empire soon started to collapse. In the early 12th century, Armenian princes of the Zakarid noble family drove out the Seljuk Turks and established a semi-independent Armenian principality in Northern and Eastern Armenia, known as Zakarid Armenia, which lasted under the patronage of the Georgian Kingdom. The noble family of Orbelians shared control with the Zakarids in various parts of the country, especially in Syunik and Vayots Dzor, while the Armenian family of Hasan-Jalalians controlled provinces of Artsakh and Utik as the Kingdom of Artsakh.\",     \"The average adult human body contains enough carbon to make about 900 pencils\" ]  generated_answers = [     \"frontal lobe and the parietal lobe\",     \"The Roman Senate was filled with exuberance due to successes against Catiline.\",     \"The Hasan-Jalalians commanded the area of Syunik and Vayots Dzor.\",     \"around 901\" ]   eval_dataset = pd.DataFrame(     {         \"prompt\": questions,         \"context\": retrieved_contexts,         \"response\": generated_answers,     } ) In\u00a0[40]: Copied! <pre>answer_eval_task = EvalTask(\n    dataset=eval_dataset,\n    metrics=[\n        \"question_answering_quality\",\n        \"coherence\",\n        \"fluency\",\n        \"verbosity\"\n    ],\n    experiment=\"rag-eval-01\",\n)\n</pre> answer_eval_task = EvalTask(     dataset=eval_dataset,     metrics=[         \"question_answering_quality\",         \"coherence\",         \"fluency\",         \"verbosity\"     ],     experiment=\"rag-eval-01\", ) In\u00a0[\u00a0]: Copied! <pre>first_result = answer_eval_task.evaluate()\nsecond_result = answer_eval_task.evaluate()\n</pre> first_result = answer_eval_task.evaluate() second_result = answer_eval_task.evaluate() In\u00a0[42]: Copied! <pre>first_result.summary_metrics\n</pre> first_result.summary_metrics Out[42]: <pre>{'row_count': 4,\n 'question_answering_quality/mean': np.float64(3.25),\n 'question_answering_quality/std': np.float64(1.2583057392117916),\n 'coherence/mean': np.float64(2.75),\n 'coherence/std': np.float64(1.707825127659933),\n 'fluency/mean': np.float64(3.25),\n 'fluency/std': np.float64(1.5),\n 'verbosity/mean': np.float64(-1.0),\n 'verbosity/std': np.float64(0.816496580927726)}</pre> In\u00a0[43]: Copied! <pre>first_df = pd.DataFrame(first_result.metrics_table)\nsecond_df = pd.DataFrame(second_result.metrics_table)\n\nfirst_df = first_df.add_prefix('first_')\nsecond_df = second_df.add_prefix('second_')\n\nresult = pd.concat([first_df, second_df], axis=1)\nresult[[\"first_question_answering_quality/score\",\"second_question_answering_quality/score\"]]\n</pre> first_df = pd.DataFrame(first_result.metrics_table) second_df = pd.DataFrame(second_result.metrics_table)  first_df = first_df.add_prefix('first_') second_df = second_df.add_prefix('second_')  result = pd.concat([first_df, second_df], axis=1) result[[\"first_question_answering_quality/score\",\"second_question_answering_quality/score\"]]  Out[43]: first_question_answering_quality/score second_question_answering_quality/score 0 3.0 3.0 1 3.0 3.0 2 5.0 5.0 3 2.0 3.0 <p>You are probably seeing different results (row 3) for at least one row; if not, try again :)</p> In\u00a0[44]: Copied! <pre>generated_answers_modal_a = [\n    \"frontal lobe and the parietal lobe\",\n    \"The Roman Senate was filled with exuberance due to successes against Catiline.\",\n    \"The Hasan-Jalalians commanded the area of Syunik and Vayots Dzor.\",\n    \"around 900 pencils could be made from an adult human body\"\n]\n\ngenerated_answers_model_b = [\n    \"frontal lobe\",\n    \"The Roman Senate was filled with exuberance due to successes against Catiline.\",\n    \"The Hasan-Jalalians commanded Vayots Dzor\",\n    \"I would guess 901\"\n]\n\neval_dataset_model_a = pd.DataFrame(\n    {\n        \"prompt\": questions,\n        \"context\": retrieved_contexts,\n        \"response\": generated_answers_modal_a,\n    }\n)\neval_dataset_model_b = pd.DataFrame(\n    {\n        \"prompt\": questions,\n        \"context\": retrieved_contexts,\n        \"response\": generated_answers_model_b,\n    }\n)\n</pre> generated_answers_modal_a = [     \"frontal lobe and the parietal lobe\",     \"The Roman Senate was filled with exuberance due to successes against Catiline.\",     \"The Hasan-Jalalians commanded the area of Syunik and Vayots Dzor.\",     \"around 900 pencils could be made from an adult human body\" ]  generated_answers_model_b = [     \"frontal lobe\",     \"The Roman Senate was filled with exuberance due to successes against Catiline.\",     \"The Hasan-Jalalians commanded Vayots Dzor\",     \"I would guess 901\" ]  eval_dataset_model_a = pd.DataFrame(     {         \"prompt\": questions,         \"context\": retrieved_contexts,         \"response\": generated_answers_modal_a,     } ) eval_dataset_model_b = pd.DataFrame(     {         \"prompt\": questions,         \"context\": retrieved_contexts,         \"response\": generated_answers_model_b,     } ) In\u00a0[45]: Copied! <pre>#Let's first define the metrics we are going to use\n\nmetrics = [\"question_answering_quality\",\n        \"coherence\",\n        \"fluency\",\n        \"verbosity\"]\nn=40\n</pre> #Let's first define the metrics we are going to use  metrics = [\"question_answering_quality\",         \"coherence\",         \"fluency\",         \"verbosity\"] n=40 In\u00a0[\u00a0]: Copied! <pre>#Model A - 40 iteration evaluation\n\nmodel_a_results = []\nfor i in range(n):\n  print(f\"running experiment {i}-------------------\")\n  answer_eval_task = EvalTask(\n      dataset=eval_dataset_model_a,\n      metrics=metrics,\n      experiment=f\"rag-model-a-1-5-eval-{i}\",\n  )\n  model_a_results.append(answer_eval_task.evaluate().summary_metrics)\n</pre> #Model A - 40 iteration evaluation  model_a_results = [] for i in range(n):   print(f\"running experiment {i}-------------------\")   answer_eval_task = EvalTask(       dataset=eval_dataset_model_a,       metrics=metrics,       experiment=f\"rag-model-a-1-5-eval-{i}\",   )   model_a_results.append(answer_eval_task.evaluate().summary_metrics) In\u00a0[\u00a0]: Copied! <pre>#Model B- 40 iteration evaluation\n\nmodel_b_results = []\nfor i in range(n):\n  print(f\"running experiment {i}-------------------\")\n  answer_eval_task = EvalTask(\n      dataset=eval_dataset_model_b,\n      metrics=metrics,\n      experiment=f\"rag-model-b-eval-{i}\",\n  )\n  model_b_results.append(answer_eval_task.evaluate().summary_metrics)\n</pre> #Model B- 40 iteration evaluation  model_b_results = [] for i in range(n):   print(f\"running experiment {i}-------------------\")   answer_eval_task = EvalTask(       dataset=eval_dataset_model_b,       metrics=metrics,       experiment=f\"rag-model-b-eval-{i}\",   )   model_b_results.append(answer_eval_task.evaluate().summary_metrics) In\u00a0[48]: Copied! <pre>mean_metrics = [\n    \"question_answering_quality/mean\",\n    \"coherence/mean\",\n    \"fluency/mean\",\n    \"verbosity/mean\",\n]\n\naverage_metrics_model_a = model_a_results_df[mean_metrics].mean()\n\n\naverage_metrics_model_a\n</pre> mean_metrics = [     \"question_answering_quality/mean\",     \"coherence/mean\",     \"fluency/mean\",     \"verbosity/mean\", ]  average_metrics_model_a = model_a_results_df[mean_metrics].mean()   average_metrics_model_a Out[48]: <pre>question_answering_quality/mean    3.625000\ncoherence/mean                     3.293750\nfluency/mean                       3.572938\nverbosity/mean                    -0.793750\ndtype: float64</pre> In\u00a0[49]: Copied! <pre>average_metrics_model_b = model_b_results_df[mean_metrics].mean()\n\n\naverage_metrics_model_b\n</pre>  average_metrics_model_b = model_b_results_df[mean_metrics].mean()   average_metrics_model_b Out[49]: <pre>question_answering_quality/mean    3.15750\ncoherence/mean                     2.83125\nfluency/mean                       3.46250\nverbosity/mean                    -1.24375\ndtype: float64</pre> In\u00a0[53]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Sample data\ndataframes = [model_a_results_df, model_b_results_df]\ndataframe_names = [\"Model A\", \"Model B\"]\n\n\n# Create KDE plots\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\n# Define a color palette\ncolors = ['#1f77b4', '#ff7f0e']  # You can add more colors if you have more dataframes\n\nfor i, metric in enumerate(mean_metrics):\n    row, col = i // 2, i % 2\n\n    means = []\n    for df, name, color in zip(dataframes, dataframe_names, colors):\n        # Plot KDE\n        sns.kdeplot(data=df[metric], ax=axs[row, col], fill=True, alpha=0.5, label=name, color=color)\n\n        # Calculate mean and plot dotted line\n        mean_value = df[metric].mean()\n        means.append(mean_value)\n        axs[row, col].axvline(mean_value, color=color, linestyle=':', linewidth=2)\n\n    # Calculate mean difference and perform t-test\n    mean_diff = means[1] - means[0]\n    t_stat, p_value = stats.ttest_ind(dataframes[0][metric], dataframes[1][metric])\n\n    title = metric.split('/')[0].replace(\"_\", \" \").title()\n    diff_title = f\"{title}\\nMean Difference: {mean_diff:.4f}\"\n    axs[row, col].set_title(diff_title, fontsize=12)\n    axs[row, col].set_xlabel('Value')\n    axs[row, col].set_ylabel('Density')\n\n\n    # Add value labels at the bottom\n    ylim = axs[row, col].get_ylim()\n    for df, name, color, mean in zip(dataframes, dataframe_names, colors, means):\n        axs[row, col].text(mean, ylim[0], f'{name}\\n{mean:.2f}', color=color,\n                           ha='center', va='bottom', fontweight='bold', fontsize=12)\n\n    # Add text annotations\n    stats_text = \"\"\n    for df, name in zip(dataframes, dataframe_names):\n        median = df[metric].median()\n        mean = df[metric].mean()\n        std = df[metric].std()\n        stats_text += f'{name}:\\n  Median: {median:.2f}\\n  Mean: {mean:.2f}\\n  Std Dev: {std:.2f}\\n\\n'\n\n    # Add statistical significance information\n    if p_value &lt; 0.02:\n        sig_text = f\"Statistically Significant (p-value: {p_value:.4f})\"\n    else:\n        sig_text = f\"Not Statistically Significant (p-value: {p_value:.4f})\"\n\n    stats_text += f\"\\n{sig_text}\"\n\n    axs[row, col].text(0.05, 0.95, stats_text,\n                       transform=axs[row, col].transAxes, verticalalignment='top',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7),\n                       fontsize=8)\n\n# Remove extra subplot\nif len(metrics) % 2 != 0:\n    axs[-1, -1].axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy import stats  # Sample data dataframes = [model_a_results_df, model_b_results_df] dataframe_names = [\"Model A\", \"Model B\"]   # Create KDE plots fig, axs = plt.subplots(2, 2, figsize=(15, 15)) # Define a color palette colors = ['#1f77b4', '#ff7f0e']  # You can add more colors if you have more dataframes  for i, metric in enumerate(mean_metrics):     row, col = i // 2, i % 2      means = []     for df, name, color in zip(dataframes, dataframe_names, colors):         # Plot KDE         sns.kdeplot(data=df[metric], ax=axs[row, col], fill=True, alpha=0.5, label=name, color=color)          # Calculate mean and plot dotted line         mean_value = df[metric].mean()         means.append(mean_value)         axs[row, col].axvline(mean_value, color=color, linestyle=':', linewidth=2)      # Calculate mean difference and perform t-test     mean_diff = means[1] - means[0]     t_stat, p_value = stats.ttest_ind(dataframes[0][metric], dataframes[1][metric])      title = metric.split('/')[0].replace(\"_\", \" \").title()     diff_title = f\"{title}\\nMean Difference: {mean_diff:.4f}\"     axs[row, col].set_title(diff_title, fontsize=12)     axs[row, col].set_xlabel('Value')     axs[row, col].set_ylabel('Density')       # Add value labels at the bottom     ylim = axs[row, col].get_ylim()     for df, name, color, mean in zip(dataframes, dataframe_names, colors, means):         axs[row, col].text(mean, ylim[0], f'{name}\\n{mean:.2f}', color=color,                            ha='center', va='bottom', fontweight='bold', fontsize=12)      # Add text annotations     stats_text = \"\"     for df, name in zip(dataframes, dataframe_names):         median = df[metric].median()         mean = df[metric].mean()         std = df[metric].std()         stats_text += f'{name}:\\n  Median: {median:.2f}\\n  Mean: {mean:.2f}\\n  Std Dev: {std:.2f}\\n\\n'      # Add statistical significance information     if p_value &lt; 0.02:         sig_text = f\"Statistically Significant (p-value: {p_value:.4f})\"     else:         sig_text = f\"Not Statistically Significant (p-value: {p_value:.4f})\"      stats_text += f\"\\n{sig_text}\"      axs[row, col].text(0.05, 0.95, stats_text,                        transform=axs[row, col].transAxes, verticalalignment='top',                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7),                        fontsize=8)  # Remove extra subplot if len(metrics) % 2 != 0:     axs[-1, -1].axis('off')  plt.tight_layout() plt.show() <p>These charts illustrate the performance of the two models across various metrics.  Model A generally demonstrates better quality, verbosity, and coherence than Model B. However, there is no statistically significant difference between the models in terms of fluency.</p> <p>Note: We can evidence that sometimes the normal distribution is not as clear as other times, that is why is important to look at the charts before making any definitive conclusion</p> In\u00a0[51]: Copied! <pre># Function to create radar chart\ndef radar_chart(data, title):\n    angles = np.linspace(0, 2*np.pi, len(mean_metrics), endpoint=False)\n    values = data[mean_metrics].mean().values\n    values = np.concatenate((values, [values[0]]))  # repeat the first value to close the polygon\n    angles = np.concatenate((angles, [angles[0]]))  # repeat the first angle to close the polygon\n\n    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n    ax.plot(angles, values)\n    ax.fill(angles, values, alpha=0.25)\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels([m.split('/')[0] for m in mean_metrics], wrap=True)\n    ax.set_title(title)\n    plt.tight_layout()\n    plt.show()\n\n# # Create radar charts for each dataframe\n# for df, name in zip(dataframes, dataframe_names):\n#     radar_chart(df, f'Radar Chart for {name}')\n\n# Create overlapping radar chart\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n\nangles = np.linspace(0, 2*np.pi, len(mean_metrics), endpoint=False)\nangles = np.concatenate((angles, [angles[0]]))  # repeat the first angle to close the polygon\n\nfor df, name in zip(dataframes, dataframe_names):\n    values = df[mean_metrics].mean().values\n    values = np.concatenate((values, [values[0]]))  # repeat the first value to close the polygon\n    ax.plot(angles, values, linewidth=2, label=name)\n    ax.fill(angles, values, alpha=0.25)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels([m.split('/')[0] for m in mean_metrics], wrap=True)\nax.set_title('Overlapping Radar Chart for All Experiments')\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n\nplt.tight_layout()\nplt.show()\n</pre> # Function to create radar chart def radar_chart(data, title):     angles = np.linspace(0, 2*np.pi, len(mean_metrics), endpoint=False)     values = data[mean_metrics].mean().values     values = np.concatenate((values, [values[0]]))  # repeat the first value to close the polygon     angles = np.concatenate((angles, [angles[0]]))  # repeat the first angle to close the polygon      fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))     ax.plot(angles, values)     ax.fill(angles, values, alpha=0.25)     ax.set_xticks(angles[:-1])     ax.set_xticklabels([m.split('/')[0] for m in mean_metrics], wrap=True)     ax.set_title(title)     plt.tight_layout()     plt.show()  # # Create radar charts for each dataframe # for df, name in zip(dataframes, dataframe_names): #     radar_chart(df, f'Radar Chart for {name}')  # Create overlapping radar chart fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))  angles = np.linspace(0, 2*np.pi, len(mean_metrics), endpoint=False) angles = np.concatenate((angles, [angles[0]]))  # repeat the first angle to close the polygon  for df, name in zip(dataframes, dataframe_names):     values = df[mean_metrics].mean().values     values = np.concatenate((values, [values[0]]))  # repeat the first value to close the polygon     ax.plot(angles, values, linewidth=2, label=name)     ax.fill(angles, values, alpha=0.25)  ax.set_xticks(angles[:-1]) ax.set_xticklabels([m.split('/')[0] for m in mean_metrics], wrap=True) ax.set_title('Overlapping Radar Chart for All Experiments') ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))  plt.tight_layout() plt.show()"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#beyond-single-runs-tendency-analysis-of-non-deterministic-eval-tasks","title":"Beyond Single Runs: Tendency Analysis of Non-Deterministic Eval Tasks\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#overview","title":"Overview\u00b6","text":"<p>This notebook explores the application of statistical methods to gain insights into the behavior of the Vertex AI Python SDK for Gen AI Evaluation Service for generated answers during Question Answering (QA) tasks. Due to the inherent variability, non-deterministic or probabilistic outcomes in these tasks, relying on a single execution may not provide a comprehensive understanding of the real performance.</p> <p>We'll delve into techniques to aggregate and analyze results from multiple runs, enabling us to:</p> <ul> <li>Identifying the problem: running multiple tests over the same data to understand the non-deterministic outputs</li> <li>Setting up a more reliable approach: using the same pair of questions and contexts, we will use the generated answers of two different models (ex. Gemini 1.5 vs Gemini Flash) as an input to our method and demonstrate an extensive methodology to derive better conclusions</li> <li>Running the experiment: for each model, we will run the QA Gen AI Evaluation Service method 40 times to extract multiple performance metrics</li> <li>Quantifying Performance: we will calculate key statistical measures like mean, median, and standard deviation to gauge central tendency and variability across executions.</li> <li>Identifying Performance: In this section, using the statistical measures calculated, we will visualize the distributions of each metric overlapping the results of both models in the same chart</li> <li>Assessing Confidence: applying confidence intervals or hypothesis testing we will draw statistically sound conclusions about the performance of eval tasks. By leveraging these statistical tools, we aim to move beyond single-run observations and gain a more robust understanding of the true behavior and capabilities of these non-deterministic tasks.</li> <li>Comparing Results: we will plot a radar chart and determine which model is better for our own goals</li> </ul> <p>Note: The purpose of this analysis is not to have a completely statistical (since we are making a lot of assumptions) proof, but to gain a more holistic view of the architecture's performance</p> <p>Documentation Reference</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#getting-started","title":"Getting Started\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#install-vertex-ai-sdk-for-gen-ai-evaluation-service","title":"Install Vertex AI SDK for Gen AI Evaluation Service\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#restart-runtime","title":"Restart runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.</p> <p>The restart might take a minute or longer. After it's restarted, continue to the next step.</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#authenticate-your-notebook-environment-colab-only","title":"Authenticate your notebook environment (Colab only)\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#set-google-cloud-project-information-and-initialize-vertex-ai-sdk","title":"Set Google Cloud project information and initialize Vertex AI SDK\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#library-settings","title":"Library settings\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#identifying-the-problem","title":"Identifying the problem\u00b6","text":"<p>In this section we are going to run 2 independent evaluation tasks and gather evidence of different results under the same following conditions:</p> <ul> <li>Questions: set of 5 random questions (same questions for each test)</li> <li>Retrieved context: for the purpose of this notebook we assume that the context is retrieved with any particular method</li> <li>Generated answers: examples of text generated by the models</li> </ul>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#prepare-dataset","title":"Prepare Dataset\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#select-metric-and-define-evaltask","title":"Select Metric and Define EvalTask\u00b6","text":"<p>Choose from the following metrics for an evaluation task. For more information about the supported evaluation metrics and how to use each metric, please see Evaluation methods and metrics.</p> <ul> <li><code>question_answering_quality</code> (overall quality)</li> <li><code>question_answering_relevance</code></li> <li><code>question_answering_helpfulness</code></li> <li><code>fulfillment</code></li> </ul> <p>You can run evaluation for just one metric, or a combination of metrics. For example, we create an <code>EvalTask</code> named <code>answer_eval_task</code> with all the QA-related metrics to compute all the metrics in one eval run as follows:</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#run-evaluation","title":"Run Evaluation\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#lets-compare-the-results-of-running-the-same-experiment-twice","title":"Let's compare the results of running the same experiment twice\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#setting-up-a-more-reliable-approach","title":"Setting up a more reliable approach\u00b6","text":"<p>if you are comparing two different architectures (Ex, Gemini 1.5 generations vs Gemini Flash generations), it's recommended to use sample distributions to understand the perfomance of each approach.</p> <p>Let's see one example</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#modal-a-vs-model-b-set-up","title":"Modal A vs Model B set up\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#running-the-experiment","title":"Running the experiment\u00b6","text":"<p>Once the answers have been generated, let's run 40 identical experiments for each approach</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#quantifying-performance","title":"Quantifying Performance\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#get-the-mean-of-each-summary-metric-for-the-40-experiments","title":"Get the mean of each summary metric for the 40 experiments\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#identifying-performance-and-assessing-confidence","title":"Identifying Performance and Assessing Confidence\u00b6","text":"<p>In this section we procced to create distribution charts for the mean of each metric evaluated per model/approach. We overlap the results of Model A on top of Model B outcomes and evidence the gaps between them.</p> <p>For each metric, we are also calculating the difference of the mean between the two series and also running a t-test so we can have certainty about our conclusions. If the test is statistically significant it means the difference might be consistent enough to determine a clear winner.</p> <p>Why the Independent Samples t-test?</p> <ul> <li>Continuous Data: The quality scores (0-5) are continuous numerical data, suitable for a t-test. Two Independent Groups: You have two separate sets of results (series), and you want to compare their means.</li> <li>Assumption of Normality: The t-test assumes that the data within each group is approximately normally distributed. While you have a limited sample size (40), if the distribution of scores isn't heavily skewed or has extreme outliers, the t-test is reasonably robust.</li> </ul>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#comparing-results","title":"Comparing Results\u00b6","text":"<p>Now we can plot a more general overlapping view using a radar chart to understand in which areas the approaches differ</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/tendency-based-evaluation/Beyond_Single_Runs_Tendency_Analysis_of_Non_Deterministic_Eval_Tasks/#conclusions","title":"Conclusions\u00b6","text":"<p>In this notebook, we demonstrated the power of statistical analysis in understanding the performance of non-deterministic eval tasks. By leveraging techniques like the independent samples t-test, we moved beyond single-run observations to gain a more comprehensive understanding of the variability and underlying trends in our results.</p> <p>The insights gleaned from this analysis can inform future iterations of the eval task, guide decision-making based on statistically sound evidence, and pave the way for more robust and reliable evaluations in the face of inherent uncertainty.</p> <p>Note: The purpose of this analysis is not to have a rigorous statistical proof (since we are making a lot of assumptions), but to gain a more holistic view of the architecture's performance</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/","title":"Evaluate Classification","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2023 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. <ul> <li>Dataset used for this sample  CARER: Contextualized Affect Representations for Emotion Recognition by Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. </li> </ul> In\u00a0[1]: Copied! <pre># from https://github.com/dair-ai/emotion_dataset - modified to binary classification\ntexts = [\n  'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n  'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n  'i like to have the same breathless feeling as a reader eager to see what will happen next',\n  'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',\n  'i don t feel particularly agitated',\n  'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',\n  'i pay attention it deepens into a feeling of being invaded and helpless',\n  'i just feel extremely comfortable with the group of people that i dont even need to hide myself',\n  'i find myself in the odd position of feeling supportive of',\n  'i was feeling as heartbroken as im sure katniss was',\n  'i feel a little mellow today',\n  'i feel like my only role now would be to tear your sails with my pessimism and discontent',\n  'i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight',\n  'i feel like reds and purples are just so rich and kind of perfect']\n\n# Positive Sentiment = 1\n# Negative Sentiment = 0\nground_truth = [ 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n\n# Sample prediction\npredicted = [ 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n</pre> # from https://github.com/dair-ai/emotion_dataset - modified to binary classification texts = [   'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',   'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',   'i like to have the same breathless feeling as a reader eager to see what will happen next',   'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',   'i don t feel particularly agitated',   'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',   'i pay attention it deepens into a feeling of being invaded and helpless',   'i just feel extremely comfortable with the group of people that i dont even need to hide myself',   'i find myself in the odd position of feeling supportive of',   'i was feeling as heartbroken as im sure katniss was',   'i feel a little mellow today',   'i feel like my only role now would be to tear your sails with my pessimism and discontent',   'i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight',   'i feel like reds and purples are just so rich and kind of perfect']  # Positive Sentiment = 1 # Negative Sentiment = 0 ground_truth = [ 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1]  # Sample prediction predicted = [ 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1] In\u00a0[2]: Copied! <pre>def count_tp_fp_fn(ground_truth_list: list, predicted_list: list, positive_class) -&gt; tuple:\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    for i in range(len(ground_truth_list)):\n        if ground_truth_list[i] == positive_class:\n            if predicted_list[i] == positive_class:\n                true_positives += 1\n            else:\n                false_negatives += 1\n        elif predicted_list[i] == positive_class:\n            false_positives += 1\n\n    return true_positives, false_positives, false_negatives\n</pre> def count_tp_fp_fn(ground_truth_list: list, predicted_list: list, positive_class) -&gt; tuple:     true_positives = 0     false_positives = 0     false_negatives = 0          for i in range(len(ground_truth_list)):         if ground_truth_list[i] == positive_class:             if predicted_list[i] == positive_class:                 true_positives += 1             else:                 false_negatives += 1         elif predicted_list[i] == positive_class:             false_positives += 1      return true_positives, false_positives, false_negatives In\u00a0[3]: Copied! <pre># Sample results\npositive_class = 1\n\ntrue_positives, false_positives, false_negatives = count_tp_fp_fn(ground_truth, predicted, positive_class)\n\nprint(f\"True Positives: {true_positives}\")\nprint(f\"False Positives: {false_positives}\")\nprint(f\"False Negatives: {false_negatives}\")\n</pre> # Sample results positive_class = 1  true_positives, false_positives, false_negatives = count_tp_fp_fn(ground_truth, predicted, positive_class)  print(f\"True Positives: {true_positives}\") print(f\"False Positives: {false_positives}\") print(f\"False Negatives: {false_negatives}\") <pre>True Positives: 5\nFalse Positives: 3\nFalse Negatives: 2\n</pre> <p>$precision = \\frac{TP}{TP + FP}$</p> In\u00a0[4]: Copied! <pre>precision = true_positives / (true_positives + false_positives)\nprint(f\"Precision: {precision:.3f}\")\n</pre> precision = true_positives / (true_positives + false_positives) print(f\"Precision: {precision:.3f}\") <pre>Precision: 0.625\n</pre> <p>$recall = \\frac{TP}{TP+FN}$</p> In\u00a0[5]: Copied! <pre>recall = true_positives / (true_positives + false_negatives)\nprint(f\"Recall: {recall:.3f}\")\n</pre> recall = true_positives / (true_positives + false_negatives) print(f\"Recall: {recall:.3f}\") <pre>Recall: 0.714\n</pre> In\u00a0[6]: Copied! <pre>print(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\n</pre> print(f\"Precision: {precision:.3f}\") print(f\"Recall: {recall:.3f}\") <pre>Precision: 0.625\nRecall: 0.714\n</pre> <p>First Method: using precision and recall</p> <p>$F_1 = \\cfrac{2}{\\cfrac{1}{precision}+\\cfrac{1}{recall}}$</p> In\u00a0[7]: Copied! <pre>f1_score_a = 2 / ((1 / precision) + (1 / recall))\nprint(f\"F1 Score calculated using precision and recall: {f1_score_a:.3f}\")\n</pre> f1_score_a = 2 / ((1 / precision) + (1 / recall)) print(f\"F1 Score calculated using precision and recall: {f1_score_a:.3f}\") <pre>F1 Score calculated using precision and recall: 0.667\n</pre> <p>Second method using TP, FP and FN</p> <p>$F_1 = \\cfrac{TP}{TP + \\cfrac{FP+FN}{2}}$</p> In\u00a0[8]: Copied! <pre>f1_score_b = true_positives / (true_positives + (false_positives + false_negatives) / 2)\nprint(f\"F1 Score calculated using TP FP and FN: {f1_score_b:.3f}\")\n</pre> f1_score_b = true_positives / (true_positives + (false_positives + false_negatives) / 2) print(f\"F1 Score calculated using TP FP and FN: {f1_score_b:.3f}\") <pre>F1 Score calculated using TP FP and FN: 0.667\n</pre> In\u00a0[9]: Copied! <pre>import math\nprint(f\"The two f1 scores are equal? {f1_score_a == f1_score_b}\")\nprint(f\"The two f1 scores are close up to 15 decimal places? {math.isclose(f1_score_a, f1_score_b, abs_tol=0.0000000000000001)}\")\nprint(f1_score_a)\nprint(f1_score_b)\n</pre> import math print(f\"The two f1 scores are equal? {f1_score_a == f1_score_b}\") print(f\"The two f1 scores are close up to 15 decimal places? {math.isclose(f1_score_a, f1_score_b, abs_tol=0.0000000000000001)}\") print(f1_score_a) print(f1_score_b) <pre>The two f1 scores are equal? True\nThe two f1 scores are close up to 15 decimal places? True\n0.6666666666666666\n0.6666666666666666\n</pre> <ul> <li>Dataset used for this sample  CARER: Contextualized Affect Representations for Emotion Recognition by Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. </li> </ul> In\u00a0[10]: Copied! <pre># from https://github.com/dair-ai/emotion_dataset\nmulti_class_texts = ['im feeling rather rotten so im not very ambitious right now',\n  'im updating my blog because i feel shitty',\n  'i never make her separate from me because i don t ever want her to feel like i m ashamed with her',\n  'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n  'i was feeling a little vain when i did this one',\n  'i cant walk into a shop anywhere where i do not feel uncomfortable',\n  'i felt anger when at the end of a telephone call',\n  'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n  'i like to have the same breathless feeling as a reader eager to see what will happen next',\n  'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',\n  'i don t feel particularly agitated',\n  'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',\n  'i pay attention it deepens into a feeling of being invaded and helpless',\n  'i just feel extremely comfortable with the group of people that i dont even need to hide myself',\n  'i find myself in the odd position of feeling supportive of',\n  'i was feeling as heartbroken as im sure katniss was',\n  'i feel a little mellow today',\n  'i feel like my only role now would be to tear your sails with my pessimism and discontent',\n  'i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight',\n  'i feel like reds and purples are just so rich and kind of perfect']\n\n\n# 0: 'sadness'\n# 1: 'joy'\n# 2: 'love'\n# 3: 'anger'\n# 4: 'fear'\n# 5: 'surprise'\nground_truth_multi = [0, 0, 0, 1, 0, 4, 3, 1, 1, 3, 4, 0, 4, 1, 2, 0, 1, 0, 3, 1]\npredicted_multi =    [0, 1, 2, 1, 2, 4, 3, 3, 1, 4, 4, 0, 4, 1, 2, 0, 1, 0, 3, 1]\n</pre> # from https://github.com/dair-ai/emotion_dataset multi_class_texts = ['im feeling rather rotten so im not very ambitious right now',   'im updating my blog because i feel shitty',   'i never make her separate from me because i don t ever want her to feel like i m ashamed with her',   'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',   'i was feeling a little vain when i did this one',   'i cant walk into a shop anywhere where i do not feel uncomfortable',   'i felt anger when at the end of a telephone call',   'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',   'i like to have the same breathless feeling as a reader eager to see what will happen next',   'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',   'i don t feel particularly agitated',   'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',   'i pay attention it deepens into a feeling of being invaded and helpless',   'i just feel extremely comfortable with the group of people that i dont even need to hide myself',   'i find myself in the odd position of feeling supportive of',   'i was feeling as heartbroken as im sure katniss was',   'i feel a little mellow today',   'i feel like my only role now would be to tear your sails with my pessimism and discontent',   'i feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight',   'i feel like reds and purples are just so rich and kind of perfect']   # 0: 'sadness' # 1: 'joy' # 2: 'love' # 3: 'anger' # 4: 'fear' # 5: 'surprise' ground_truth_multi = [0, 0, 0, 1, 0, 4, 3, 1, 1, 3, 4, 0, 4, 1, 2, 0, 1, 0, 3, 1] predicted_multi =    [0, 1, 2, 1, 2, 4, 3, 3, 1, 4, 4, 0, 4, 1, 2, 0, 1, 0, 3, 1] In\u00a0[11]: Copied! <pre># Sample Results\nn_class = 5\nmulticlass_results_list = [count_tp_fp_fn(ground_truth_multi, predicted_multi, i) for i in range(n_class)]\ntrue_positives_list = [class_result[0] for class_result in multiclass_results_list]\nfalse_positives_list = [class_result[1] for class_result in multiclass_results_list]\nfalse_negatives_list = [class_result[2] for class_result in multiclass_results_list]\n</pre> # Sample Results n_class = 5 multiclass_results_list = [count_tp_fp_fn(ground_truth_multi, predicted_multi, i) for i in range(n_class)] true_positives_list = [class_result[0] for class_result in multiclass_results_list] false_positives_list = [class_result[1] for class_result in multiclass_results_list] false_negatives_list = [class_result[2] for class_result in multiclass_results_list] In\u00a0[12]: Copied! <pre>true_positives_list\n</pre> true_positives_list Out[12]: <pre>[4, 5, 1, 2, 3]</pre> In\u00a0[13]: Copied! <pre>false_positives_list\n</pre> false_positives_list Out[13]: <pre>[0, 1, 2, 1, 1]</pre> In\u00a0[14]: Copied! <pre>false_negatives_list\n</pre> false_negatives_list Out[14]: <pre>[3, 1, 0, 1, 0]</pre> <p>$Macro F_1 = \\cfrac{\\sum_{i=1}^{n} F1 Score_i}{n}$</p> <p>Example for 2 classes</p> In\u00a0[15]: Copied! <pre>f1_score_0 = true_positives_list[0] / (true_positives_list[0] + (false_positives_list[0] + false_negatives_list[0]) / 2)\nf1_score_1 = true_positives_list[1] / (true_positives_list[1] + (false_positives_list[1] + false_negatives_list[1]) / 2)\n</pre> f1_score_0 = true_positives_list[0] / (true_positives_list[0] + (false_positives_list[0] + false_negatives_list[0]) / 2) f1_score_1 = true_positives_list[1] / (true_positives_list[1] + (false_positives_list[1] + false_negatives_list[1]) / 2) In\u00a0[16]: Copied! <pre>macro_f1_score = (f1_score_0 + f1_score_1) / 2\n\nprint(macro_f1_score)\n</pre> macro_f1_score = (f1_score_0 + f1_score_1) / 2  print(macro_f1_score) <pre>0.7803030303030303\n</pre> <p>Example for all classes</p> In\u00a0[17]: Copied! <pre>f1_scores = [true_positives_list[i] / (true_positives_list[i] + (false_positives_list[i] + false_negatives_list[i]) / 2) for i in range(n_class)]\n</pre> f1_scores = [true_positives_list[i] / (true_positives_list[i] + (false_positives_list[i] + false_negatives_list[i]) / 2) for i in range(n_class)] In\u00a0[18]: Copied! <pre>print(f1_scores)\n</pre> print(f1_scores) <pre>[0.7272727272727273, 0.8333333333333334, 0.5, 0.6666666666666666, 0.8571428571428571]\n</pre> In\u00a0[19]: Copied! <pre>macro_f1_score = sum(f1_scores) / len(f1_scores)\n\nprint(macro_f1_score)\n</pre> macro_f1_score = sum(f1_scores) / len(f1_scores)  print(macro_f1_score) <pre>0.7168831168831169\n</pre> In\u00a0[20]: Copied! <pre>from statistics import mean\n</pre> from statistics import mean In\u00a0[21]: Copied! <pre>mean(f1_scores)\n</pre> mean(f1_scores) Out[21]: <pre>0.7168831168831169</pre> <p>$Micro F_1 = \\cfrac{\\sum_{i=1}^{n} TP_i}{\\sum_{i=1}^{n} TP_i + \\cfrac{\\sum_{i=1}^{n} FP_i + \\sum_{i=1}^{n} FN_i}{2}}$</p> In\u00a0[22]: Copied! <pre>micro_f1_score = sum(true_positives_list) / (sum(true_positives_list) + ((sum(false_positives_list) + sum(false_negatives_list))/2))\n</pre> micro_f1_score = sum(true_positives_list) / (sum(true_positives_list) + ((sum(false_positives_list) + sum(false_negatives_list))/2)) In\u00a0[23]: Copied! <pre>print(micro_f1_score)\n</pre> print(micro_f1_score) <pre>0.75\n</pre> In\u00a0[24]: Copied! <pre>tp_sum = sum(true_positives_list)\nfp_sum = sum(false_positives_list)\nfn_sum = sum(false_negatives_list)\n</pre> tp_sum = sum(true_positives_list) fp_sum = sum(false_positives_list) fn_sum = sum(false_negatives_list) In\u00a0[25]: Copied! <pre>micro_f1_score = tp_sum / (tp_sum + (fp_sum + fn_sum) / 2)\n</pre> micro_f1_score = tp_sum / (tp_sum + (fp_sum + fn_sum) / 2) In\u00a0[26]: Copied! <pre>print(micro_f1_score)\n</pre> print(micro_f1_score) <pre>0.75\n</pre> In\u00a0[27]: Copied! <pre>!pip install -U scikit-learn\n</pre> !pip install -U scikit-learn <pre>Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.3.0)\nCollecting scikit-learn\n  Downloading scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9 MB 4.2 MB/s eta 0:00:01\nRequirement already satisfied: joblib&gt;=1.1.1 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.11.2)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.25.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.2.0)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.3.0\n    Uninstalling scikit-learn-1.3.0:\n      Successfully uninstalled scikit-learn-1.3.0\nSuccessfully installed scikit-learn-1.3.1\n</pre> In\u00a0[28]: Copied! <pre>from sklearn.metrics import f1_score\n</pre> from sklearn.metrics import f1_score In\u00a0[29]: Copied! <pre># Per class\nf1_score(ground_truth_multi, predicted_multi, average=None)\n</pre> # Per class f1_score(ground_truth_multi, predicted_multi, average=None) Out[29]: <pre>array([0.72727273, 0.83333333, 0.5       , 0.66666667, 0.85714286])</pre> In\u00a0[30]: Copied! <pre># Macro\nf1_score(ground_truth_multi, predicted_multi, average='macro')\n</pre> # Macro f1_score(ground_truth_multi, predicted_multi, average='macro') Out[30]: <pre>0.7168831168831169</pre> In\u00a0[31]: Copied! <pre># Micro\nf1_score(ground_truth_multi, predicted_multi, average='micro')\n</pre> # Micro f1_score(ground_truth_multi, predicted_multi, average='micro') Out[31]: <pre>0.75</pre>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#evaluate-classification","title":"Evaluate Classification\u00b6","text":"Author(s) Renato Leite (renatoleite@), Egon Soares (egon@) Last updated 09/05/2023"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#per-class","title":"Per Class\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#f1-score","title":"F1 Score\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#multiclass","title":"Multiclass\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#macrof1","title":"MacroF1\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#microf1","title":"MicroF1\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_1_Classification/#scikit-learn","title":"Scikit Learn\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_2_Summarization/","title":"Evaluate Summarization","text":"Author(s) Renato Leite (renatoleite@), Egon Soares (egon@) Last updated 09/05/2023 <p>ROUGE-L uses LCS-based F-measure to estimate the similarity between two summaries X of length m and Y of length n, assuming X is a reference summary sentence and Y is a candidate summary sentence, as follows:</p> <p>$Recall_{lcs} = \\cfrac{LCS(X,Y)}{m}$</p> <p>$Precision_{lcs} = \\cfrac{LCS(X,Y)}{n}$</p> <p>$F_{lcs} = \\cfrac{(1+\\beta\u00b2)Recall_{lcs} Precision_{lcs}}{\\beta\u00b2Precision_{lcs}+Recall_{lcs}}$</p> <p>$\\beta = \\cfrac{Precision_{lcs}}{Recall_{lcs}}$</p> <p>$ROUGE-L = \\cfrac{(1+(\\cfrac{Precision_{lcs}}{Recall_{lcs}})\u00b2)Recall_{lcs} Precision_{lcs}}{(\\cfrac{Precision_{lcs}}{Recall_{lcs}})\u00b2Precision_{lcs}+Recall_{lcs}}$</p> <p>Size of LCS:</p> <p>$ LCS(X_i, Y_j) =   \\begin{cases}     0       &amp; \\quad \\text{if } i=0 \\text{ or } j=0 \\\\     LCS(X_{i-1}, Y_{j-1}) + 1  &amp; \\quad \\text{if } i,j&gt;0 \\text{ and } x_i=y_j \\\\     max\\left\\{LCS(X_i, Y_{j-1}),LCS(X_{i-1}, Y_j)\\right\\}  &amp; \\quad \\text{if } i,j&gt;0 \\text{ and } x_i \\neq y_j   \\end{cases} $</p> <p>String of LCS:</p> <p>$ LCS(X_i, Y_j) =   \\begin{cases}     \\epsilon       &amp; \\quad \\text{if } i=0 \\text{ or } j=0 \\\\     LCS(X_{i-1}, Y_{j-1})\\frown x_i  &amp; \\quad \\text{if } i,j&gt;0 \\text{ and } x_i=y_j \\\\     max\\left\\{LCS(X_i, Y_{j-1}),LCS(X_{i-1}, Y_j)\\right\\}  &amp; \\quad \\text{if } i,j&gt;0 \\text{ and } x_i \\neq y_j   \\end{cases} $</p> <p>$\\epsilon \\implies \\text{empty string}$</p> <p>$\\frown \\implies \\text{append element}$</p> In\u00a0[1]: Copied! <pre>reference = \"police killed the gunman\"\ncandidate = \"police kill the gunman\"\n</pre> reference = \"police killed the gunman\" candidate = \"police kill the gunman\" In\u00a0[2]: Copied! <pre>#Recursive LCS\ndef lcs(X, Y, m, n):\n    if m == 0 or n == 0:\n        return 0\n    elif X[m-1] == Y[n-1]:\n        return 1 + lcs(X, Y, m-1, n-1)\n    else:\n        return max(lcs(X, Y, m, n-1), lcs(X, Y, m-1, n))\n</pre> #Recursive LCS def lcs(X, Y, m, n):     if m == 0 or n == 0:         return 0     elif X[m-1] == Y[n-1]:         return 1 + lcs(X, Y, m-1, n-1)     else:         return max(lcs(X, Y, m, n-1), lcs(X, Y, m-1, n)) In\u00a0[3]: Copied! <pre>def lcs_sequence(X, Y, m, n):\n    if m == 0 or n == 0:\n        return []\n    elif X[m-1] == Y[n-1]:\n        \n        return lcs_sequence(X, Y, m-1, n-1) + [X[m-1]]\n    else:\n        a = lcs_sequence(X, Y, m, n-1)\n        b = lcs_sequence(X, Y, m-1, n)\n        return a if len(a) &gt; len(b) else b\n</pre> def lcs_sequence(X, Y, m, n):     if m == 0 or n == 0:         return []     elif X[m-1] == Y[n-1]:                  return lcs_sequence(X, Y, m-1, n-1) + [X[m-1]]     else:         a = lcs_sequence(X, Y, m, n-1)         b = lcs_sequence(X, Y, m-1, n)         return a if len(a) &gt; len(b) else b In\u00a0[4]: Copied! <pre>X = reference.split()\nY = candidate.split()\nlcs(X, Y, len(X), len(Y))\n</pre> X = reference.split() Y = candidate.split() lcs(X, Y, len(X), len(Y)) Out[4]: <pre>3</pre> In\u00a0[5]: Copied! <pre>\" \".join(lcs_sequence(X, Y, len(X), len(Y)))\n</pre> \" \".join(lcs_sequence(X, Y, len(X), len(Y))) Out[5]: <pre>'police the gunman'</pre> In\u00a0[6]: Copied! <pre># Dynamic Programming LCS\ndef lcs_dp(X, Y, m, n, dp):\n \n    if m == 0 or n == 0:\n        return 0\n    elif dp[m][n] != -1:\n        return dp[m][n]\n    elif X[m - 1] == Y[n - 1]:\n        dp[m][n] = 1 + lcs_dp(X, Y, m - 1, n - 1, dp)\n        return dp[m][n]\n \n    dp[m][n] = max(lcs_dp(X, Y, m, n - 1, dp), lcs_dp(X, Y, m - 1, n, dp))\n    return dp[m][n]\n</pre> # Dynamic Programming LCS def lcs_dp(X, Y, m, n, dp):       if m == 0 or n == 0:         return 0     elif dp[m][n] != -1:         return dp[m][n]     elif X[m - 1] == Y[n - 1]:         dp[m][n] = 1 + lcs_dp(X, Y, m - 1, n - 1, dp)         return dp[m][n]       dp[m][n] = max(lcs_dp(X, Y, m, n - 1, dp), lcs_dp(X, Y, m - 1, n, dp))     return dp[m][n] In\u00a0[7]: Copied! <pre>dp = [[-1 for i in range(len(X) + 1)] for j in range(len(Y) + 1)]\nlcs_score = lcs_dp(X, Y, len(X), len(Y), dp)\nlcs_score\n</pre> dp = [[-1 for i in range(len(X) + 1)] for j in range(len(Y) + 1)] lcs_score = lcs_dp(X, Y, len(X), len(Y), dp) lcs_score Out[7]: <pre>3</pre> In\u00a0[8]: Copied! <pre>r_lcs = lcs_score/len(X)\np_lcs = lcs_score/len(Y)\n</pre> r_lcs = lcs_score/len(X) p_lcs = lcs_score/len(Y) In\u00a0[9]: Copied! <pre>r_lcs\n</pre> r_lcs Out[9]: <pre>0.75</pre> In\u00a0[10]: Copied! <pre>p_lcs\n</pre> p_lcs Out[10]: <pre>0.75</pre> In\u00a0[11]: Copied! <pre># Default beta, can be another number to weight between precision and recall\nbeta = p_lcs / r_lcs\nbeta\n</pre> # Default beta, can be another number to weight between precision and recall beta = p_lcs / r_lcs beta Out[11]: <pre>1.0</pre> In\u00a0[12]: Copied! <pre>num = (1 + (beta**2)) * r_lcs * p_lcs\ndenom = r_lcs + ((beta**2) * p_lcs)\nrouge_l = num / denom\n</pre> num = (1 + (beta**2)) * r_lcs * p_lcs denom = r_lcs + ((beta**2) * p_lcs) rouge_l = num / denom In\u00a0[13]: Copied! <pre>rouge_l\n</pre> rouge_l Out[13]: <pre>0.75</pre> In\u00a0[14]: Copied! <pre>def rouge_l(reference, candidate):\n    X = reference.split()\n    Y = candidate.split()\n    m = len(X)\n    n = len(Y)\n    if m == 0 or n == 0:\n        return 0\n    \n    dp = [[-1 for i in range(n + 1)]for j in range(m + 1)]\n    lcs_score = lcs_dp(X, Y, m, n, dp)\n    r_lcs = lcs_score/m\n    p_lcs = lcs_score/n\n    \n    epsilon = 1e-12 # Prevents division by 0\n    r_lcs = epsilon if r_lcs == 0 else r_lcs\n    beta = p_lcs / (r_lcs + epsilon)\n    num = (1 + (beta**2)) * r_lcs * p_lcs\n    denom = r_lcs + ((beta**2) * p_lcs)\n    denom = epsilon if denom == 0 else denom\n    return num / denom\n</pre> def rouge_l(reference, candidate):     X = reference.split()     Y = candidate.split()     m = len(X)     n = len(Y)     if m == 0 or n == 0:         return 0          dp = [[-1 for i in range(n + 1)]for j in range(m + 1)]     lcs_score = lcs_dp(X, Y, m, n, dp)     r_lcs = lcs_score/m     p_lcs = lcs_score/n          epsilon = 1e-12 # Prevents division by 0     r_lcs = epsilon if r_lcs == 0 else r_lcs     beta = p_lcs / (r_lcs + epsilon)     num = (1 + (beta**2)) * r_lcs * p_lcs     denom = r_lcs + ((beta**2) * p_lcs)     denom = epsilon if denom == 0 else denom     return num / denom In\u00a0[15]: Copied! <pre>rouge_l(reference, candidate)\n</pre> rouge_l(reference, candidate) Out[15]: <pre>0.75</pre> In\u00a0[16]: Copied! <pre>!pip install rouge-score\n</pre> !pip install rouge-score <pre>Requirement already satisfied: rouge-score in ./venv/lib/python3.9/site-packages (0.1.2)\nRequirement already satisfied: absl-py in ./venv/lib/python3.9/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in ./venv/lib/python3.9/site-packages (from rouge-score) (3.8.1)\nRequirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (from rouge-score) (1.25.2)\nRequirement already satisfied: six&gt;=1.14.0 in ./venv/lib/python3.9/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: regex&gt;=2021.8.3 in ./venv/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (2023.8.8)\nRequirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (4.66.1)\nRequirement already satisfied: joblib in ./venv/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (1.3.2)\nRequirement already satisfied: click in ./venv/lib/python3.9/site-packages (from nltk-&gt;rouge-score) (8.1.7)\n</pre> In\u00a0[17]: Copied! <pre>from rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n</pre> from rouge_score import rouge_scorer  scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False) In\u00a0[18]: Copied! <pre>scorer.score(reference, candidate)\n</pre> scorer.score(reference, candidate) Out[18]: <pre>{'rougeL': Score(precision=0.75, recall=0.75, fmeasure=0.75)}</pre> In\u00a0[19]: Copied! <pre>scorer.score('The quick brown fox jumps over the lazy dog',\n                      'The quick brown dog jumps on the log.')\n</pre> scorer.score('The quick brown fox jumps over the lazy dog',                       'The quick brown dog jumps on the log.') Out[19]: <pre>{'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}</pre>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_2_Summarization/#evaluate-summarization","title":"Evaluate Summarization\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_2_Summarization/#rouge-l","title":"ROUGE-L\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_2_Summarization/#lcs","title":"LCS\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_2_Summarization/#google-research-implementation","title":"Google Research Implementation\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/","title":"Evaluate Text Generation","text":"Author(s) Renato Leite (renatoleite@), Egon Soares (egon@) Last updated 10/22/2023 <p>https://dl.acm.org/doi/pdf/10.3115/1073083.1073135</p> <p>$BLEU = \\text{Brevity Penalty}\\times(\\exp(\\sum_{n=1}^{N}w_n\\log(\\text{modified precision}(n))))$</p> <p>$N = 4$ - This is the baseline used in the paper</p> <p>$w_n = 1 / N$ - This is for using uniform weights</p> <p>$\\text{Brevity Penalty} =   \\begin{cases}     1       &amp; \\quad \\text{if } c &gt; r\\\\     e^{(1-r/c)}  &amp; \\quad \\text{if } c \\leq r   \\end{cases}$</p> <p>$\\text{modified precision}(n) = \\cfrac{\\sum \\text{Count Clip}(n)}{\\sum \\text{Count n-gram}_{candidate}}$</p> <p>$\\text{Count Clip}(n) = min(\\text{Count n-gram}_{candidate}, max(\\text{Count n-gram}_{reference}))$</p> <p>https://cloud.google.com/translate/automl/docs/evaluate#bleu</p> <p>$\\text{BLEU} = \\underbrace{\\vphantom{\\prod_i^4}\\min\\Big(1,        \\exp\\big(1-\\frac{reference_{length}}     {candidate_{length}}\\big)\\Big)}_{\\text{brevity penalty}}  \\underbrace{\\Big(\\prod_{i=1}^{4}     precision_i\\Big)^{1/4}}_{\\text{n-gram overlap}}$</p> <p>$\\text{Brevity Penalty} = min(1, \\exp(1-\\cfrac{reference_{length}}{candidate_{length}}))$</p> <p>$\\text{n-gram overlap} = (\\displaystyle\\prod_{i=1}^{4} precision_i)^\\frac{1}{4}$</p> <p>$precision_i = \\dfrac{\\sum_{\\text{sentence}\\in\\text{Candidate-Corpus}}\\sum_{i\\in\\text{sentence}}\\min(m^i_{candidate}, m^i_{reference})}  {w_{total Candidate}^i = \\sum_{\\text{sentence'}\\in\\text{Candidate-Corpus}}\\sum_{i'\\in\\text{snt'}} m^{i'}_{candidate}}$</p> <p>$m_{candidate}^i$: is the count of i-gram in the candidate matching the reference</p> <p>$m_{reference}^i$: is the count of i-gram in the reference</p> <p>$w_{totalCandidate}^i$:     is the total number of i-grams in the candidate</p> <p>$\\text{Brevity Penalty} =   \\begin{cases}     1       &amp; \\quad \\text{if } c \\geq r\\\\     e^{(1-r/c)}  &amp; \\quad \\text{if } c &lt; r   \\end{cases}$</p> <p>$ c = length_{candidate}$, $r = length_{reference}$</p> <p>$\\text{Brevity Penalty} = min(1, \\exp(1-\\cfrac{reference_{length}}{candidate_{length}}))$</p> In\u00a0[1]: Copied! <pre>import math\n</pre> import math In\u00a0[2]: Copied! <pre>def calculate_brevity_penalty(reference_len: int, candidate_len: int) -&gt; float:\n    # Raise an error if any number is negative\n    if reference_len &lt; 0 or candidate_len &lt; 0:\n        raise ValueError(\"Length cannot be negative\")\n    # If the candidate length is greater than the reference length, r/c &lt; 1, exp(positive number) &gt; 1,  brevity penalty = 1\n    if candidate_len &gt; reference_len:\n        print(f\"Candidate length \\t ({candidate_len}) \\t is greater than the reference length \\t ({reference_len}), \\t so the Brevity Penalty is equal to \\t 1.000\")\n        return 1.0\n    # If the lengths are equal, then r/c = 1, and exp(0) = 1\n    if candidate_len == reference_len:\n        print(f\"Candidate length \\t ({candidate_len}) \\t is equal to the reference length \\t ({reference_len}), \\t so the Brevity Penalty is equal to \\t 1.000\")\n        return 1.0\n    # If candidate is empty, brevity penalty = 0, because r/0 -&gt; inf and exp(-inf) -&gt; 0\n    if candidate_len == 0:\n        print(f\"Candidate length \\t ({candidate_len}) \\t is equal to 0.0, \\t\\t\\t\\t so the Brevity Penalty is equal to \\t 0.000\")\n        return 0.0\n\n    # If the candidate length is less than the reference length, brevity penalty = exp(1-r/c)\n    print(f\"Candidate length \\t ({candidate_len}) \\t is less than the reference length \\t ({reference_len}),\\t so the Brevity Penalty is equal to \\t {math.exp(1 - reference_len / candidate_len):.3f}\")\n    return math.exp(1 - reference_len / candidate_len)\n</pre> def calculate_brevity_penalty(reference_len: int, candidate_len: int) -&gt; float:     # Raise an error if any number is negative     if reference_len &lt; 0 or candidate_len &lt; 0:         raise ValueError(\"Length cannot be negative\")     # If the candidate length is greater than the reference length, r/c &lt; 1, exp(positive number) &gt; 1,  brevity penalty = 1     if candidate_len &gt; reference_len:         print(f\"Candidate length \\t ({candidate_len}) \\t is greater than the reference length \\t ({reference_len}), \\t so the Brevity Penalty is equal to \\t 1.000\")         return 1.0     # If the lengths are equal, then r/c = 1, and exp(0) = 1     if candidate_len == reference_len:         print(f\"Candidate length \\t ({candidate_len}) \\t is equal to the reference length \\t ({reference_len}), \\t so the Brevity Penalty is equal to \\t 1.000\")         return 1.0     # If candidate is empty, brevity penalty = 0, because r/0 -&gt; inf and exp(-inf) -&gt; 0     if candidate_len == 0:         print(f\"Candidate length \\t ({candidate_len}) \\t is equal to 0.0, \\t\\t\\t\\t so the Brevity Penalty is equal to \\t 0.000\")         return 0.0      # If the candidate length is less than the reference length, brevity penalty = exp(1-r/c)     print(f\"Candidate length \\t ({candidate_len}) \\t is less than the reference length \\t ({reference_len}),\\t so the Brevity Penalty is equal to \\t {math.exp(1 - reference_len / candidate_len):.3f}\")     return math.exp(1 - reference_len / candidate_len) In\u00a0[3]: Copied! <pre>def calculate_brevity_penalty_2(reference_len: int, candidate_len: int) -&gt; float:\n    # Raise an error if any number is negative\n    if reference_len &lt; 0 or candidate_len &lt; 0:\n        raise ValueError(\"Length cannot be negative\")\n    # Avoid a division by 0\n    if candidate_len == 0:\n        if reference_len == 0:\n            return 1.0\n        else:\n            return 0.0 \n    return min(1.0, math.exp(1 - reference_len / (candidate_len)))\n</pre> def calculate_brevity_penalty_2(reference_len: int, candidate_len: int) -&gt; float:     # Raise an error if any number is negative     if reference_len &lt; 0 or candidate_len &lt; 0:         raise ValueError(\"Length cannot be negative\")     # Avoid a division by 0     if candidate_len == 0:         if reference_len == 0:             return 1.0         else:             return 0.0      return min(1.0, math.exp(1 - reference_len / (candidate_len))) In\u00a0[4]: Copied! <pre>candidates = [\"It is a guide to action which ensures that the military always obeys the commands of the party.\",\n              \"It is to insure the troops forever hearing the activity guidebook that party direct.\",\n              \"\"]\n</pre> candidates = [\"It is a guide to action which ensures that the military always obeys the commands of the party.\",               \"It is to insure the troops forever hearing the activity guidebook that party direct.\",               \"\"] In\u00a0[5]: Copied! <pre>references = [\"It is a guide to action that ensures that the military will forever heed Party commands.\",\n              \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\",\n              \"It is the practical guide for the army always to heed the directions of the party.\"]\n</pre> references = [\"It is a guide to action that ensures that the military will forever heed Party commands.\",               \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\",               \"It is the practical guide for the army always to heed the directions of the party.\"] In\u00a0[6]: Copied! <pre>from itertools import product\n</pre> from itertools import product In\u00a0[7]: Copied! <pre>bp1 = [calculate_brevity_penalty(len(reference), len(candidate)) for reference, candidate in product(references, candidates)]\n</pre> bp1 = [calculate_brevity_penalty(len(reference), len(candidate)) for reference, candidate in product(references, candidates)] <pre>Candidate length \t (95) \t is greater than the reference length \t (88), \t so the Brevity Penalty is equal to \t 1.000\nCandidate length \t (84) \t is less than the reference length \t (88),\t so the Brevity Penalty is equal to \t 0.953\nCandidate length \t (0) \t is equal to 0.0, \t\t\t\t so the Brevity Penalty is equal to \t 0.000\nCandidate length \t (95) \t is less than the reference length \t (109),\t so the Brevity Penalty is equal to \t 0.863\nCandidate length \t (84) \t is less than the reference length \t (109),\t so the Brevity Penalty is equal to \t 0.743\nCandidate length \t (0) \t is equal to 0.0, \t\t\t\t so the Brevity Penalty is equal to \t 0.000\nCandidate length \t (95) \t is greater than the reference length \t (82), \t so the Brevity Penalty is equal to \t 1.000\nCandidate length \t (84) \t is greater than the reference length \t (82), \t so the Brevity Penalty is equal to \t 1.000\nCandidate length \t (0) \t is equal to 0.0, \t\t\t\t so the Brevity Penalty is equal to \t 0.000\n</pre> In\u00a0[8]: Copied! <pre>bp_2 = [calculate_brevity_penalty_2(len(reference), len(candidate)) for reference, candidate in product(references, candidates)]\n</pre> bp_2 = [calculate_brevity_penalty_2(len(reference), len(candidate)) for reference, candidate in product(references, candidates)] In\u00a0[9]: Copied! <pre>bp1 == bp_2\n</pre> bp1 == bp_2 Out[9]: <pre>True</pre> <p>$\\text{modified precision}(n) = \\cfrac{\\sum \\text{Count Clip}(n)}{\\sum \\text{Count n-gram}_{candidate}}$</p> <p>$\\text{Count Clip}(n) = min(\\text{Count n-gram}_{candidate}, max(\\text{Count n-gram}_{reference}))$</p> In\u00a0[10]: Copied! <pre>from collections import Counter\nfrom fractions import Fraction\nfrom itertools import tee\n\n\ndef ngrams(sequence, n):\n    # Creates the sliding window, of n no. of items.\n    # `iterables` is a tuple of iterables where each iterable is a window of n items.\n    iterables = tee(iter(sequence), n)\n\n    for i, sub_iterable in enumerate(iterables):  # For each window,\n        for _ in range(i):  # iterate through every order of ngrams\n            next(sub_iterable, None)  # generate the ngrams within the window.\n    return zip(*iterables)  # Unpack and flattens the iterables.\n\n\ndef count_clip(counts: Counter, max_counts: dict) -&gt; dict:\n    clipped_counts = {}\n    for ngram, count in counts.items():\n        clipped_count = min(count, max_counts[ngram])\n        clipped_counts[ngram] = clipped_count\n\n    return clipped_counts\n        \n\ndef calculate_modified_precision(references, candidate, n):\n    candidate = candidate.split()\n    candidate_counts = Counter(ngrams(candidate, n)) if len(candidate) &gt;= n else Counter()\n    \n    max_counts = {}\n    for ref in references:\n        reference = ref.split()\n        reference_counts = (\n            Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()\n        )\n        for ngram in candidate_counts:\n            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n    clipped_counts = count_clip(candidate_counts, max_counts)\n    numerator = sum(clipped_counts.values())\n    \n    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n    denominator = max(1, sum(candidate_counts.values()))\n\n    return Fraction(numerator, denominator, _normalize=False)\n</pre> from collections import Counter from fractions import Fraction from itertools import tee   def ngrams(sequence, n):     # Creates the sliding window, of n no. of items.     # `iterables` is a tuple of iterables where each iterable is a window of n items.     iterables = tee(iter(sequence), n)      for i, sub_iterable in enumerate(iterables):  # For each window,         for _ in range(i):  # iterate through every order of ngrams             next(sub_iterable, None)  # generate the ngrams within the window.     return zip(*iterables)  # Unpack and flattens the iterables.   def count_clip(counts: Counter, max_counts: dict) -&gt; dict:     clipped_counts = {}     for ngram, count in counts.items():         clipped_count = min(count, max_counts[ngram])         clipped_counts[ngram] = clipped_count      return clipped_counts           def calculate_modified_precision(references, candidate, n):     candidate = candidate.split()     candidate_counts = Counter(ngrams(candidate, n)) if len(candidate) &gt;= n else Counter()          max_counts = {}     for ref in references:         reference = ref.split()         reference_counts = (             Counter(ngrams(reference, n)) if len(reference) &gt;= n else Counter()         )         for ngram in candidate_counts:             max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])      clipped_counts = count_clip(candidate_counts, max_counts)     numerator = sum(clipped_counts.values())          # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.     denominator = max(1, sum(candidate_counts.values()))      return Fraction(numerator, denominator, _normalize=False) In\u00a0[11]: Copied! <pre>print(\"References\\n\")\n_ = [print(reference) for reference in references]\n</pre> print(\"References\\n\") _ = [print(reference) for reference in references] <pre>References\n\nIt is a guide to action that ensures that the military will forever heed Party commands.\nIt is the guiding principle which guarantees the military forces always being under the command of the Party.\nIt is the practical guide for the army always to heed the directions of the party.\n</pre> In\u00a0[12]: Copied! <pre>print(\"Candidates\\n\")\n_ = [print(f\"Candidate {i} is '{candidate}'\") for i, candidate in enumerate(candidates)]\n</pre> print(\"Candidates\\n\") _ = [print(f\"Candidate {i} is '{candidate}'\") for i, candidate in enumerate(candidates)] <pre>Candidates\n\nCandidate 0 is 'It is a guide to action which ensures that the military always obeys the commands of the party.'\nCandidate 1 is 'It is to insure the troops forever hearing the activity guidebook that party direct.'\nCandidate 2 is ''\n</pre> In\u00a0[13]: Copied! <pre>[f\"The {j+1}-gram modified precision for candidate {i} is {calculate_modified_precision(references, candidate, j+1)}\" for i, candidate in enumerate(candidates) for j in range(4)]\n</pre> [f\"The {j+1}-gram modified precision for candidate {i} is {calculate_modified_precision(references, candidate, j+1)}\" for i, candidate in enumerate(candidates) for j in range(4)] Out[13]: <pre>['The 1-gram modified precision for candidate 0 is 16/18',\n 'The 2-gram modified precision for candidate 0 is 10/17',\n 'The 3-gram modified precision for candidate 0 is 7/16',\n 'The 4-gram modified precision for candidate 0 is 4/15',\n 'The 1-gram modified precision for candidate 1 is 7/14',\n 'The 2-gram modified precision for candidate 1 is 1/13',\n 'The 3-gram modified precision for candidate 1 is 0/12',\n 'The 4-gram modified precision for candidate 1 is 0/11',\n 'The 1-gram modified precision for candidate 2 is 0',\n 'The 2-gram modified precision for candidate 2 is 0',\n 'The 3-gram modified precision for candidate 2 is 0',\n 'The 4-gram modified precision for candidate 2 is 0']</pre> <p>$\\text{n-gram overlap} = \\exp(\\sum_{n=1}^{N}w_n\\log(\\text{modified precision}(n)))$</p> In\u00a0[14]: Copied! <pre>def calculate_n_gram_overlap(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n\n    # compute modified precision for 1-4 ngrams\n    modified_precision_numerators = Counter()  \n    modified_precision_denominators = Counter()  \n    candidate_lengths, reference_lengths = 0, 0\n\n    for i, _ in enumerate(weights, start=1):\n        modified_precision_i = calculate_modified_precision(references, candidate, i)\n        modified_precision_numerators[i] += modified_precision_i.numerator\n        modified_precision_denominators[i] += modified_precision_i.denominator\n\n    # remove zero precision\n    modified_precision_n = [\n        Fraction(modified_precision_numerators[i], modified_precision_denominators[i], \n        _normalize=False)\n        for i, _ in enumerate(weights, start=1)\n        if modified_precision_numerators[i] &gt; 0\n    ]\n    weighted_precisions = (weight_i * math.log(precision_i) for weight_i, precision_i in zip(weights, modified_precision_n))\n    precisions_sum = math.fsum(weighted_precisions)\n\n    return math.exp(precisions_sum)\n\ndef bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):  \n    candidate_len = len(candidate.split())\n    references_lens = (len(reference.split()) for reference in references)\n\n    # Reference length closest to the candidate length\n    closest_reference_len = min(\n        references_lens, key=lambda reference_len: (abs(reference_len - candidate_len), reference_len)\n    )\n    brevity_penalty = calculate_brevity_penalty_2(closest_reference_len, candidate_len)\n    n_gram_overlap = calculate_n_gram_overlap(references, candidate, weights)\n    \n    return brevity_penalty * n_gram_overlap\n    \n</pre> def calculate_n_gram_overlap(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):      # compute modified precision for 1-4 ngrams     modified_precision_numerators = Counter()       modified_precision_denominators = Counter()       candidate_lengths, reference_lengths = 0, 0      for i, _ in enumerate(weights, start=1):         modified_precision_i = calculate_modified_precision(references, candidate, i)         modified_precision_numerators[i] += modified_precision_i.numerator         modified_precision_denominators[i] += modified_precision_i.denominator      # remove zero precision     modified_precision_n = [         Fraction(modified_precision_numerators[i], modified_precision_denominators[i],          _normalize=False)         for i, _ in enumerate(weights, start=1)         if modified_precision_numerators[i] &gt; 0     ]     weighted_precisions = (weight_i * math.log(precision_i) for weight_i, precision_i in zip(weights, modified_precision_n))     precisions_sum = math.fsum(weighted_precisions)      return math.exp(precisions_sum)  def bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):       candidate_len = len(candidate.split())     references_lens = (len(reference.split()) for reference in references)      # Reference length closest to the candidate length     closest_reference_len = min(         references_lens, key=lambda reference_len: (abs(reference_len - candidate_len), reference_len)     )     brevity_penalty = calculate_brevity_penalty_2(closest_reference_len, candidate_len)     n_gram_overlap = calculate_n_gram_overlap(references, candidate, weights)          return brevity_penalty * n_gram_overlap      <p>$BLEU = \\text{Brevity Penalty}\\times\\text{n-gram overlap}$</p> In\u00a0[15]: Copied! <pre>def bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):  \n    candidate_len = len(candidate.split())\n    references_lens = (len(reference.split()) for reference in references)\n\n    # Reference length closest to the candidate length\n    closest_reference_len = min(\n        references_lens, key=lambda reference_len: (abs(reference_len - candidate_len), reference_len)\n    )\n    brevity_penalty = calculate_brevity_penalty_2(closest_reference_len, candidate_len)\n    n_gram_overlap = calculate_n_gram_overlap(references, candidate, weights)\n    \n    return brevity_penalty * n_gram_overlap\n</pre> def bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):       candidate_len = len(candidate.split())     references_lens = (len(reference.split()) for reference in references)      # Reference length closest to the candidate length     closest_reference_len = min(         references_lens, key=lambda reference_len: (abs(reference_len - candidate_len), reference_len)     )     brevity_penalty = calculate_brevity_penalty_2(closest_reference_len, candidate_len)     n_gram_overlap = calculate_n_gram_overlap(references, candidate, weights)          return brevity_penalty * n_gram_overlap In\u00a0[16]: Copied! <pre>bleu(references, candidates[0])\n</pre> bleu(references, candidates[0]) Out[16]: <pre>0.4969770530031034</pre> In\u00a0[17]: Copied! <pre>!pip install -U nltk\n</pre> !pip install -U nltk <pre>Requirement already satisfied: nltk in ./venv/lib/python3.9/site-packages (3.8.1)\nCollecting nltk\n  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n  Using cached nltk-3.8-py3-none-any.whl (1.5 MB)\nRequirement already satisfied: click in ./venv/lib/python3.9/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (from nltk) (4.66.1)\nRequirement already satisfied: joblib in ./venv/lib/python3.9/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in ./venv/lib/python3.9/site-packages (from nltk) (2023.8.8)\n</pre> In\u00a0[18]: Copied! <pre>from nltk.translate.bleu_score import sentence_bleu\n\nnltk_bleu_score = sentence_bleu([reference.split() for reference in references], candidates[0].split())\nprint(nltk_bleu_score)\n</pre> from nltk.translate.bleu_score import sentence_bleu  nltk_bleu_score = sentence_bleu([reference.split() for reference in references], candidates[0].split()) print(nltk_bleu_score) <pre>0.4969770530031034\n</pre> <p>See Theory_Evaluate_2_Summarization.ipynb</p>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#evaluate-text-generation","title":"Evaluate Text Generation\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#bleu","title":"BLEU\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#explanations","title":"Explanations\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#original-paper","title":"Original paper\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#alternative-explanation","title":"Alternative explanation\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#brevity-penalty","title":"Brevity Penalty\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#precision","title":"Precision\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#n-gram-overlap","title":"n-gram overlap\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#bleu","title":"BLEU\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#nltk-implementation","title":"NLTK Implementation\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_3_Text_Generation/#rouge-l","title":"ROUGE-L\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_4_QandA/","title":"Evaluate Q&amp;A","text":"Author(s) Renato Leite (renatoleite@), Egon Soares (egon@) Last updated 09/05/2023 <p>$ EM(Truth, Prediction) =   \\begin{cases}     0       &amp; \\quad \\text{if } Truth \\neq Prediction\\\\     1  &amp; \\quad \\text{if } Truth = Prediction   \\end{cases} $</p> In\u00a0[1]: Copied! <pre>def exact_match(ground_truth:str , answer:str) -&gt; int:\n    return 1 if ground_truth == answer else 0\n</pre> def exact_match(ground_truth:str , answer:str) -&gt; int:     return 1 if ground_truth == answer else 0 In\u00a0[2]: Copied! <pre>ground_truth_1 = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\"\nanswer_1_a = \"\"\nanswer_1_b = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\"\nanswer_1_c = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin.\"\n</pre> ground_truth_1 = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\" answer_1_a = \"\" answer_1_b = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\" answer_1_c = \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin.\" In\u00a0[3]: Copied! <pre>print(f\"Ground Truth: {ground_truth_1}\")\nprint(f\"\\nAnswer a: {answer_1_a}\\nEM: {exact_match(ground_truth_1, answer_1_a)}\")\nprint(f\"\\nAnswer b: {answer_1_b}\\nEM: {exact_match(ground_truth_1, answer_1_b)}\")\nprint(f\"\\nAnswer c: {answer_1_c}\\nEM: {exact_match(ground_truth_1, answer_1_c)}\")\n</pre> print(f\"Ground Truth: {ground_truth_1}\") print(f\"\\nAnswer a: {answer_1_a}\\nEM: {exact_match(ground_truth_1, answer_1_a)}\") print(f\"\\nAnswer b: {answer_1_b}\\nEM: {exact_match(ground_truth_1, answer_1_b)}\") print(f\"\\nAnswer c: {answer_1_c}\\nEM: {exact_match(ground_truth_1, answer_1_c)}\") <pre>Ground Truth: Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\n\nAnswer a: \nEM: 0\n\nAnswer b: Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\nEM: 1\n\nAnswer c: Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin.\nEM: 0\n</pre> In\u00a0[4]: Copied! <pre>ground_truth_2 = \"\"\nanswer_2_a = \"\"\nanswer_2_b = \"Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter.\"\nanswer_2_c = \"No answer found.\"\n</pre> ground_truth_2 = \"\" answer_2_a = \"\" answer_2_b = \"Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter.\" answer_2_c = \"No answer found.\" In\u00a0[5]: Copied! <pre>print(f\"Ground Truth: {ground_truth_2}\")\nprint(f\"\\nAnswer a: {answer_2_a}\\nEM: {exact_match(ground_truth_2, answer_2_a)}\")\nprint(f\"\\nAnswer b: {answer_2_b}\\nEM: {exact_match(ground_truth_2, answer_2_b)}\")\nprint(f\"\\nAnswer c: {answer_2_c}\\nEM: {exact_match(ground_truth_2, answer_2_c)}\")\n</pre> print(f\"Ground Truth: {ground_truth_2}\") print(f\"\\nAnswer a: {answer_2_a}\\nEM: {exact_match(ground_truth_2, answer_2_a)}\") print(f\"\\nAnswer b: {answer_2_b}\\nEM: {exact_match(ground_truth_2, answer_2_b)}\") print(f\"\\nAnswer c: {answer_2_c}\\nEM: {exact_match(ground_truth_2, answer_2_c)}\") <pre>Ground Truth: \n\nAnswer a: \nEM: 1\n\nAnswer b: Google and YouTube are the two most visited websites worldwide followed by Facebook and Twitter.\nEM: 0\n\nAnswer c: No answer found.\nEM: 0\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_4_QandA/#evaluate-qa","title":"Evaluate Q&amp;A\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_4_QandA/#exact-match","title":"Exact Match\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_4_QandA/#case-1-there-is-an-answer","title":"Case 1 - There is an answer\u00b6","text":""},{"location":"genai-on-vertex-ai/vertex_evaluation_services/theory/Theory_Evaluate_4_QandA/#case-2-there-is-no-answer","title":"Case 2 - There is no answer\u00b6","text":""},{"location":"research-operationalization/","title":"Research Operationalization","text":"<p>This folder contains code samples and hands-on labs demonstrating the operationalization of latest research models or frameworks from Google DeepMind and Research teams on Google Cloud including Vertex AI.</p> <ul> <li>TimesFM - Time-Series Foundation Model: This folders illustrates the operationalization of TimesFM model in a generative AI application, in the context of a retail merchant analyzing performance of a particular item/product.</li> </ul> <p>A few other repositories you may find interesting:</p> <ul> <li> <p>Developing NLP solutions with T5X and Vertex AI: This repository compiles prescriptive guidance and code samples that show how to operationalize the Google Research T5X framework using Google Cloud Vertex AI. Using T5X with Vertex AI enables streamlined experimentation, development, and deployment of natural language processing (NLP) solutions at scale.</p> </li> <li> <p>AlphaFold batch inference with Vertex AI Pipelines: This repository compiles prescriptive guidance and code samples demonstrating how to operationalize AlphaFold batch inference using Vertex AI Pipelines.</p> </li> </ul>"},{"location":"research-operationalization/timesfm/","title":"TimesFM - Foundation Model for Time-Series Forecasting","text":"<ul> <li> <p>Operationalizing TimesFM on Vertex AI: This notebook shows how to operationalize TimesFM model on Vertex AI in the context of complementing a Vertex AI Gemini based Generative AI application with predictive open source models such as TimesFM. This notebook was demonstrated as part of Google IO 2024 talk. We recommend to watch the talk to get familiarized with concepts presented in this notebook.</p> <p></p> </li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/","title":"Operationalizing TimesFM on Vertex AI","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n</pre> # Copyright 2024 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. Author(s) Rajesh Thallam, Skander Hannachi Video An LLM journey speed run: Hugging Face to Vertex AI <ul> <li>Write requirements.txt file</li> </ul> In\u00a0[\u00a0]: Copied! <pre>PATH_TO_REQUIREMENTS_TXT = \"requirements.txt\"\n</pre> PATH_TO_REQUIREMENTS_TXT = \"requirements.txt\" In\u00a0[\u00a0]: Copied! <pre>%%writefile $PATH_TO_REQUIREMENTS_TXT\ngoogle-cloud-storage\ngoogle-cloud-secret-manager\ngoogle-cloud-bigquery\ngoogle-cloud-bigquery-storage\ngoogle-cloud-secret-manager\ngoogle-cloud-aiplatform\ngoogle-cloud-aiplatform[prediction]&gt;=1.16.0\nkaggle\npandas\ndb-dtypes\nnumpy\nmatplotlib\nlangchain==0.1.20\nlangchainhub==0.1.15\nlangchain-google-vertexai==1.0.3\ncloudpickle==3.0.0\npydantic==2.7.1\nprotobuf==3.19.6\n</pre> %%writefile $PATH_TO_REQUIREMENTS_TXT google-cloud-storage google-cloud-secret-manager google-cloud-bigquery google-cloud-bigquery-storage google-cloud-secret-manager google-cloud-aiplatform google-cloud-aiplatform[prediction]&gt;=1.16.0 kaggle pandas db-dtypes numpy matplotlib langchain==0.1.20 langchainhub==0.1.15 langchain-google-vertexai==1.0.3 cloudpickle==3.0.0 pydantic==2.7.1 protobuf==3.19.6 In\u00a0[\u00a0]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    USER_FLAG = \"\"\nelse:\n    USER_FLAG = \"--user\"\n</pre> import sys  if \"google.colab\" in sys.modules:     USER_FLAG = \"\" else:     USER_FLAG = \"--user\" In\u00a0[\u00a0]: Copied! <pre>! pip install $USER_FLAG -r $PATH_TO_REQUIREMENTS_TXT -q --no-warn-conflicts\n</pre> ! pip install $USER_FLAG -r $PATH_TO_REQUIREMENTS_TXT -q --no-warn-conflicts In\u00a0[\u00a0]: Copied! <pre>import IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n</pre> import IPython  app = IPython.Application.instance() app.kernel.do_shutdown(True) \u26a0\ufe0f The kernel is going to restart. Please wait until it is finished before continuing to the next step. \u26a0\ufe0f In\u00a0[\u00a0]: Copied! <pre># Colab authentication.\nimport sys\n\nif \"google.colab\" in sys.modules:\n    from google.colab import auth\n\n    auth.authenticate_user()\n    print(\"Authenticated\")\n</pre> # Colab authentication. import sys  if \"google.colab\" in sys.modules:     from google.colab import auth      auth.authenticate_user()     print(\"Authenticated\") In\u00a0[\u00a0]: Copied! <pre># Define variables\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nLOCATION = \"us-central1\"  # @param {type:\"string\"}\nSTAGING_BUCKET = \"[your-bucket-name]\"  # @param {type:\"string\"}\nSTAGING_BUCKET_URI = f\"gs://{STAGING_BUCKET}\"\n</pre> # Define variables PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"} LOCATION = \"us-central1\"  # @param {type:\"string\"} STAGING_BUCKET = \"[your-bucket-name]\"  # @param {type:\"string\"} STAGING_BUCKET_URI = f\"gs://{STAGING_BUCKET}\" In\u00a0[\u00a0]: Copied! <pre># Enable required APIs\n! gcloud services enable \\\n    iam.googleapis.com \\\n    storage-component.googleapis.com \\\n    compute.googleapis.com \\\n    aiplatform.googleapis.com \\\n    bigquery.googleapis.com \\\n    secretmanager.googleapis.com \\\n    cloudresourcemanager.googleapis.com \\\n    --project $PROJECT_ID\n</pre> # Enable required APIs ! gcloud services enable \\     iam.googleapis.com \\     storage-component.googleapis.com \\     compute.googleapis.com \\     aiplatform.googleapis.com \\     bigquery.googleapis.com \\     secretmanager.googleapis.com \\     cloudresourcemanager.googleapis.com \\     --project $PROJECT_ID In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\n\nimport vertexai\n\nvertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)\n\nprint(\"Vertex AI SDK initialized.\")\nprint(f\"Vertex AI SDK version = {vertexai.__version__}\")\n</pre> import os import sys  import vertexai  vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET_URI)  print(\"Vertex AI SDK initialized.\") print(f\"Vertex AI SDK version = {vertexai.__version__}\") In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\n\nprint(f\"Using this region: {LOCATION}\")\n\nnow = datetime.now().strftime(\"%Y%m%d%H%M%S\")\nassert STAGING_BUCKET_URI.startswith(\n    \"gs://\"\n), \"STAGING_BUCKET_URI must start with `gs://`.\"\n\n# Create a unique GCS bucket for this notebook, if not specified by the user\nif (\n    STAGING_BUCKET_URI is None\n    or STAGING_BUCKET_URI.strip() == \"\"\n    or STAGING_BUCKET_URI == \"gs://\"\n):\n    STAGING_BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n    ! gsutil mb -l {REGION} {STAGING_BUCKET_URI}\nelse:\n    STAGING_BUCKET_NAME = \"/\".join(STAGING_BUCKET_URI.split(\"/\")[:3])\n    shell_output = ! gsutil ls -Lb {STAGING_BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n    bucket_region = shell_output[0].strip().lower()\n    if not LOCATION.startswith(bucket_region):\n        raise ValueError(\n            f\"Bucket region {bucket_region} is different from notebook region\"\n            f\" {LOCATION}\"\n        )\nprint(f\"Using this GCS Bucket: {STAGING_BUCKET_URI}\")\n</pre> from datetime import datetime  print(f\"Using this region: {LOCATION}\")  now = datetime.now().strftime(\"%Y%m%d%H%M%S\") assert STAGING_BUCKET_URI.startswith(     \"gs://\" ), \"STAGING_BUCKET_URI must start with `gs://`.\"  # Create a unique GCS bucket for this notebook, if not specified by the user if (     STAGING_BUCKET_URI is None     or STAGING_BUCKET_URI.strip() == \"\"     or STAGING_BUCKET_URI == \"gs://\" ):     STAGING_BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"     ! gsutil mb -l {REGION} {STAGING_BUCKET_URI} else:     STAGING_BUCKET_NAME = \"/\".join(STAGING_BUCKET_URI.split(\"/\")[:3])     shell_output = ! gsutil ls -Lb {STAGING_BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"     bucket_region = shell_output[0].strip().lower()     if not LOCATION.startswith(bucket_region):         raise ValueError(             f\"Bucket region {bucket_region} is different from notebook region\"             f\" {LOCATION}\"         ) print(f\"Using this GCS Bucket: {STAGING_BUCKET_URI}\") <p>Option #1. Use Google Cloud Secrets Manager</p> <p>Follow this step-by-step guide to add Kaggle API key as secrets using Cloud Secret Manager. Use following names as secret ids.</p> <ul> <li><code>KAGGLE_KEY</code></li> </ul> <p>The following code fetches secret versions and sets appropriate environment variables for rest of the notebook to work.</p> In\u00a0[\u00a0]: Copied! <pre>from google.cloud import secretmanager\n\n\nclass SecretManager:\n    def __init__(self, project_id: str):\n        self.project_id = project_id\n        self._client = secretmanager.SecretManagerServiceClient()\n\n    def get_secret(self, secret_id: str):\n        name = self._client.secret_version_path(self.project_id, secret_id, \"latest\")\n        response = self._client.access_secret_version(name=name)\n        return response.payload.data.decode(\"UTF-8\")\n\n\nsm = SecretManager(project_id=PROJECT_ID)\nos.environ[\"KAGGLE_USERNAME\"] = sm.get_secret(\"KAGGLE_USERNAME\")\nos.environ[\"KAGGLE_KEY\"] = sm.get_secret(\"KAGGLE_KEY\")\n</pre> from google.cloud import secretmanager   class SecretManager:     def __init__(self, project_id: str):         self.project_id = project_id         self._client = secretmanager.SecretManagerServiceClient()      def get_secret(self, secret_id: str):         name = self._client.secret_version_path(self.project_id, secret_id, \"latest\")         response = self._client.access_secret_version(name=name)         return response.payload.data.decode(\"UTF-8\")   sm = SecretManager(project_id=PROJECT_ID) os.environ[\"KAGGLE_USERNAME\"] = sm.get_secret(\"KAGGLE_USERNAME\") os.environ[\"KAGGLE_KEY\"] = sm.get_secret(\"KAGGLE_KEY\") <p>Option #2. Using Colab Secrets</p> <p>You can safely store your private keys, such as your kaggle API tokens, in Colab Secrets. Values stored in Secrets are private, visible only to you and the notebooks you select.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>if \"google.colab\" in sys.modules:\n    from google.colab import userdata\n\n    os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n    os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n</pre> if \"google.colab\" in sys.modules:     from google.colab import userdata      os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")     os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\") <p>Option #3. Use Python configuration file</p> <p>Add Kaggle API key to the configuration file.</p> <p>DO NOT commit configuration file to GitHub repository.</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile config.ini\n[kaggle]\nKAGGLE_USERNAME = xxxxxxx # REPLACE WITH KAGGLE USERNAME\nKAGGLE_KEY = xxxxxxx # REPLACE WITH KAGGLE API KEY\n</pre> %%writefile config.ini [kaggle] KAGGLE_USERNAME = xxxxxxx # REPLACE WITH KAGGLE USERNAME KAGGLE_KEY = xxxxxxx # REPLACE WITH KAGGLE API KEY In\u00a0[\u00a0]: Copied! <pre>import configparser\n\n# read configuration file and set env variables\nconfig = configparser.ConfigParser()\nconfig.read(\"config.ini\")\n\nos.environ[\"KAGGLE_USERNAME\"] = config[\"kaggle\"][\"KAGGLE_USERNAME\"]\nos.environ[\"KAGGLE_KEY\"] = config[\"kaggle\"][\"KAGGLE_KEY\"]\n</pre> import configparser  # read configuration file and set env variables config = configparser.ConfigParser() config.read(\"config.ini\")  os.environ[\"KAGGLE_USERNAME\"] = config[\"kaggle\"][\"KAGGLE_USERNAME\"] os.environ[\"KAGGLE_KEY\"] = config[\"kaggle\"][\"KAGGLE_KEY\"] <p>The notebook is divided into sections as shown:</p> <p></p> <ul> <li>Configure variables</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Kaggle dataset\nKAGGLE_DATASET = \"thedevastator/unlock-profits-with-e-commerce-sales-data\"\n\n# paths for managing data locally and Cloud Storage bucket\nLOCAL_DATA_PATH = \"data\"\nGCS_DATA_PATH = f\"{STAGING_BUCKET_URI}/googleio24/data/amazon_sale_report.csv\"\n\n# BigQuery datasets\nBQ_DATASET_ID = \"[your-bq-dataset-id]\"  # @param {type:\"string\"}\nBQ_LOCATION = \"US\"\nBQ_TABLE_SALES_RAW = \"sales_raw\"\nBQ_TABLE_SALES_DAILY = \"sales_daily\"\n</pre> # Kaggle dataset KAGGLE_DATASET = \"thedevastator/unlock-profits-with-e-commerce-sales-data\"  # paths for managing data locally and Cloud Storage bucket LOCAL_DATA_PATH = \"data\" GCS_DATA_PATH = f\"{STAGING_BUCKET_URI}/googleio24/data/amazon_sale_report.csv\"  # BigQuery datasets BQ_DATASET_ID = \"[your-bq-dataset-id]\"  # @param {type:\"string\"} BQ_LOCATION = \"US\" BQ_TABLE_SALES_RAW = \"sales_raw\" BQ_TABLE_SALES_DAILY = \"sales_daily\" <ul> <li>Create local directory</li> </ul> In\u00a0[\u00a0]: Copied! <pre>! mkdir -p $LOCAL_DATA_PATH\n</pre> ! mkdir -p $LOCAL_DATA_PATH <ul> <li>Authenticate with Kaggle API</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from kaggle.api.kaggle_api_extended import KaggleApi\n\nkgl_api = KaggleApi()\nkgl_api.authenticate()\n</pre> from kaggle.api.kaggle_api_extended import KaggleApi  kgl_api = KaggleApi() kgl_api.authenticate() In\u00a0[\u00a0]: Copied! <pre>from google.cloud import bigquery\n\nbq_client = bigquery.Client(project=PROJECT_ID)\n</pre> from google.cloud import bigquery  bq_client = bigquery.Client(project=PROJECT_ID) <ul> <li>List files within Kaggle dataset</li> </ul> In\u00a0[\u00a0]: Copied! <pre>kgl_api.dataset_list_files(KAGGLE_DATASET).files\n</pre> kgl_api.dataset_list_files(KAGGLE_DATASET).files <ul> <li>Download specific file from the Kaggle dataset</li> </ul> In\u00a0[\u00a0]: Copied! <pre>kgl_api.dataset_download_file(\n    KAGGLE_DATASET, file_name=\"Amazon Sale Report.csv\", path=LOCAL_DATA_PATH\n)\n\n# unzip file\n! unzip -f $LOCAL_DATA_PATH/*.zip -d $LOCAL_DATA_PATH &amp;&amp; ls -ltr $LOCAL_DATA_PATH\n</pre> kgl_api.dataset_download_file(     KAGGLE_DATASET, file_name=\"Amazon Sale Report.csv\", path=LOCAL_DATA_PATH )  # unzip file ! unzip -f $LOCAL_DATA_PATH/*.zip -d $LOCAL_DATA_PATH &amp;&amp; ls -ltr $LOCAL_DATA_PATH <ul> <li>Copy downloaded files to Cloud Storage bucket</li> </ul> In\u00a0[\u00a0]: Copied! <pre>! gsutil cp $LOCAL_DATA_PATH/'Amazon Sale Report.csv' $GCS_DATA_PATH\n</pre> ! gsutil cp $LOCAL_DATA_PATH/'Amazon Sale Report.csv' $GCS_DATA_PATH In\u00a0[\u00a0]: Copied! <pre># create dataset\n! set -x &amp;&amp; bq mk --force=true \\\n    --project_id $PROJECT_ID \\\n    --location $BQ_LOCATION \\\n    --dataset $BQ_DATASET_ID\n</pre> # create dataset ! set -x &amp;&amp; bq mk --force=true \\     --project_id $PROJECT_ID \\     --location $BQ_LOCATION \\     --dataset $BQ_DATASET_ID In\u00a0[\u00a0]: Copied! <pre>load_sql = f\"\"\"LOAD DATA OVERWRITE `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}`\n  FROM FILES(\n    format='CSV',\n    skip_leading_rows=1,\n    uris = ['{GCS_DATA_PATH}']\n  )\n\"\"\"\n\njob = bq_client.query(load_sql)  # API request.\njob.result()  # Waits for the query to finish.\n\nprint(f\"Data loaded into {PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}\")\n</pre> load_sql = f\"\"\"LOAD DATA OVERWRITE `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}`   FROM FILES(     format='CSV',     skip_leading_rows=1,     uris = ['{GCS_DATA_PATH}']   ) \"\"\"  job = bq_client.query(load_sql)  # API request. job.result()  # Waits for the query to finish.  print(f\"Data loaded into {PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}\") In\u00a0[\u00a0]: Copied! <pre>ddl_sql = f\"\"\"CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}` AS\n(\nWITH daily_sales AS (\n  SELECT\n    DATE_BUCKET(date, INTERVAL 1 DAY) AS date,\n    ROUND(SUM(AMOUNT), 2) AS total_sales,\n    ROUND(SUM(QTY), 2) AS total_qty,\n    sku\n  FROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}`\n  GROUP BY date, sku\n)\nSELECT\n  date,\n  sku,\n  IFNULL(total_sales, 0) total_sales,\n  IFNULL(total_qty, 0) total_inventory\nFROM (\n  SELECT\n    date,\n    sku,\n    total_sales,\n    total_qty\n  FROM GAP_FILL(\n    TABLE daily_sales,\n    ts_column =&gt; 'date',\n    bucket_width =&gt; INTERVAL 1 DAY,\n    partitioning_columns =&gt; ['sku'],\n    value_columns =&gt; [\n      ('total_sales', 'null'),\n      ('total_qty', 'null')\n    ]\n  )\n )\n)\n\"\"\"\n\njob = bq_client.query(ddl_sql)  # API request.\njob.result()  # Waits for the query to finish.\n\nprint(\n    f\"Data prepared and loaded into {PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}\"\n)\n</pre> ddl_sql = f\"\"\"CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}` AS ( WITH daily_sales AS (   SELECT     DATE_BUCKET(date, INTERVAL 1 DAY) AS date,     ROUND(SUM(AMOUNT), 2) AS total_sales,     ROUND(SUM(QTY), 2) AS total_qty,     sku   FROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_RAW}`   GROUP BY date, sku ) SELECT   date,   sku,   IFNULL(total_sales, 0) total_sales,   IFNULL(total_qty, 0) total_inventory FROM (   SELECT     date,     sku,     total_sales,     total_qty   FROM GAP_FILL(     TABLE daily_sales,     ts_column =&gt; 'date',     bucket_width =&gt; INTERVAL 1 DAY,     partitioning_columns =&gt; ['sku'],     value_columns =&gt; [       ('total_sales', 'null'),       ('total_qty', 'null')     ]   )  ) ) \"\"\"  job = bq_client.query(ddl_sql)  # API request. job.result()  # Waits for the query to finish.  print(     f\"Data prepared and loaded into {PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}\" ) <ul> <li>Run few SQL queries to find # of SKUs and sample data for a SKU.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>query = f\"\"\"SELECT sku, COUNTIF(total_sales&lt;&gt;0) CNT\nFROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}`\nGROUP BY sku\nORDER BY 2 DESC\n\"\"\"\n\nbq_client.query(query).to_dataframe()\n</pre> query = f\"\"\"SELECT sku, COUNTIF(total_sales&lt;&gt;0) CNT FROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}` GROUP BY sku ORDER BY 2 DESC \"\"\"  bq_client.query(query).to_dataframe() In\u00a0[\u00a0]: Copied! <pre>query = f\"\"\"SELECT date, sku, total_sales, total_inventory\nFROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}`\nWHERE sku = 'J0230-SKD-M'\nORDER BY DATE\"\"\"\n\nbq_client.query(query).to_dataframe()\n</pre> query = f\"\"\"SELECT date, sku, total_sales, total_inventory FROM `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_SALES_DAILY}` WHERE sku = 'J0230-SKD-M' ORDER BY DATE\"\"\"  bq_client.query(query).to_dataframe() <ul> <li>Import libraries</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import json\nimport os\n# Import the necessary packages\nfrom datetime import datetime\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform.prediction import LocalModel\n</pre> import json import os # Import the necessary packages from datetime import datetime from typing import Tuple  import numpy as np import pandas as pd from google.cloud import aiplatform from google.cloud.aiplatform.prediction import LocalModel <ul> <li>Configure staging bucket for model artifacts</li> </ul> In\u00a0[\u00a0]: Copied! <pre>STAGING_BUCKET = os.path.join(STAGING_BUCKET_URI, \"temporal\")\nMODEL_BUCKET = os.path.join(STAGING_BUCKET_URI, \"timesfm\")\n</pre> STAGING_BUCKET = os.path.join(STAGING_BUCKET_URI, \"temporal\") MODEL_BUCKET = os.path.join(STAGING_BUCKET_URI, \"timesfm\") <ul> <li>Setting up default service account</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Set up default SERVICE_ACCOUNT\nSERVICE_ACCOUNT = None\nshell_output = ! gcloud projects describe $PROJECT_ID\nproject_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\nSERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\nprint(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n</pre> # Set up default SERVICE_ACCOUNT SERVICE_ACCOUNT = None shell_output = ! gcloud projects describe $PROJECT_ID project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\") SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\" print(\"Using this default Service Account:\", SERVICE_ACCOUNT) <ul> <li>Provision permissions to the service account with the Cloud Storage bucket</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\nBUCKET_NAME = \"/\".join(STAGING_BUCKET_URI.split(\"/\")[:3])\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n</pre> # Provision permissions to the SERVICE_ACCOUNT with the GCS bucket BUCKET_NAME = \"/\".join(STAGING_BUCKET_URI.split(\"/\")[:3]) ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME In\u00a0[\u00a0]: Copied! <pre>VERTEX_AI_MODEL_GARDEN_TIMESFM = \"gs://vertex-model-garden-public-us/timesfm\"  # @param {type:\"string\", isTemplate:true} [\"gs://vertex-model-garden-public-us/timesfm\", \"gs://vertex-model-garden-public-eu/timesfm\", \"gs://vertex-model-garden-public-asia/timesfm\"]\nMODEL_VARIANT = \"timesfm-1.0-200m\"  # @param [\"timesfm-1.0-200m\"]\n\nprint(\n    \"Copying TimesFM model artifacts from\",\n    f\"{VERTEX_AI_MODEL_GARDEN_TIMESFM}/{MODEL_VARIANT}\",\n    \"to\",\n    MODEL_BUCKET,\n)\n\n! gsutil -m cp -r -R $VERTEX_AI_MODEL_GARDEN_TIMESFM/$MODEL_VARIANT $MODEL_BUCKET\n\ncheckpoint_path = MODEL_BUCKET\n</pre> VERTEX_AI_MODEL_GARDEN_TIMESFM = \"gs://vertex-model-garden-public-us/timesfm\"  # @param {type:\"string\", isTemplate:true} [\"gs://vertex-model-garden-public-us/timesfm\", \"gs://vertex-model-garden-public-eu/timesfm\", \"gs://vertex-model-garden-public-asia/timesfm\"] MODEL_VARIANT = \"timesfm-1.0-200m\"  # @param [\"timesfm-1.0-200m\"]  print(     \"Copying TimesFM model artifacts from\",     f\"{VERTEX_AI_MODEL_GARDEN_TIMESFM}/{MODEL_VARIANT}\",     \"to\",     MODEL_BUCKET, )  ! gsutil -m cp -r -R $VERTEX_AI_MODEL_GARDEN_TIMESFM/$MODEL_VARIANT $MODEL_BUCKET  checkpoint_path = MODEL_BUCKET In\u00a0[\u00a0]: Copied! <pre>! gsutil ls $checkpoint_path\n</pre> ! gsutil ls $checkpoint_path <ul> <li>Set TimesFM prebuilt serving docker image</li> </ul> In\u00a0[\u00a0]: Copied! <pre># The pre-built serving docker images.\nSERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-timesfm-serve:20240528_1310_RC00\"\n</pre> # The pre-built serving docker images. SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-timesfm-serve:20240528_1310_RC00\" <ul> <li>Utility function to deploy the model</li> </ul> In\u00a0[\u00a0]: Copied! <pre># @title utility functions to deploy the model\ndef get_job_name_with_datetime(prefix: str) -&gt; str:\n    \"\"\"Gets the job name with date time when triggering training or deployment\n\n    jobs in Vertex AI.\n    \"\"\"\n    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n\n\ndef deploy_model(\n    model_name: str,\n    checkpoint_path: str,\n    horizon: str,\n    machine_type: str = \"g2-standard-4\",\n    accelerator_type: str = \"NVIDIA_L4\",\n    accelerator_count: int = 1,\n    deploy_source: str = \"notebook\",\n) -&gt; Tuple[aiplatform.Model, aiplatform.Endpoint]:\n    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n    model_name_with_time = get_job_name_with_datetime(model_name)\n\n    endpoints = aiplatform.Endpoint.list(filter=f'display_name=\"{model_name}-endpoint\"')\n\n    if len(endpoints) &gt; 0:\n        print(f\"Using existing endpoint {endpoints[0].resource_name}\")\n        endpoint = aiplatform.Endpoint(endpoints[0].resource_name)\n    else:\n        print(f\"Creating a new endpoint {model_name}-endpoint\")\n        endpoint = aiplatform.Endpoint.create(\n            display_name=f\"{model_name}-endpoint\",\n            credentials=aiplatform.initializer.global_config.credentials,\n        )\n\n    if accelerator_type == \"ACCELERATOR_TYPE_UNSPECIFIED\":\n        timesfm_backend = \"cpu\"\n        accelerator_type = None\n    elif accelerator_type.startswith(\"NVIDIA\"):\n        timesfm_backend = \"gpu\"\n    else:\n        timesfm_backend = \"tpu\"\n\n    model = aiplatform.Model.upload(\n        display_name=model_name_with_time,\n        artifact_uri=checkpoint_path,\n        serving_container_image_uri=SERVE_DOCKER_URI,\n        serving_container_ports=[8080],\n        serving_container_predict_route=\"/predict\",\n        serving_container_health_route=\"/health\",\n        serving_container_environment_variables={\n            \"DEPLOY_SOURCE\": deploy_source,\n            \"TIMESFM_HORIZON\": str(horizon),\n            \"TIMESFM_BACKEND\": timesfm_backend,\n        },\n        credentials=aiplatform.initializer.global_config.credentials,\n    )\n    print(\n        f\"Deploying {model_name_with_time} on {machine_type} with\"\n        f\" {accelerator_count} {accelerator_type} GPU(s).\"\n    )\n    model.deploy(\n        endpoint=endpoint,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        deploy_request_timeout=1800,\n        service_account=SERVICE_ACCOUNT,\n        enable_access_logging=True,\n        min_replica_count=1,\n        sync=True,\n    )\n    return model, endpoint\n\n\ndef get_quota(project_id: str, region: str, resource_id: str) -&gt; int:\n    \"\"\"Returns the quota for a resource in a region.\n\n    Returns -1 if can not figure out the quota.\n    \"\"\"\n    quota_list_output = !gcloud alpha services quota list --service=\"aiplatform.googleapis.com\"  --consumer=projects/$project_id --filter=\"$service_endpoint/$resource_id\" --format=json\n    # Use '.s' on the command output because it is an SList type.\n    quota_data = json.loads(quota_list_output.s)\n    if len(quota_data) == 0 or \"consumerQuotaLimits\" not in quota_data[0]:\n        return -1\n    if (\n        len(quota_data[0][\"consumerQuotaLimits\"]) == 0\n        or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n    ):\n        return -1\n    all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n    for region_data in all_regions_data:\n        if (\n            region_data.get(\"dimensions\")\n            and region_data[\"dimensions\"][\"region\"] == region\n        ):\n            if \"effectiveLimit\" in region_data:\n                return int(region_data[\"effectiveLimit\"])\n            else:\n                return 0\n    return -1\n\n\ndef get_resource_id(accelerator_type: str, is_for_training: bool) -&gt; str:\n    \"\"\"Returns the resource id for a given accelerator type and the use case.\n\n    Args:\n      accelerator_type: The accelerator type.\n      is_for_training: Whether the resource is used for training. Set false for\n        serving use case.\n\n    Returns:\n      The resource id.\n    \"\"\"\n    training_accelerator_map = {\n        \"NVIDIA_TESLA_V100\": \"custom_model_training_nvidia_v100_gpus\",\n        \"NVIDIA_L4\": \"custom_model_training_nvidia_l4_gpus\",\n        \"NVIDIA_TESLA_A100\": \"custom_model_training_nvidia_a100_gpus\",\n        \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_training_cpus\",\n    }\n    serving_accelerator_map = {\n        \"NVIDIA_TESLA_V100\": \"custom_model_serving_nvidia_v100_gpus\",\n        \"NVIDIA_L4\": \"custom_model_serving_nvidia_l4_gpus\",\n        \"NVIDIA_TESLA_A100\": \"custom_model_serving_nvidia_a100_gpus\",\n        \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_serving_cpus\",\n    }\n    if is_for_training:\n        if accelerator_type in training_accelerator_map:\n            return training_accelerator_map[accelerator_type]\n        else:\n            raise ValueError(\n                f\"Could not find accelerator type: {accelerator_type} for training.\"\n            )\n    else:\n        if accelerator_type in serving_accelerator_map:\n            return serving_accelerator_map[accelerator_type]\n        else:\n            raise ValueError(\n                f\"Could not find accelerator type: {accelerator_type} for serving.\"\n            )\n\n\ndef check_quota(\n    project_id: str,\n    region: str,\n    accelerator_type: str,\n    accelerator_count: int,\n    is_for_training: bool,\n):\n    \"\"\"Checks if the project and the region has the required quota.\"\"\"\n    resource_id = get_resource_id(accelerator_type, is_for_training)\n    quota = get_quota(project_id, region, resource_id)\n    quota_request_instruction = (\n        \"Either use \"\n        \"a different region or request additional quota. Follow \"\n        \"instructions here \"\n        \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n        \" to check quota in a region or request additional quota for \"\n        \"your project.\"\n    )\n    if quota == -1:\n        raise ValueError(\n            f\"\"\"Quota not found for: {resource_id} in {region}.\n            {quota_request_instruction}\"\"\"\n        )\n    if quota &lt; accelerator_count:\n        raise ValueError(\n            f\"\"\"Quota not enough for {resource_id} in {region}:\n            {quota} &lt; {accelerator_count}.\n            {quota_request_instruction}\"\"\"\n        )\n</pre> # @title utility functions to deploy the model def get_job_name_with_datetime(prefix: str) -&gt; str:     \"\"\"Gets the job name with date time when triggering training or deployment      jobs in Vertex AI.     \"\"\"     return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")   def deploy_model(     model_name: str,     checkpoint_path: str,     horizon: str,     machine_type: str = \"g2-standard-4\",     accelerator_type: str = \"NVIDIA_L4\",     accelerator_count: int = 1,     deploy_source: str = \"notebook\", ) -&gt; Tuple[aiplatform.Model, aiplatform.Endpoint]:     \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"     model_name_with_time = get_job_name_with_datetime(model_name)      endpoints = aiplatform.Endpoint.list(filter=f'display_name=\"{model_name}-endpoint\"')      if len(endpoints) &gt; 0:         print(f\"Using existing endpoint {endpoints[0].resource_name}\")         endpoint = aiplatform.Endpoint(endpoints[0].resource_name)     else:         print(f\"Creating a new endpoint {model_name}-endpoint\")         endpoint = aiplatform.Endpoint.create(             display_name=f\"{model_name}-endpoint\",             credentials=aiplatform.initializer.global_config.credentials,         )      if accelerator_type == \"ACCELERATOR_TYPE_UNSPECIFIED\":         timesfm_backend = \"cpu\"         accelerator_type = None     elif accelerator_type.startswith(\"NVIDIA\"):         timesfm_backend = \"gpu\"     else:         timesfm_backend = \"tpu\"      model = aiplatform.Model.upload(         display_name=model_name_with_time,         artifact_uri=checkpoint_path,         serving_container_image_uri=SERVE_DOCKER_URI,         serving_container_ports=[8080],         serving_container_predict_route=\"/predict\",         serving_container_health_route=\"/health\",         serving_container_environment_variables={             \"DEPLOY_SOURCE\": deploy_source,             \"TIMESFM_HORIZON\": str(horizon),             \"TIMESFM_BACKEND\": timesfm_backend,         },         credentials=aiplatform.initializer.global_config.credentials,     )     print(         f\"Deploying {model_name_with_time} on {machine_type} with\"         f\" {accelerator_count} {accelerator_type} GPU(s).\"     )     model.deploy(         endpoint=endpoint,         machine_type=machine_type,         accelerator_type=accelerator_type,         accelerator_count=accelerator_count,         deploy_request_timeout=1800,         service_account=SERVICE_ACCOUNT,         enable_access_logging=True,         min_replica_count=1,         sync=True,     )     return model, endpoint   def get_quota(project_id: str, region: str, resource_id: str) -&gt; int:     \"\"\"Returns the quota for a resource in a region.      Returns -1 if can not figure out the quota.     \"\"\"     quota_list_output = !gcloud alpha services quota list --service=\"aiplatform.googleapis.com\"  --consumer=projects/$project_id --filter=\"$service_endpoint/$resource_id\" --format=json     # Use '.s' on the command output because it is an SList type.     quota_data = json.loads(quota_list_output.s)     if len(quota_data) == 0 or \"consumerQuotaLimits\" not in quota_data[0]:         return -1     if (         len(quota_data[0][\"consumerQuotaLimits\"]) == 0         or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]     ):         return -1     all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]     for region_data in all_regions_data:         if (             region_data.get(\"dimensions\")             and region_data[\"dimensions\"][\"region\"] == region         ):             if \"effectiveLimit\" in region_data:                 return int(region_data[\"effectiveLimit\"])             else:                 return 0     return -1   def get_resource_id(accelerator_type: str, is_for_training: bool) -&gt; str:     \"\"\"Returns the resource id for a given accelerator type and the use case.      Args:       accelerator_type: The accelerator type.       is_for_training: Whether the resource is used for training. Set false for         serving use case.      Returns:       The resource id.     \"\"\"     training_accelerator_map = {         \"NVIDIA_TESLA_V100\": \"custom_model_training_nvidia_v100_gpus\",         \"NVIDIA_L4\": \"custom_model_training_nvidia_l4_gpus\",         \"NVIDIA_TESLA_A100\": \"custom_model_training_nvidia_a100_gpus\",         \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_training_cpus\",     }     serving_accelerator_map = {         \"NVIDIA_TESLA_V100\": \"custom_model_serving_nvidia_v100_gpus\",         \"NVIDIA_L4\": \"custom_model_serving_nvidia_l4_gpus\",         \"NVIDIA_TESLA_A100\": \"custom_model_serving_nvidia_a100_gpus\",         \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_serving_cpus\",     }     if is_for_training:         if accelerator_type in training_accelerator_map:             return training_accelerator_map[accelerator_type]         else:             raise ValueError(                 f\"Could not find accelerator type: {accelerator_type} for training.\"             )     else:         if accelerator_type in serving_accelerator_map:             return serving_accelerator_map[accelerator_type]         else:             raise ValueError(                 f\"Could not find accelerator type: {accelerator_type} for serving.\"             )   def check_quota(     project_id: str,     region: str,     accelerator_type: str,     accelerator_count: int,     is_for_training: bool, ):     \"\"\"Checks if the project and the region has the required quota.\"\"\"     resource_id = get_resource_id(accelerator_type, is_for_training)     quota = get_quota(project_id, region, resource_id)     quota_request_instruction = (         \"Either use \"         \"a different region or request additional quota. Follow \"         \"instructions here \"         \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"         \" to check quota in a region or request additional quota for \"         \"your project.\"     )     if quota == -1:         raise ValueError(             f\"\"\"Quota not found for: {resource_id} in {region}.             {quota_request_instruction}\"\"\"         )     if quota &lt; accelerator_count:         raise ValueError(             f\"\"\"Quota not enough for {resource_id} in {region}:             {quota} &lt; {accelerator_count}.             {quota_request_instruction}\"\"\"         ) In\u00a0[\u00a0]: Copied! <pre>DEFAULT_HTTP_PORT = 7080\n\nlocal_model = LocalModel(\n    serving_container_image_uri=SERVE_DOCKER_URI,\n    serving_container_predict_route=\"/predict\",\n    serving_container_health_route=\"/health\",\n    serving_container_ports=[DEFAULT_HTTP_PORT],\n)\n</pre> DEFAULT_HTTP_PORT = 7080  local_model = LocalModel(     serving_container_image_uri=SERVE_DOCKER_URI,     serving_container_predict_route=\"/predict\",     serving_container_health_route=\"/health\",     serving_container_ports=[DEFAULT_HTTP_PORT], ) <ul> <li>You can inspect the container's spec to get useful information such as image URI and environment variables.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>local_model.get_serving_container_spec()\n</pre> local_model.get_serving_container_spec() <ul> <li>Deploy the model to local endpoint and send a prediction request</li> </ul> In\u00a0[\u00a0]: Copied! <pre>instances = [\n    {\"input\": np.sin(np.linspace(0, 20, 100)).tolist(), \"freq\": 0},\n    {\"input\": np.sin(np.linspace(0, 40, 500)).tolist(), \"freq\": 0},\n    {\n        \"input\": (\n            np.sin(np.linspace(0, 50, 300)) + np.sin(np.linspace(1, 71, 300)) * 0.5\n        ).tolist(),\n        \"freq\": 0,\n    },\n]\npayload = {\"instances\": instances}\n\nwith open(\"payload.json\", \"w\") as f:\n    json.dump(payload, f)\n</pre> instances = [     {\"input\": np.sin(np.linspace(0, 20, 100)).tolist(), \"freq\": 0},     {\"input\": np.sin(np.linspace(0, 40, 500)).tolist(), \"freq\": 0},     {         \"input\": (             np.sin(np.linspace(0, 50, 300)) + np.sin(np.linspace(1, 71, 300)) * 0.5         ).tolist(),         \"freq\": 0,     }, ] payload = {\"instances\": instances}  with open(\"payload.json\", \"w\") as f:     json.dump(payload, f) In\u00a0[\u00a0]: Copied! <pre>with local_model.deploy_to_local_endpoint(\n    artifact_uri=f\"{MODEL_BUCKET}\",\n    host_port=DEFAULT_HTTP_PORT,\n    container_ready_timeout=1500,\n) as local_endpoint:\n    health_check_response = local_endpoint.run_health_check()\n    predict_response = local_endpoint.predict(\n        request_file=\"payload.json\", headers={\"Content-Type\": \"application/json\"}\n    )\n</pre> with local_model.deploy_to_local_endpoint(     artifact_uri=f\"{MODEL_BUCKET}\",     host_port=DEFAULT_HTTP_PORT,     container_ready_timeout=1500, ) as local_endpoint:     health_check_response = local_endpoint.run_health_check()     predict_response = local_endpoint.predict(         request_file=\"payload.json\", headers={\"Content-Type\": \"application/json\"}     ) <ul> <li>Print out the predict response, health check response and its content.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>print(health_check_response, health_check_response.content)\nprint(predict_response, predict_response.content)\n</pre> print(health_check_response, health_check_response.content) print(predict_response, predict_response.content) <ul> <li>Also print out all the container logs. You will see the logs of container startup, serving requests, and container teardown.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>local_endpoint.print_container_logs_if_container_is_not_running(show_all=True)\n# local_endpoint.print_container_logs(show_all=True)\n</pre> local_endpoint.print_container_logs_if_container_is_not_running(show_all=True) # local_endpoint.print_container_logs(show_all=True) In\u00a0[\u00a0]: Copied! <pre>print(f\"Loading checkpoint from {MODEL_BUCKET}.\")\n</pre> print(f\"Loading checkpoint from {MODEL_BUCKET}.\") <ul> <li>Choose the backend (accelerator type) to use to deploy the model.</li> </ul>  \u24d8          <li> TimesFM is fast even with the CPU backend. Consider GPU only if you need to handle large queries per second. </li> <li> After deployment, please take a look at the log to get the model / endpoint that you can use in another session. </li> In\u00a0[\u00a0]: Copied! <pre>accelerator_type = \"CPU\"  # @param [\"CPU\", \"NVIDIA_L4\"]\nif accelerator_type == \"NVIDIA_L4\":\n    machine_type = \"g2-standard-4\"\n    accelerator_count = 1\nelif accelerator_type == \"CPU\":\n    accelerator_type = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n    machine_type = \"n1-standard-8\"\n    accelerator_count = 0\nelse:\n    raise ValueError(\n        f\"Recommended machine settings not found for: {accelerator_type}. To use\"\n        \" another another accelerator, edit this code block to pass in an\"\n        \" appropriate `machine_type`, `accelerator_type`, and\"\n        \" `accelerator_count` to the deploy_model function by clicking `Show\"\n        \" Code` and then modifying the code.\"\n    )\n\nif accelerator_type != \"ACCELERATOR_TYPE_UNSPECIFIED\":\n    check_quota(\n        project_id=PROJECT_ID,\n        region=REGION,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        is_for_training=False,\n    )\n\nprint(\"Quota is OK.\")\n</pre> accelerator_type = \"CPU\"  # @param [\"CPU\", \"NVIDIA_L4\"] if accelerator_type == \"NVIDIA_L4\":     machine_type = \"g2-standard-4\"     accelerator_count = 1 elif accelerator_type == \"CPU\":     accelerator_type = \"ACCELERATOR_TYPE_UNSPECIFIED\"     machine_type = \"n1-standard-8\"     accelerator_count = 0 else:     raise ValueError(         f\"Recommended machine settings not found for: {accelerator_type}. To use\"         \" another another accelerator, edit this code block to pass in an\"         \" appropriate `machine_type`, `accelerator_type`, and\"         \" `accelerator_count` to the deploy_model function by clicking `Show\"         \" Code` and then modifying the code.\"     )  if accelerator_type != \"ACCELERATOR_TYPE_UNSPECIFIED\":     check_quota(         project_id=PROJECT_ID,         region=REGION,         accelerator_type=accelerator_type,         accelerator_count=accelerator_count,         is_for_training=False,     )  print(\"Quota is OK.\") <ul> <li>Specify the forecast horizon TimesFM will be queried on to compile its computation.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>horizon = 256  # @param {type:\"number\"}\n</pre> horizon = 256  # @param {type:\"number\"} <ul> <li>Deploy model to endpoint if does not exist</li> </ul> \u26a0\ufe0f Deployment may take upto 20 minutes. Please be patient... \u26a0\ufe0f In\u00a0[\u00a0]: Copied! <pre>print(\"Creating endpoint.\")\n\nTIMESFM_MODEL_DISPLAY_NAME = f\"timesfm-{MODEL_VARIANT}\"\nTIMESFM_ENDPOINT_DISPLAY_NAME = f\"{TIMESFM_MODEL_DISPLAY_NAME}-endpoint\"\n\nmodel, endpoint = deploy_model(\n    model_name=TIMESFM_MODEL_DISPLAY_NAME,\n    checkpoint_path=checkpoint_path,\n    horizon=horizon,\n    machine_type=machine_type,\n    accelerator_type=accelerator_type,\n    accelerator_count=accelerator_count,\n)\n</pre> print(\"Creating endpoint.\")  TIMESFM_MODEL_DISPLAY_NAME = f\"timesfm-{MODEL_VARIANT}\" TIMESFM_ENDPOINT_DISPLAY_NAME = f\"{TIMESFM_MODEL_DISPLAY_NAME}-endpoint\"  model, endpoint = deploy_model(     model_name=TIMESFM_MODEL_DISPLAY_NAME,     checkpoint_path=checkpoint_path,     horizon=horizon,     machine_type=machine_type,     accelerator_type=accelerator_type,     accelerator_count=accelerator_count, ) In\u00a0[\u00a0]: Copied! <pre>endpoints = aiplatform.Endpoint.list(\n    filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"'\n)\n\nif len(endpoints) &gt; 0:\n    endpoint = aiplatform.Endpoint(endpoints[0].resource_name)\nelse:\n    raise Exception(\n        f\"Endpoint does not exist with name {TIMESFM_ENDPOINT_DISPLAY_NAME}\"\n    )\n</pre> endpoints = aiplatform.Endpoint.list(     filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"' )  if len(endpoints) &gt; 0:     endpoint = aiplatform.Endpoint(endpoints[0].resource_name) else:     raise Exception(         f\"Endpoint does not exist with name {TIMESFM_ENDPOINT_DISPLAY_NAME}\"     ) In\u00a0[\u00a0]: Copied! <pre>with open(\"payload.json\") as f:\n    response = endpoint.predict(**json.load(f))\n</pre> with open(\"payload.json\") as f:     response = endpoint.predict(**json.load(f)) In\u00a0[\u00a0]: Copied! <pre>! cat payload.json | jq -c\n</pre> ! cat payload.json | jq -c In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame(response.predictions).head()\n</pre> pd.DataFrame(response.predictions).head() <p>To start, we\u2019ll need to define functions that Gemini will use as tools to interact with external systems and APIs to retrieve real-time information. You just write Python functions and use them as tools when defining the agent!</p> In\u00a0[\u00a0]: Copied! <pre>import ast\nfrom typing import Any, Dict, Sequence\n</pre> import ast from typing import Any, Dict, Sequence In\u00a0[\u00a0]: Copied! <pre>def list_datasets():\n    \"\"\"Get a list of datasets that will help answer the user's question.\n\n    args:\n      None\n    \"\"\"\n    # from google.cloud import bigquery\n    # client = bigquery.Client(project=PROJECT_ID)\n    # return [dataset.dataset_id for dataset in list_datasets()]\n    return [BQ_DATASET_ID]\n\n\ndef list_tables(dataset_id: str):\n    \"\"\"List tables in a dataset that will help answer the user's question\n\n    args:\n      dataset_id (str)     Dataset ID to fetch tables from.\n    \"\"\"\n    from google.cloud import bigquery\n\n    client = bigquery.Client(project=PROJECT_ID)\n    tables = client.list_tables(dataset_id)\n    return str([table.table_id for table in tables])\n\n\ndef get_table(table_id: str):\n    \"\"\"Get information about a table, including the description, schema, and\n    number of rows that will help answer the user's question.\n    Always use the fully qualified dataset and table names.\n\n    args:\n      table_id (str)       Fully qualified ID of the table to get information about\n    \"\"\"\n    from google.cloud import bigquery\n\n    client = bigquery.Client(project=PROJECT_ID)\n    table = client.get_table(table_id)\n    return table.to_api_repr()\n\n\ndef run_sql_query(query: str):\n    \"\"\"Get information from data in BigQuery using SQL queries.\n\n    args:\n      query (str)         SQL query on a single line that will help give\n         quantitative answers to the user's question when run on a BigQuery\n         dataset and table. In the SQL query, always use the fully qualified\n         dataset and table names.\",\n    \"\"\"\n    from google.cloud import bigquery\n\n    client = bigquery.Client(project=PROJECT_ID)\n    job_config = bigquery.QueryJobConfig(\n        maximum_bytes_billed=100000000\n    )  # Data limit per query job\n    try:\n        cleaned_query = query.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\\\\", \"\")\n        print(cleaned_query)\n        query_job = client.query(cleaned_query, job_config=job_config)\n        result = query_job.result()\n        result = str([dict(row) for row in result])\n        result = result.replace(\"\\\\\", \"\").replace(\"\\n\", \"\")\n        return result\n    except Exception as e:\n        result = f\"{str(e)}\"\n        return result\n</pre> def list_datasets():     \"\"\"Get a list of datasets that will help answer the user's question.      args:       None     \"\"\"     # from google.cloud import bigquery     # client = bigquery.Client(project=PROJECT_ID)     # return [dataset.dataset_id for dataset in list_datasets()]     return [BQ_DATASET_ID]   def list_tables(dataset_id: str):     \"\"\"List tables in a dataset that will help answer the user's question      args:       dataset_id (str)     Dataset ID to fetch tables from.     \"\"\"     from google.cloud import bigquery      client = bigquery.Client(project=PROJECT_ID)     tables = client.list_tables(dataset_id)     return str([table.table_id for table in tables])   def get_table(table_id: str):     \"\"\"Get information about a table, including the description, schema, and     number of rows that will help answer the user's question.     Always use the fully qualified dataset and table names.      args:       table_id (str)       Fully qualified ID of the table to get information about     \"\"\"     from google.cloud import bigquery      client = bigquery.Client(project=PROJECT_ID)     table = client.get_table(table_id)     return table.to_api_repr()   def run_sql_query(query: str):     \"\"\"Get information from data in BigQuery using SQL queries.      args:       query (str)         SQL query on a single line that will help give          quantitative answers to the user's question when run on a BigQuery          dataset and table. In the SQL query, always use the fully qualified          dataset and table names.\",     \"\"\"     from google.cloud import bigquery      client = bigquery.Client(project=PROJECT_ID)     job_config = bigquery.QueryJobConfig(         maximum_bytes_billed=100000000     )  # Data limit per query job     try:         cleaned_query = query.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\\\\", \"\")         print(cleaned_query)         query_job = client.query(cleaned_query, job_config=job_config)         result = query_job.result()         result = str([dict(row) for row in result])         result = result.replace(\"\\\\\", \"\").replace(\"\\n\", \"\")         return result     except Exception as e:         result = f\"{str(e)}\"         return result <ul> <li>Run sample queries and test it out</li> </ul> In\u00a0[\u00a0]: Copied! <pre>dataset = [dataset for dataset in list_datasets()][0]\ndataset\n</pre> dataset = [dataset for dataset in list_datasets()][0] dataset In\u00a0[\u00a0]: Copied! <pre>get_table(f\"{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily\")\n</pre> get_table(f\"{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily\") In\u00a0[\u00a0]: Copied! <pre>def prepare_timesfm_payload(ts: Sequence[float]) -&gt; Dict[str, Sequence[Any]]:\n    \"\"\"format payload to work forecasting model endpoint\"\"\"\n    return {\"instances\": [{\"input\": ts}]}\n\n\ndef run_forecasts(ts: Sequence[float], return_quantiles: bool = False):\n    \"\"\"Use this function to generate forecasts or estimates based on historical\n    time-series contexts such as sales, inventory that you fetch from sales\n    tables or related tables. The function returns point forecasts.\n    Quantile forecasts are returned when enabled.\n\n    input args:\n      ts (Sequence):\n        input sequence of time-series context.\n      return_quantiles (bool):\n        return quantile forecasts when enabled.\n\n    returns:\n      returns list of point forecasts and quantile forecasts for each of the\n      input time-series context.\n    \"\"\"\n    aiplatform.init(project=PROJECT_ID, location=LOCATION)\n    endpoints = aiplatform.Endpoint.list(\n        filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"'\n    )\n    endpoint = aiplatform.Endpoint(endpoints[0].resource_name)\n\n    payload = prepare_timesfm_payload(ts)\n    forecasts = endpoint.predict(**payload)\n    num_horizon = 30\n    if len(forecasts.predictions) &gt; 0:\n        point_forecast = forecasts.predictions[0][\"point_forecast\"][:num_horizon]\n        if return_quantiles:\n            qf_data = forecasts.predictions[0][\"quantile_forecast\"]\n            quantile_forecast = list(zip(*qf_data[:num_horizon]))\n            return point_forecast, quantile_forecast\n        else:\n            return (point_forecast,)\n    else:\n        return \"Failed to generate forecasts\"\n</pre> def prepare_timesfm_payload(ts: Sequence[float]) -&gt; Dict[str, Sequence[Any]]:     \"\"\"format payload to work forecasting model endpoint\"\"\"     return {\"instances\": [{\"input\": ts}]}   def run_forecasts(ts: Sequence[float], return_quantiles: bool = False):     \"\"\"Use this function to generate forecasts or estimates based on historical     time-series contexts such as sales, inventory that you fetch from sales     tables or related tables. The function returns point forecasts.     Quantile forecasts are returned when enabled.      input args:       ts (Sequence):         input sequence of time-series context.       return_quantiles (bool):         return quantile forecasts when enabled.      returns:       returns list of point forecasts and quantile forecasts for each of the       input time-series context.     \"\"\"     aiplatform.init(project=PROJECT_ID, location=LOCATION)     endpoints = aiplatform.Endpoint.list(         filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"'     )     endpoint = aiplatform.Endpoint(endpoints[0].resource_name)      payload = prepare_timesfm_payload(ts)     forecasts = endpoint.predict(**payload)     num_horizon = 30     if len(forecasts.predictions) &gt; 0:         point_forecast = forecasts.predictions[0][\"point_forecast\"][:num_horizon]         if return_quantiles:             qf_data = forecasts.predictions[0][\"quantile_forecast\"]             quantile_forecast = list(zip(*qf_data[:num_horizon]))             return point_forecast, quantile_forecast         else:             return (point_forecast,)     else:         return \"Failed to generate forecasts\" <ul> <li>Define a SKU</li> </ul> In\u00a0[\u00a0]: Copied! <pre># sku = \"JNE3797-KR-XXXL\"\nsku = \"J0230-SKD-M\"\n</pre> # sku = \"JNE3797-KR-XXXL\" sku = \"J0230-SKD-M\" <ul> <li>Get historical time-series context for SKU</li> </ul> In\u00a0[\u00a0]: Copied! <pre>query = f\"\"\"\nSELECT total_inventory\nFROM `{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily`\nWHERE sku = '{sku}'\nAND date &lt;= DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)\nORDER BY date\n\"\"\"\nprint(query)\nresult = run_sql_query(query)\nts = [row[\"total_inventory\"] for row in ast.literal_eval(result)]\n</pre> query = f\"\"\" SELECT total_inventory FROM `{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily` WHERE sku = '{sku}' AND date &lt;= DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) ORDER BY date \"\"\" print(query) result = run_sql_query(query) ts = [row[\"total_inventory\"] for row in ast.literal_eval(result)] <ul> <li>Call TimesFM model to generate forecasts</li> </ul> In\u00a0[\u00a0]: Copied! <pre>forecasts = run_forecasts(ts, return_quantiles=True)\npoint_forecast = forecasts[0]\nquantile_forecast = list(zip(*forecasts[1])) if len(forecasts) &gt; 1 else []\n</pre> forecasts = run_forecasts(ts, return_quantiles=True) point_forecast = forecasts[0] quantile_forecast = list(zip(*forecasts[1])) if len(forecasts) &gt; 1 else [] <ul> <li>Get actual values for the SKU that will be predicted by the model</li> </ul> In\u00a0[\u00a0]: Copied! <pre>query = f\"\"\"\nSELECT total_inventory\nFROM `{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily`\nWHERE sku = '{sku}'\nAND date &gt; DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)\nORDER BY date\n\"\"\"\nprint(query)\nresult = run_sql_query(query)\nts_actuals = [row[\"total_inventory\"] for row in ast.literal_eval(result)]\n</pre> query = f\"\"\" SELECT total_inventory FROM `{PROJECT_ID}.{BQ_DATASET_ID}.sales_daily` WHERE sku = '{sku}' AND date &gt; DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR) ORDER BY date \"\"\" print(query) result = run_sql_query(query) ts_actuals = [row[\"total_inventory\"] for row in ast.literal_eval(result)] <ul> <li>Plot historical time-series with point and quantile forecasts and actuals</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set()\nsns.set_style(\"dark\")\n\n\ndef visualize_forecast(\n    context: list[float],\n    horizon_mean: list[float],\n    ground_truth: list[float] | None = None,\n    horizon_lower: list[float] | None = None,\n    horizon_upper: list[float] | None = None,\n    ylabel: str | None = None,\n    title: str | None = None,\n):\n    plt_range = list(range(len(context) + len(horizon_mean)))\n    plt.figure(figsize=(10, 6))\n    plt.plot(\n        plt_range,\n        context + [np.nan for _ in horizon_mean],\n        color=\"tab:cyan\",\n        label=\"context\",\n    )\n    plt.plot(\n        plt_range,\n        [np.nan for _ in context] + horizon_mean,\n        color=\"tab:red\",\n        label=\"forecast\",\n    )\n    if ground_truth:\n        plt.plot(\n            list(range(len(context) + len(ground_truth))),\n            [np.nan for _ in context] + ground_truth,\n            color=\"tab:purple\",\n            label=\"ground truth\",\n        )\n    if horizon_upper and horizon_lower:\n        plt.plot(\n            plt_range,\n            [np.nan for _ in context] + horizon_upper,\n            color=\"tab:orange\",\n            linestyle=\"--\",\n            label=\"forecast, upper\",\n        )\n        plt.plot(\n            plt_range,\n            [np.nan for _ in context] + horizon_lower,\n            color=\"tab:orange\",\n            linestyle=\":\",\n            label=\"forecast, lower\",\n        )\n        plt.fill_between(\n            plt_range,\n            [np.nan for _ in context] + horizon_upper,\n            [np.nan for _ in context] + horizon_lower,\n            color=\"tab:orange\",\n            alpha=0.2,\n        )\n    plt.ylabel(ylabel) if ylabel else None\n    plt.title(title) if title else None\n    plt.xlabel(\"time\")\n    plt.legend()\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np import seaborn as sns  sns.set() sns.set_style(\"dark\")   def visualize_forecast(     context: list[float],     horizon_mean: list[float],     ground_truth: list[float] | None = None,     horizon_lower: list[float] | None = None,     horizon_upper: list[float] | None = None,     ylabel: str | None = None,     title: str | None = None, ):     plt_range = list(range(len(context) + len(horizon_mean)))     plt.figure(figsize=(10, 6))     plt.plot(         plt_range,         context + [np.nan for _ in horizon_mean],         color=\"tab:cyan\",         label=\"context\",     )     plt.plot(         plt_range,         [np.nan for _ in context] + horizon_mean,         color=\"tab:red\",         label=\"forecast\",     )     if ground_truth:         plt.plot(             list(range(len(context) + len(ground_truth))),             [np.nan for _ in context] + ground_truth,             color=\"tab:purple\",             label=\"ground truth\",         )     if horizon_upper and horizon_lower:         plt.plot(             plt_range,             [np.nan for _ in context] + horizon_upper,             color=\"tab:orange\",             linestyle=\"--\",             label=\"forecast, upper\",         )         plt.plot(             plt_range,             [np.nan for _ in context] + horizon_lower,             color=\"tab:orange\",             linestyle=\":\",             label=\"forecast, lower\",         )         plt.fill_between(             plt_range,             [np.nan for _ in context] + horizon_upper,             [np.nan for _ in context] + horizon_lower,             color=\"tab:orange\",             alpha=0.2,         )     plt.ylabel(ylabel) if ylabel else None     plt.title(title) if title else None     plt.xlabel(\"time\")     plt.legend()     plt.show() In\u00a0[\u00a0]: Copied! <pre>visualize_forecast(\n    context=ts,\n    horizon_mean=point_forecast,\n    ground_truth=ts_actuals,\n    horizon_lower=[x[2] for x in quantile_forecast],\n    horizon_upper=[x[8] for x in quantile_forecast],\n    title=\"forecasts\",\n    ylabel=\"inventory\",\n)\n</pre> visualize_forecast(     context=ts,     horizon_mean=point_forecast,     ground_truth=ts_actuals,     horizon_lower=[x[2] for x in quantile_forecast],     horizon_upper=[x[8] for x in quantile_forecast],     title=\"forecasts\",     ylabel=\"inventory\", ) <p>After defining all of the functions that you want to include as tools in your AI agent, you can define an agent using our LangChain template</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Markdown, display\nfrom vertexai.preview import reasoning_engines\n</pre> from IPython.display import Markdown, display from vertexai.preview import reasoning_engines <ul> <li>Configure the model name to be used for reasoning</li> </ul> In\u00a0[\u00a0]: Copied! <pre>AGENT_MODEL = \"gemini-2.0-flash-001\"\n</pre> AGENT_MODEL = \"gemini-2.0-flash-001\" In\u00a0[\u00a0]: Copied! <pre>model = AGENT_MODEL\n\nagent = reasoning_engines.LangchainAgent(\n    model=model,\n    model_kwargs={\"temperature\": 0.3},\n    tools=[list_datasets, list_tables, get_table, run_sql_query, run_forecasts],\n    agent_executor_kwargs={\"return_intermediate_steps\": True, \"verbose\": True},\n)\nagent.set_up()\n\nprompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed for generated forecasts after querying table.\"\n</pre> model = AGENT_MODEL  agent = reasoning_engines.LangchainAgent(     model=model,     model_kwargs={\"temperature\": 0.3},     tools=[list_datasets, list_tables, get_table, run_sql_query, run_forecasts],     agent_executor_kwargs={\"return_intermediate_steps\": True, \"verbose\": True}, ) agent.set_up()  prompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed for generated forecasts after querying table.\" <p>Note that the <code>tools</code> kwarg includes references to the functions that were described earlier, and the LangChain template in Reasoning Engine introspects the function name, function arguments, default argument values, docstrings, and type hints so that it can pass all of this information as part of the tool description to the agent and Gemini model.</p> <p>We designed this LangChain template so that you can quickly get started out-of-the-box using default values. We also built the template so that you can have maximum flexibility when customizing the layers of your agent to modify reasoning behavior, generative model parameters, swap out the default agent logic for another type of LangChain agent, or even swap out LangChain for an entirely different orchestration framework!</p> In\u00a0[\u00a0]: Copied! <pre># sku = \"JNE3797-KR-XXXL\"\nsku = \"J0230-SKD-M\"\n</pre> # sku = \"JNE3797-KR-XXXL\" sku = \"J0230-SKD-M\" In\u00a0[\u00a0]: Copied! <pre>response = agent.query(\n    input=f\"\"\"{prompt_prefix} What are daily sales for SKU {sku} last 20 days by date?\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = agent.query(     input=f\"\"\"{prompt_prefix} What are daily sales for SKU {sku} last 20 days by date?\"\"\" ) display(Markdown(response[\"output\"])) <p>Let's take a deeper look behind the scenes of this example query and break down what actions the AI agent took at runtime to go from the user\u2019s input prompt to the output that contains a natural language summary of the answer:</p> <ol> <li>User submits a query: The user sends an input prompt asking about daily sales for a SKU.</li> <li>Send query and tools to model: The agent packages the query with tool descriptions and sends it to the Gemini model.</li> <li>Model decides on tool usage: Based on the query and tool descriptions, the Gemini model decides whether to utilize a specific function (<code>run_sql_query</code>) and which parameters to send as inputs to the function (runs SQL queries on BigQuery dataset).</li> <li>Application calls the tool: The application executes the model\u2019s instructions by calling the appropriate function (<code>list_datasets</code>, <code>list_tables</code>, <code>get_table</code>, <code>run_sql_query</code>, <code>run_forecasts</code>) with the provided parameters.</li> <li>Tool results: The application receives a response from the tool (an API response payload).</li> <li>Return results to model: The application sends the API response payload to the model.</li> <li>Return results to agent: The agent interacts with the model to understand the observation based on the response.</li> <li>Agent determines next steps: This process repeats if the agent determines additional tool calls are necessary or if the agent should prepare a final response to send to the user.</li> <li>Model generates response: Based on the results from the external API and the agent iterations, the model then generates a natural language response for the user that contains the latest sales and sales forecasts.</li> </ol> <p>Recommend reading this reference to know about Building and Deploying AI Agents with LangChain on Vertex AI</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>response = agent.query(\n    input=f\"\"\"{prompt_prefix} Generate daily sales forecasts for SKU {sku} using only last 2 weeks of sales. Display as a table with date.\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = agent.query(     input=f\"\"\"{prompt_prefix} Generate daily sales forecasts for SKU {sku} using only last 2 weeks of sales. Display as a table with date.\"\"\" ) display(Markdown(response[\"output\"])) In\u00a0[\u00a0]: Copied! <pre>class CustomLangChainAgent:\n    def set_up(self):\n        from typing import List, Union\n\n        import langchain_google_vertexai\n        from langchain import hub\n        from langchain.agents import AgentExecutor  # type: ignore\n        from langchain.agents.format_scratchpad import \\\n            format_to_openai_function_messages\n        from langchain.tools.base import StructuredTool\n        from langchain_core.agents import (AgentAction, AgentActionMessageLog,\n                                           AgentFinish)\n        from langchain_core.output_parsers import BaseOutputParser\n        from langchain_core.outputs import ChatGeneration, Generation\n        from langchain_core.prompts import MessagesPlaceholder\n\n        class _TestOutputParser(BaseOutputParser):\n            def parse_result(\n                self, result: List[Generation], *, partial: bool = False\n            ) -&gt; Union[AgentAction, AgentFinish]:\n                if not isinstance(result[0], ChatGeneration):\n                    raise ValueError(\n                        \"This output parser only works on ChatGeneration output\"\n                    )\n                message = result[0].message\n                function_call = message.additional_kwargs.get(\"function_call\", {})\n                if function_call:\n                    function_name = function_call[\"name\"]\n                    tool_input = function_call.get(\"arguments\", {})\n                    tool_input = json.loads(tool_input)\n\n                    content_msg = (\n                        f\"responded: {message.content}\\n\" if message.content else \"\\n\"\n                    )\n                    log_msg = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\n                    return AgentActionMessageLog(\n                        tool=function_name,\n                        tool_input=tool_input,\n                        log=log_msg,\n                        message_log=[message],\n                    )\n\n                return AgentFinish(\n                    return_values={\"output\": message.content}, log=str(message.content)\n                )\n\n            def parse(self, text: str) -&gt; Union[AgentAction, AgentFinish]:\n                raise ValueError(\"Can only parse messages\")\n\n        tools_func = [\n            list_datasets,\n            list_tables,\n            get_table,\n            run_sql_query,\n            run_forecasts,\n        ]\n\n        tools = [StructuredTool.from_function(tool) for tool in tools_func]\n\n        prompt_template = hub.pull(\"homanp/superagent\")\n        prompt = prompt_template.from_messages(\n            [\n                (\"user\", \"{input}\"),\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            ]\n        )\n\n        llm = langchain_google_vertexai.chat_models.ChatVertexAI(\n            model_name=\"gemini-2.0-flash-001\",\n            temperature=0.3,\n        )\n\n        agent = (\n            {  # type: ignore\n                \"input\": lambda x: x[\"input\"],\n                \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n                    x[\"intermediate_steps\"]\n                ),\n            }\n            | prompt\n            | llm.bind(functions=tools)\n            | _TestOutputParser()\n        )\n        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n    def query(self, query: str):\n        prompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use the tools provided for generating forecasts. \"\n        return self.agent_executor.invoke({\"input\": f\"{prompt_prefix} {query}\"})\n</pre> class CustomLangChainAgent:     def set_up(self):         from typing import List, Union          import langchain_google_vertexai         from langchain import hub         from langchain.agents import AgentExecutor  # type: ignore         from langchain.agents.format_scratchpad import \\             format_to_openai_function_messages         from langchain.tools.base import StructuredTool         from langchain_core.agents import (AgentAction, AgentActionMessageLog,                                            AgentFinish)         from langchain_core.output_parsers import BaseOutputParser         from langchain_core.outputs import ChatGeneration, Generation         from langchain_core.prompts import MessagesPlaceholder          class _TestOutputParser(BaseOutputParser):             def parse_result(                 self, result: List[Generation], *, partial: bool = False             ) -&gt; Union[AgentAction, AgentFinish]:                 if not isinstance(result[0], ChatGeneration):                     raise ValueError(                         \"This output parser only works on ChatGeneration output\"                     )                 message = result[0].message                 function_call = message.additional_kwargs.get(\"function_call\", {})                 if function_call:                     function_name = function_call[\"name\"]                     tool_input = function_call.get(\"arguments\", {})                     tool_input = json.loads(tool_input)                      content_msg = (                         f\"responded: {message.content}\\n\" if message.content else \"\\n\"                     )                     log_msg = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"                     return AgentActionMessageLog(                         tool=function_name,                         tool_input=tool_input,                         log=log_msg,                         message_log=[message],                     )                  return AgentFinish(                     return_values={\"output\": message.content}, log=str(message.content)                 )              def parse(self, text: str) -&gt; Union[AgentAction, AgentFinish]:                 raise ValueError(\"Can only parse messages\")          tools_func = [             list_datasets,             list_tables,             get_table,             run_sql_query,             run_forecasts,         ]          tools = [StructuredTool.from_function(tool) for tool in tools_func]          prompt_template = hub.pull(\"homanp/superagent\")         prompt = prompt_template.from_messages(             [                 (\"user\", \"{input}\"),                 MessagesPlaceholder(variable_name=\"agent_scratchpad\"),             ]         )          llm = langchain_google_vertexai.chat_models.ChatVertexAI(             model_name=\"gemini-2.0-flash-001\",             temperature=0.3,         )          agent = (             {  # type: ignore                 \"input\": lambda x: x[\"input\"],                 \"agent_scratchpad\": lambda x: format_to_openai_function_messages(                     x[\"intermediate_steps\"]                 ),             }             | prompt             | llm.bind(functions=tools)             | _TestOutputParser()         )         self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)      def query(self, query: str):         prompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use the tools provided for generating forecasts. \"         return self.agent_executor.invoke({\"input\": f\"{prompt_prefix} {query}\"}) In\u00a0[\u00a0]: Copied! <pre>agent = CustomLangChainAgent()\nagent.set_up()\n</pre> agent = CustomLangChainAgent() agent.set_up() In\u00a0[\u00a0]: Copied! <pre># sku = \"JNE3797-KR-XXXL\"\nsku = \"J0230-SKD-M\"\nprompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed for generated forecasts after querying table.\"\n</pre> # sku = \"JNE3797-KR-XXXL\" sku = \"J0230-SKD-M\" prompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed for generated forecasts after querying table.\" In\u00a0[\u00a0]: Copied! <pre>response = agent.query(\n    query=f\"\"\"{prompt_prefix} Show me daily sales for SKU {sku} last 20 days by date? Display results as a table.\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = agent.query(     query=f\"\"\"{prompt_prefix} Show me daily sales for SKU {sku} last 20 days by date? Display results as a table.\"\"\" ) display(Markdown(response[\"output\"])) In\u00a0[\u00a0]: Copied! <pre>response = agent.query(\n    query=f\"\"\"Generate daily sales forecasts for SKU {sku} based on last 2 weeks sales.\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = agent.query(     query=f\"\"\"Generate daily sales forecasts for SKU {sku} based on last 2 weeks sales.\"\"\" ) display(Markdown(response[\"output\"])) <ul> <li>Let's re-define the agent to avoid any stateful information in the agent due to our testing in the previous cell</li> </ul> In\u00a0[\u00a0]: Copied! <pre>model = AGENT_MODEL\nagent_name = \"review-product-performance\"\n\nagent = reasoning_engines.LangchainAgent(\n    model=model,\n    model_kwargs={\"temperature\": 0.3},\n    tools=[list_datasets, list_tables, get_table, run_sql_query, run_forecasts],\n    agent_executor_kwargs={\"return_intermediate_steps\": True, \"verbose\": True},\n)\n</pre> model = AGENT_MODEL agent_name = \"review-product-performance\"  agent = reasoning_engines.LangchainAgent(     model=model,     model_kwargs={\"temperature\": 0.3},     tools=[list_datasets, list_tables, get_table, run_sql_query, run_forecasts],     agent_executor_kwargs={\"return_intermediate_steps\": True, \"verbose\": True}, ) In\u00a0[\u00a0]: Copied! <pre>%%bash -s $PROJECT_ID\n\nPROJECT_ID=$1\nPROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\") &amp;&amp; \\\nSERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-aiplatform-re.iam.gserviceaccount.com\" &amp;&amp; \\\necho $SERVICE_ACCOUNT &amp;&amp; \\\n# Grant Cloud Storage permission\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n    --role=\"roles/storage.admin\" \\\n    --quiet &amp;&amp; \\\n# Grant AI Platform permission.\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n    --role=\"roles/aiplatform.user\" \\\n    --quiet &amp;&amp; \\\n# Grant BigQuery user and job permissions\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n    --role=\"roles/bigquery.user\" \\\n    --quiet &amp;&amp; \\\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n    --role=\"roles/bigquery.dataViewer\" \\\n    --quiet &amp;&amp; \\\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n    --role=\"roles/bigquery.jobUser\" \\\n    --quiet &amp;&amp; \\\ngcloud projects get-iam-policy $PROJECT_ID \\\n    --filter=bindings.members:serviceAccount:$SERVICE_ACCOUNT\n</pre> %%bash -s $PROJECT_ID  PROJECT_ID=$1 PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\") &amp;&amp; \\ SERVICE_ACCOUNT=\"service-${PROJECT_NUMBER}@gcp-sa-aiplatform-re.iam.gserviceaccount.com\" &amp;&amp; \\ echo $SERVICE_ACCOUNT &amp;&amp; \\ # Grant Cloud Storage permission gcloud projects add-iam-policy-binding $PROJECT_ID \\     --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\     --role=\"roles/storage.admin\" \\     --quiet &amp;&amp; \\ # Grant AI Platform permission. gcloud projects add-iam-policy-binding $PROJECT_ID \\     --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\     --role=\"roles/aiplatform.user\" \\     --quiet &amp;&amp; \\ # Grant BigQuery user and job permissions gcloud projects add-iam-policy-binding $PROJECT_ID \\     --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\     --role=\"roles/bigquery.user\" \\     --quiet &amp;&amp; \\ gcloud projects add-iam-policy-binding $PROJECT_ID \\     --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\     --role=\"roles/bigquery.dataViewer\" \\     --quiet &amp;&amp; \\ gcloud projects add-iam-policy-binding $PROJECT_ID \\     --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\     --role=\"roles/bigquery.jobUser\" \\     --quiet &amp;&amp; \\ gcloud projects get-iam-policy $PROJECT_ID \\     --filter=bindings.members:serviceAccount:$SERVICE_ACCOUNT In\u00a0[\u00a0]: Copied! <pre>remote_agent = reasoning_engines.ReasoningEngine.create(\n    reasoning_engine=agent,\n    reasoning_engine_name=agent_name,\n    display_name=agent_name,\n    requirements=[\n        \"google-cloud-aiplatform==1.51.0\",\n        \"google-cloud-bigquery==3.22.0\",\n        \"langchain==0.1.20\",\n        \"langchain-google-vertexai==1.0.3\",\n        \"cloudpickle==3.0.0\",\n        \"pydantic==2.7.1\",\n    ],\n)\n</pre> remote_agent = reasoning_engines.ReasoningEngine.create(     reasoning_engine=agent,     reasoning_engine_name=agent_name,     display_name=agent_name,     requirements=[         \"google-cloud-aiplatform==1.51.0\",         \"google-cloud-bigquery==3.22.0\",         \"langchain==0.1.20\",         \"langchain-google-vertexai==1.0.3\",         \"cloudpickle==3.0.0\",         \"pydantic==2.7.1\",     ], ) In\u00a0[\u00a0]: Copied! <pre>engines = [\n    engine.resource_name\n    for engine in reasoning_engines.ReasoningEngine.list(\n        filter=f'display_name=\"{agent_name}\"'\n    )\n]\n\nif len(engines) &gt; 0:\n    engine_id = engines[0]\nelse:\n    raise Exception(\"Reasoning engine agent with that name does not exist\")\n</pre> engines = [     engine.resource_name     for engine in reasoning_engines.ReasoningEngine.list(         filter=f'display_name=\"{agent_name}\"'     ) ]  if len(engines) &gt; 0:     engine_id = engines[0] else:     raise Exception(\"Reasoning engine agent with that name does not exist\") In\u00a0[\u00a0]: Copied! <pre>engines = [\n    engine.resource_name\n    for engine in reasoning_engines.ReasoningEngine.list(\n        filter=f'display_name=\"{agent_name}\"'\n    )\n]\n</pre> engines = [     engine.resource_name     for engine in reasoning_engines.ReasoningEngine.list(         filter=f'display_name=\"{agent_name}\"'     ) ] In\u00a0[\u00a0]: Copied! <pre>remote_agent = reasoning_engines.ReasoningEngine(engine_id)\n</pre> remote_agent = reasoning_engines.ReasoningEngine(engine_id) In\u00a0[\u00a0]: Copied! <pre># sku = \"JNE3797-KR-XXXL\"\nsku = \"J0230-SKD-M\"\nprompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed to generate forecasts after querying the relevant tables.\"\n</pre> # sku = \"JNE3797-KR-XXXL\" sku = \"J0230-SKD-M\" prompt_prefix = \"REMEMBER: Current date is May 12, 2022. Use tools as needed to generate forecasts after querying the relevant tables.\" In\u00a0[\u00a0]: Copied! <pre>response = remote_agent.query(\n    input=f\"\"\"Which tables can you query from {BQ_DATASET_ID} dataset?\"\"\"\n)\nprint(response[\"output\"])\n</pre> response = remote_agent.query(     input=f\"\"\"Which tables can you query from {BQ_DATASET_ID} dataset?\"\"\" ) print(response[\"output\"]) In\u00a0[\u00a0]: Copied! <pre>response = remote_agent.query(\n    input=f\"\"\"{prompt_prefix} What are daily sales for SKU {sku} in last 2 weeks with date? Display as a table with date.\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = remote_agent.query(     input=f\"\"\"{prompt_prefix} What are daily sales for SKU {sku} in last 2 weeks with date? Display as a table with date.\"\"\" ) display(Markdown(response[\"output\"])) In\u00a0[\u00a0]: Copied! <pre>response = remote_agent.query(\n    input=f\"\"\"{prompt_prefix} Generate daily sales forecasts for SKU {sku} using only last 2 weeks of sales. Display as a table with date.\"\"\"\n)\ndisplay(Markdown(response[\"output\"]))\n</pre> response = remote_agent.query(     input=f\"\"\"{prompt_prefix} Generate daily sales forecasts for SKU {sku} using only last 2 weeks of sales. Display as a table with date.\"\"\" ) display(Markdown(response[\"output\"])) In\u00a0[\u00a0]: Copied! <pre>delete_agent = True\ndelete_endpoint = True\ndelete_bq_dataset = True\ndelete_bucket = True\n</pre> delete_agent = True delete_endpoint = True delete_bq_dataset = True delete_bucket = True <ul> <li>\ud83d\uddd1\ufe0f Remove reasoning engine agents deployed</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if delete_agent:\n    # list engines and filter\n    engines = [\n        engine.resource_name\n        for engine in reasoning_engines.ReasoningEngine.list(\n            filter=f'display_name=\"{agent_name}\"'\n        )\n    ]\n    if len(engines) &gt; 0:\n        engine_id = engines[0]\n        agent = reasoning_engines.ReasoningEngine(engine_id)\n        print(f\"Deleting agent {agent.display_name}\")\n        agent.delete()\n    else:\n        raise Exception(\n            f\"Reasoning engine agent with name `{agent_name}` does not exist\"\n        )\n</pre> if delete_agent:     # list engines and filter     engines = [         engine.resource_name         for engine in reasoning_engines.ReasoningEngine.list(             filter=f'display_name=\"{agent_name}\"'         )     ]     if len(engines) &gt; 0:         engine_id = engines[0]         agent = reasoning_engines.ReasoningEngine(engine_id)         print(f\"Deleting agent {agent.display_name}\")         agent.delete()     else:         raise Exception(             f\"Reasoning engine agent with name `{agent_name}` does not exist\"         ) <ul> <li>\ud83d\uddd1\ufe0f Remove Vertex AI prediction endpoint deployed with TimesFM model</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if delete_endpoint:\n    endpoints = aiplatform.Endpoint.list(\n        filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"'\n    )\n\n    if len(endpoints) &gt; 0:\n        # Undeploy model and delete endpoint.\n        endpoint = aiplatform.Endpoint(endpoints[0].resource_name)\n        deployed_models = [\n            aiplatform.Model(model.model) for model in endpoint.list_models()\n        ]\n        print(f\"Deleting endpoint {endpoint.display_name}\")\n        endpoint.delete(force=True)\n        # Delete models\n        [model.delete() for model in deployed_models]\n    else:\n        raise Exception(\n            f\"Endpoint with name {TIMESFM_ENDPOINT_DISPLAY_NAME} does not exist\"\n        )\n</pre> if delete_endpoint:     endpoints = aiplatform.Endpoint.list(         filter=f'display_name=\"{TIMESFM_ENDPOINT_DISPLAY_NAME}\"'     )      if len(endpoints) &gt; 0:         # Undeploy model and delete endpoint.         endpoint = aiplatform.Endpoint(endpoints[0].resource_name)         deployed_models = [             aiplatform.Model(model.model) for model in endpoint.list_models()         ]         print(f\"Deleting endpoint {endpoint.display_name}\")         endpoint.delete(force=True)         # Delete models         [model.delete() for model in deployed_models]     else:         raise Exception(             f\"Endpoint with name {TIMESFM_ENDPOINT_DISPLAY_NAME} does not exist\"         ) <ul> <li>\ud83d\uddd1\ufe0f Remove BigQuery tables and datasets</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if delete_bq_dataset:\n    print(f\"Deleting BigQuery dataset with id {BQ_DATASET_ID}\")\n    ! bq rm -r -f $BQ_DATASET_ID\n    ! bq ls\n</pre> if delete_bq_dataset:     print(f\"Deleting BigQuery dataset with id {BQ_DATASET_ID}\")     ! bq rm -r -f $BQ_DATASET_ID     ! bq ls <ul> <li>\ud83d\uddd1\ufe0f Remove Google Cloud Storage bucket</li> </ul> In\u00a0[\u00a0]: Copied! <pre>if delete_bucket:\n    print(f\"Deleting contents from the Cloud Storage bucket {STAGING_BUCKET_URI}\")\n    # uncomment below line to delete contents of the bucket\n    # ! gsutil -m rm -r $STAGING_BUCKET_URI\n</pre> if delete_bucket:     print(f\"Deleting contents from the Cloud Storage bucket {STAGING_BUCKET_URI}\")     # uncomment below line to delete contents of the bucket     # ! gsutil -m rm -r $STAGING_BUCKET_URI"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#operationalizing-timesfm-on-vertex-ai","title":"Operationalizing TimesFM on Vertex AI\u00b6","text":"Run in Colab       Run in Colab Enterprise       View on GitHub       Open in Vertex AI Workbench"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#overview","title":"\ud83d\udccc Overview\u00b6","text":"<p>This notebook shows how to operationalize TimesFM model on Vertex AI within the context of complementing a Vertex AI Gemini based Generative AI application with predictive open source models such as TimesFM. This notebook was demonstrated as part of Google IO 2024 talk and recommended to watch the talk to get familiarized with concepts presented in this notebook.</p> <ul> <li>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. TimesFM is now available on Vertex AI Model Garden.</li> <li>Gemini Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.</li> <li>Vertex AI Reasoning Engine (LangChain on Vertex AI) is a managed service that helps you to build and deploy an agent reasoning framework. It gives developers the flexibility to choose how much reasoning they want to delegate to the LLM and how much they want to handle with customized code. Developers can define Python functions that get used as tools via Gemini Function Calling. Reasoning Engine integrates closely with the Python SDK for the Gemini model in Vertex AI, and it can manage prompts, agents, and examples in a modular way. Reasoning Engine is compatible with LangChain, LlamaIndex, or other Python frameworks.</li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#architecture","title":"\ud83d\udcd0 Architecture\u00b6","text":"<p>Following is a high-level architecture of what we will build in this notebook.</p> <p>You will perform the following steps:</p> <ul> <li>Deploy Google's open source TimesFM forecasting foundation model from the Vertex Model Garden</li> <li>Integrate TimesFM with a generative AI agent using Vertex AI Gemini's function calling</li> <li>Deploy the agent on Vertex AI Reasoning Engine (LangChain on Vertex AI) using default or custom LangChain template.</li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#getting-started","title":"\ud83c\udfac Getting Started\u00b6","text":"<p>The following steps are necessary to run this notebook, no matter what notebook environment you're using.</p> <p>If you're entirely new to Google Cloud, get started here.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#google-cloud-project-setup","title":"Google Cloud Project Setup\u00b6","text":"<ol> <li>Select or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.</li> <li>Make sure that billing is enabled for your project.</li> <li>Enable the Service Usage API</li> <li>Enable the Vertex AI API.</li> <li>Enable the Cloud Storage API.</li> <li>Enable the Cloud BigQuery API.</li> <li>Enable the Cloud Resource Manager API.</li> </ol>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#google-cloud-permissions","title":"Google Cloud Permissions\u00b6","text":"<p>To run the complete Notebook,you will need to have the Owner role for your project. At minimum, you need the following roles:</p> <ul> <li><code>roles/serviceusage.serviceUsageAdmin</code> to enable APIs</li> <li><code>roles/iam.serviceAccountAdmin</code> to modify service agent permissions</li> <li><code>roles/aiplatform.user</code> to use AI Platform components</li> <li><code>roles/storage.objectAdmin</code> to modify and delete GCS buckets</li> <li><code>roles/bigquery.user</code> and <code>roles/bigquery.dataViewer</code> to query BigQuery tables</li> <li><code>roles/bigquery.jobUser</code> to run BigQuery jobs</li> <li><code>roles/secretmanager.secretAccessor</code> to access secret versions in Cloud Secret Manager</li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#install-vertex-ai-sdk-and-other-required-packages","title":"Install Vertex AI SDK and other required packages\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#restart-runtime","title":"Restart Runtime\u00b6","text":"<p>To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.</p> <p>You may see the restart reported as a crash, but it is working as-intended -- you are merely restarting the runtime.</p> <p>The restart might take a minute or longer. After it's restarted, continue to the next step.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#authenticate","title":"Authenticate\u00b6","text":"<p>If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud project.</p> <p>If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into Application Default Credentials for your local environment and initializing the Google Cloud CLI. In many cases, running <code>gcloud auth application-default login</code> in a shell on the machine running the notebook kernel is sufficient.</p> <p>More authentication options are discussed here.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#set-google-cloud-project-information","title":"Set Google Cloud project information\u00b6","text":"<p>To get started using Vertex AI, you must have an existing Google Cloud project and enable the Vertex AI API.</p> <p>Learn more about setting up a project and a development environment.</p> <p>Make sure to change <code>PROJECT_ID</code> in the next cell. You can leave the values for <code>LOCATION</code>unless you have a specific reason to change them.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#enable-required-google-cloud-apis","title":"Enable required Google Cloud APIs\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#initialize-vertex-ai-sdk","title":"Initialize Vertex AI SDK\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#create-staging-cloud-storage-bucket","title":"Create staging Cloud Storage bucket\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#configure-secrets","title":"Configure secrets\u00b6","text":"<p>This notebooks accesses datasets on Kaggle. To access the dataset from Kaggle, you'll need Kaggle API Key/Token. There are a few ways to manage these API keys depending on what environment you are using to run this notebook.</p> \u26a0\ufe0f Mishandling API tokens or secret credentials can lead to unauthorized access and data breaches. Never hardcode these values directly into your code. Utilize secure storage solutions like Cloud Secret Manager or Colab Secrets. \u26a0\ufe0f"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#lets-build","title":"Let's Build!\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#00-data-preparation","title":"\ud83d\udc68\u200d\ud83c\udf73 00 - Data Preparation\u00b6","text":"<p>In this section, we prepare data required for rest of the steps. We use e-commerce sales data from Kaggle. Please check the link for terms of use of the dataset featured in this notebook.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-1-download-dataset-from-kaggle","title":"Step 1. Download dataset from Kaggle\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-create-bigquery-dataset","title":"Step 2. Create BigQuery dataset\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-3-load-dataset-to-bigquery-table","title":"Step 3. Load dataset to BigQuery table\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-4-prepare-and-transform-data-in-bigquery-table","title":"Step 4. Prepare and transform data in BigQuery table\u00b6","text":"<p>Prepare data by adding transformations to interpolate missing data points using BigQuery time series functions such as <code>GAP_FILL</code> and time series windowing.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#01-deploy-timesfm-on-vertex-ai-endpoint-from-vertex-ai-model-garden","title":"\ud83e\ude82 01 - Deploy TimesFM on Vertex AI Endpoint from Vertex AI Model Garden\u00b6","text":"<p>This steps deploys TimesFM model to Vertex AI Endpoint from Vertex AI Model Garden.</p> <p>The TimesFM is a 200M parameter transformer based model trained in the decoder only fashion on a pretrain dataset containing over 100 billion real-world timepoints. It performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator input.</p> <p>TimesFM model can be used for times series forecasting and the model takes as input context a univariate time series, along with an optional frequency parameter. The model forecasts the time series into a future horizon of any length.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-1-set-up-prediction-environment","title":"Step 1. Set up prediction environment\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-copy-timesfm-model-artifacts-to-staging-bucket","title":"Step 2. Copy TimesFM model artifacts to staging bucket\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-3-define-utility-functions-to-deploy-the-model","title":"Step 3. Define utility functions to deploy the model\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-6-run-the-container-locally-optional","title":"Step 6. Run the container locally [Optional]\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-7-deploy-the-timesfm-to-vertex-ai-endpoint","title":"Step 7. Deploy the TimesFM to Vertex AI endpoint\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#02-define-functions","title":"\ud83d\udee0\ufe0f 02 - Define functions\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-1-define-functions-to-interact-with-bigquery-using-natural-language-nl2sql","title":"Step 1. Define functions to interact with BigQuery using Natural Language (NL2SQL)\u00b6","text":"<p>Define functions to</p> <ul> <li>\u2705 Get list of datasets from BigQuery</li> <li>\u2705 Get list of tables from a dataset that will help answer user's query</li> <li>\u2705 Get information about a table including description and schema that will help answer user's query</li> <li>\u2705 Get information from data in BigQuery by running SQL queries</li> </ul> <p>The function descriptions should be concise and clear, as these descriptions are to the Gemini model for the agent.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-define-function-to-get-forecasts-using-timesfm-model-on-vertex-ai","title":"Step 2. Define function to get forecasts using TimesFM model on Vertex AI\u00b6","text":"<p>Define function to</p> <ul> <li>\u2705 Predict forecasts based on historical time-series contexts</li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-3-run-a-few-tests-and-plot-forecasts","title":"Step 3. Run a few tests and plot forecasts\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#03-define-agent-with-model-and-tools","title":"\ud83e\udde0 03 - Define Agent with Model and Tools\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-1-define-agent-with-default-langchain-template-using-vertex-ai-reasoning-engines","title":"Step 1. Define agent with default LangChain template using Vertex AI Reasoning Engines\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-run-agent-locally-and-understand-how-it-works","title":"Step 2. Run agent locally and understand how it works\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-3-define-agent-with-custom-langchain-template-optional","title":"Step 3. Define agent with custom LangChain template [Optional]\u00b6","text":"<p>Define with custom LangChain template as needed to include any additional error handling, custom flows, output parsing etc. or even swap out LangChain for an entirely different orchestration framework!</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#04-deploy-your-agent-on-vertex-ai","title":"\ud83d\ude80 04 - Deploy your agent on Vertex AI\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-1-set-up-service-agent-permissions","title":"Step 1. Set up service agent permissions\u00b6","text":"<p>Grant required permissions to the Google-managed reasoning engine service account. Refer to the documentation for details.</p> <p>NOTE: You would need Project Owner or Project IAM Admin permissions to add necessary IAM policy bindings.</p>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-deploy-the-agent-to-reasoning-engine-in-vertex-ai","title":"Step 2. Deploy the agent to Reasoning Engine in Vertex AI\u00b6","text":"<ul> <li>Deploy the agent to Reasoning Engine in Vertex AI by calling <code>reasoning_engines.ReasoningEngine.create()</code> along with the instance of the agent and the Python packages that agent requires at runtime:</li> </ul>"},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#step-2-test-remote-agent","title":"Step 2. Test remote agent\u00b6","text":""},{"location":"research-operationalization/timesfm/operationalizing_timesfm_on_vertexai/#cleaning-up","title":"\ud83e\uddf9 Cleaning up\u00b6","text":"<p>Clean up resources created in this notebook.</p>"}]}